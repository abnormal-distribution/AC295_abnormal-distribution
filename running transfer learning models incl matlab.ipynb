{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qltYDOKkFYds"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import scipy as sp\n",
    "import scipy.io as sio \n",
    "import scipy.sparse as scp\n",
    "from scipy.sparse.linalg import svds as SVD\n",
    "\n",
    "import sklearn \n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import matlab\n",
    "import matlab.engine\n",
    "import scipy.io as sio\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zg4nBe8bHued",
    "outputId": "c70d7d4a-21c4-4748-f485-a75db2dde40a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/gdrive')\n",
    "basepath = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t15Up--rFcFP"
   },
   "source": [
    "## Spectral Feature Alignment (SFA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8iU1UJIvFhsk"
   },
   "outputs": [],
   "source": [
    "class SFA:\n",
    "    '''\n",
    "    spectral feature alignment\n",
    "    '''\n",
    "    def __init__(self,l=500,K=100, base_classifer=svm.SVC()):\n",
    "        self.l = l\n",
    "        self.K = K\n",
    "        self.m = 0\n",
    "        self.ut = None\n",
    "        self.phi = 1\n",
    "        self.base_classifer = base_classifer\n",
    "        self.ix = None\n",
    "        self._ix = None\n",
    "        return\n",
    "\n",
    "    def fit(self, Xs,Xt):\n",
    "\n",
    "        # Sort indices by highest sum of columns (highest word frequency)\n",
    "        ix_s = np.argsort(np.sum(Xs, axis=0))\n",
    "        ix_t = np.argsort(np.sum(Xt, axis=0))\n",
    "\n",
    "        ix_s = ix_s[::-1][:self.l]\n",
    "        ix_t = ix_t[::-1][:self.l]\n",
    "        # Instersect words with highest word frequency in both source and target\n",
    "        ix = np.intersect1d(ix_s, ix_t)\n",
    "        # Complement of previous index\n",
    "        _ix = np.setdiff1d(range(Xs.shape[1]), ix)\n",
    "        self.ix = ix\n",
    "        self._ix = _ix\n",
    "        self.m = len(_ix)\n",
    "        self.l = len(ix)\n",
    "\n",
    "        \n",
    "        X = np.concatenate((Xs, Xt), axis=0)\n",
    "        # High frequency word matrix. 1 in every spot where there was a high \n",
    "        # frequency word count\n",
    "        DI = (X[:, ix]>0).astype('float')\n",
    "        # Low frequency word matrix. 1 in every spot where there's a low \n",
    "        # frequency word count\n",
    "        DS = (X[:, _ix]>0).astype('float')\n",
    "\n",
    "        # construct co-occurrence matrix DSxDI\n",
    "        M = np.zeros((self.m,self.l))\n",
    "        for i in range(X.shape[0]):\n",
    "            tem1 = np.reshape(DS[i], (1, self.m))\n",
    "            tem2 = np.reshape(DI[i], (1, self.l))\n",
    "            M += np.matmul(tem1.T, tem2)\n",
    "        M = M/np.linalg.norm(M, 'fro')\n",
    "        M = scp.lil_matrix(M)\n",
    "        D1 = scp.lil_matrix((self.m, self.m))\n",
    "        D2 = scp.lil_matrix((self.l, self.l))\n",
    "        for i in range(self.m):\n",
    "            D1[i,i] = 1.0/np.sqrt(np.sum(M[i,:]))\n",
    "        for i in range(self.l):\n",
    "            D2[i,i] = 1.0/np.sqrt(np.sum(M[:,i]))\n",
    "        B = (D1.tocsr().dot(M.tocsr())).dot(D2.tocsr())\n",
    "        ut, s, vt = SVD(B.tocsc(), k=self.K)\n",
    "        self.ut = ut\n",
    "        return ut\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.concatenate((X, X[:, self._ix].dot(self.ut)), axis=1)\n",
    "\n",
    "    def fit_predict(self,Xs, Xt, X_test, Ys, Y_test):\n",
    "        ut = self.fit(Xs, Xt)\n",
    "        Xs = self.transform(Xs)\n",
    "        self.base_classifer.fit(Xs, Ys)\n",
    "        X_test = self.transform(X_test)\n",
    "        y_pred = self.base_classifer.predict(X_test)\n",
    "        acc = accuracy_score(Y_test, y_pred)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "FykURd88F0j_",
    "outputId": "401bff42-dcc7-499f-8bba-22d0714bed49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K D 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D K 0.7155\n"
     ]
    }
   ],
   "source": [
    "#datasets = ['K', 'D', 'B', 'E']\n",
    "datasets = ['K', 'D']\n",
    "\n",
    "results = {}\n",
    "for dataset1 in datasets:\n",
    "    for dataset2 in datasets:\n",
    "        if dataset1 == dataset2:\n",
    "            continue\n",
    "        [Xs, Ys, X_test, Y_test, Xt]=joblib.load(\n",
    "                    os.path.join(basepath, dataset1+'-'+dataset2+'.pkl'))\n",
    "        Xs = Xs.astype('float')\n",
    "        X_test = X_test.astype('float')\n",
    "        Xt = Xt.astype('float')\n",
    "        model = SFA()\n",
    "        acc = model.fit_predict(Xs, Xt, X_test, Ys, Y_test)\n",
    "        print(dataset1, dataset2, acc)\n",
    "        results[dataset1+dataset2] = acc\n",
    "\n",
    "with open(\"SFA_record.json\",'w') as json_file:\t\n",
    "\t\tjson.dump(results, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzeBOUXkHL76"
   },
   "source": [
    "## Structural Correspondence Learning (SCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZheCaa8mHK5v"
   },
   "outputs": [],
   "source": [
    "class SCL(object):\n",
    "    '''\n",
    "    class of structural correspondence learning \n",
    "    '''\n",
    "    def __init__(self, l2=1.0, num_pivots=10, base_classifer=LinearSVC()):\n",
    "        self.l2 = l2\n",
    "        self.num_pivots = num_pivots\n",
    "        self.W = 0\n",
    "        self.base_classifer = base_classifer\n",
    "        # self.train_data_dim = None\n",
    "\n",
    "    def fit(self, Xs, Xt):\n",
    "        '''\n",
    "        find pivot features and transfer the Xs and Xt\n",
    "        Param Xs: source data\n",
    "        Param Xt: target data\n",
    "        output Xs_new: new source data features\n",
    "        output Xt_new: new target data features\n",
    "        output W: transform matrix\n",
    "        '''\n",
    "        _, ds = Xs.shape\n",
    "        _, dt = Xt.shape\n",
    "        assert ds == dt\n",
    "        X = np.concatenate((Xs, Xt), axis=0)\n",
    "        ix = np.argsort(np.sum(X, axis=0))\n",
    "        ix = ix[::-1][:self.num_pivots]\n",
    "        pivots = (X[:, ix]>0).astype('float')\n",
    "        p = np.zeros((ds, self.num_pivots))\n",
    "        # train for the classifers \n",
    "        for i in range(self.num_pivots):\n",
    "            clf = linear_model.SGDClassifier(loss=\"modified_huber\", alpha=self.l2)\n",
    "            clf.fit(X, pivots[:, i])\n",
    "            p[:, i] = clf.coef_\n",
    "        _, W = np.linalg.eig(np.cov(p))\n",
    "        W = W[:, :self.num_pivots].astype('float')\n",
    "        self.W = W\n",
    "        Xs_new = np.concatenate((np.dot(Xs, W), Xs), axis=1)\n",
    "        Xt_new = np.concatenate((np.dot(Xt, W), Xt), axis=1)\n",
    "\n",
    "        return Xs_new, Xt_new, W\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        transform the origianl data by add new features\n",
    "        Param X: original data\n",
    "        output x_new: X with new features\n",
    "        '''\n",
    "        X_new = np.concatenate((np.dot(X, self.W),X), axis=1)\n",
    "        return X_new\n",
    "    \n",
    "    def fit_predict(self, Xs, Xt, X_test, Ys, Y_test):\n",
    "        self.fit(Xs, Xt)\n",
    "        Xs = self.transform(Xs)\n",
    "        self.base_classifer.fit(Xs, Ys)\n",
    "        X_test = self.transform(X_test)\n",
    "        y_pred = self.base_classifer.predict(X_test)\n",
    "        acc = accuracy_score(Y_test, y_pred)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "oi-eufjsIRwk",
    "outputId": "b1d01b1c-3b0b-4482-e70b-8f180ba626c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K D 0.7215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D K 0.747\n"
     ]
    }
   ],
   "source": [
    "#datasets = ['K', 'D', 'B', 'E']\n",
    "datasets = ['K', 'D']\n",
    "\n",
    "results = {}\n",
    "for dataset1 in datasets:\n",
    "    for dataset2 in datasets:\n",
    "        if dataset1 == dataset2:\n",
    "            continue\n",
    "        [Xs, Ys, X_test, Y_test, Xt]=joblib.load(\n",
    "                    os.path.join(basepath, dataset1+'-'+dataset2+'.pkl'))\n",
    "        Xs = Xs.astype('float')\n",
    "        X_test = X_test.astype('float')\n",
    "        Xt = Xt.astype('float')\n",
    "        model = SCL()\n",
    "        acc = model.fit_predict(Xs, Xt, X_test, Ys, Y_test)\n",
    "        print(dataset1, dataset2, acc)\n",
    "        results[dataset1+dataset2] = acc\n",
    "\n",
    "with open(\"SCL_record.json\",'w') as json_file:\t\n",
    "\t\tjson.dump(results, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMhAZrDuIzcG"
   },
   "source": [
    " ## Marginalized Denoising Autoencoders for Domain Adaptation (mSDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qRrWE9OkI-LR"
   },
   "outputs": [],
   "source": [
    "class mSDA(object):\n",
    "    '''\n",
    "    Implement mSDA.\n",
    "    To read more about the SDA, check the following paper:\n",
    "        Chen M , Xu Z , Weinberger K , et al.\n",
    "        Marginalized Denoising Autoencoders for Domain Adaptation[J].\n",
    "        Computer Science, 2012.\n",
    "    This implementation of mSDA is based on both the sample code the authors provided\n",
    "    as well as the equations in the paper.\n",
    "    The code is modified according to https://github.com/douxu896/mSDA\n",
    "    '''\n",
    "    def __init__(self, p=None, l=5, act=np.tanh, Ws=None, bias=True):\n",
    "        '''\n",
    "        :param p: corruption probability\n",
    "        :param l: number of layers\n",
    "        :param act: what nonlinearity to use? if None, not to use nonlinearity.\n",
    "        :param Ws: model parameters. Can optionally pass in precomputed Ws to use to transform X.\n",
    "                (e.g. if transforming test X with Ws learned from training X)\n",
    "        :param bias: Whether to use bias?\n",
    "        '''\n",
    "        self.p = p\n",
    "        self.l = l\n",
    "        self.act = act\n",
    "        self.Ws = Ws\n",
    "        self.bias = bias\n",
    "\n",
    "    def mDA(self, X, W=None):\n",
    "        '''\n",
    "        One layer Marginalized Denoising Autoencoder.\n",
    "        Learn a representation h of X by reconstructing \"corrupted\" input but marginalizing out corruption\n",
    "        :param X: input features, shape:(num_samples,num_features)\n",
    "        :param W: model parameters. Can optionally pass in precomputed W to use to transform X.\n",
    "                (e.g. if transforming test X with W learned from training X)\n",
    "        :return: model parameters, reconstructed representation.\n",
    "        '''\n",
    "        if self.bias:\n",
    "            X=np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "        if W is None:\n",
    "            W = self._compute_reconstruction_W(X)\n",
    "        h = np.dot(X, W)  # no nonlinearity\n",
    "        if self.act is not None:\n",
    "            h = self.act(h)  # inject nonlinearity\n",
    "        return W, h\n",
    "\n",
    "    def _compute_reconstruction_W(self, X):\n",
    "        '''\n",
    "        Learn reconstruction parameters.\n",
    "        :param X: input features, shape:(num_samples,num_features)\n",
    "        :return: model parameters.\n",
    "        '''\n",
    "        # typecast to correct Xtype\n",
    "        X.dtype = \"float64\"\n",
    "        d = X.shape[1]\n",
    "        # Represents the probability that a given feature will be corrupted\n",
    "        if self.bias:\n",
    "            q = np.ones(\n",
    "                (d-1, 1)) * (1 - self.p)\n",
    "            # add bias probability\n",
    "            q=np.vstack((q,1))\n",
    "        else:\n",
    "            q = np.ones(\n",
    "                (d, 1)) * (1 - self.p)\n",
    "\n",
    "        S = np.dot(X.transpose(), X)\n",
    "        Q = S * (np.dot(q, q.transpose()))\n",
    "        Q[np.diag_indices_from(Q)] = q[:,0] * np.diag(S)\n",
    "        P = S * numpy.matlib.repmat(q, 1, d)\n",
    "\n",
    "        # solve equation of the form W = BA^-1\n",
    "        A = Q + 10**-5 * np.eye(d)\n",
    "        B = P[:-1,:]\n",
    "        W = np.linalg.solve(A.transpose(), B.transpose())\n",
    "        return W\n",
    "\n",
    "    def fit(self, X):\n",
    "        '''\n",
    "        Stack mDA layers on top of each other, using previous layer as input for the next\n",
    "        :param X: input features, shape:(num_samples,num_features)\n",
    "        :return: None\n",
    "        '''\n",
    "        Ws = list()\n",
    "        hs = list()\n",
    "        hs.append(X)\n",
    "        for layer in range(0, self.l):\n",
    "            W, h = self.mDA(hs[-1])\n",
    "            Ws.append(W)\n",
    "            hs.append(h)\n",
    "        self.Ws = Ws\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Should be called after fit!\n",
    "        Stack mDA layers on top of each other, using previous layer as input for the next\n",
    "        :param X: input features, shape:(num_samples,num_features)\n",
    "        :return: reconstructed representation of the last layer.\n",
    "        '''\n",
    "        if self.Ws is None:\n",
    "            raise ValueError('Please fit on some data first.')\n",
    "        hs = list()\n",
    "        hs.append(X)\n",
    "        for layer in range(0, self.l):\n",
    "            _, h = self.mDA(hs[-1], self.Ws[layer])\n",
    "            hs.append(h)\n",
    "        return hs[-1]\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        '''\n",
    "        Stack mDA layers on top of each other, using previous layer as input for the next\n",
    "        :param X: input features, shape:(num_samples,num_features)\n",
    "        :return: reconstructed representation of the last layer.\n",
    "        '''\n",
    "        Ws = list()\n",
    "        hs = list()\n",
    "        hs.append(X)\n",
    "        for layer in range(0, self.l):\n",
    "            W, h = self.mDA(hs[-1])\n",
    "            Ws.append(W)\n",
    "            hs.append(h)\n",
    "        self.Ws = Ws\n",
    "        return hs[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "AZpDwheGJRQJ",
    "outputId": "60219136-95be-4962-b447-c3e4294b40dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xs acc on regular X:  0.6275\n",
      "Xt acc on regular X:  0.7495\n",
      "Shape of mSDA Xs_reps h:  (2000, 5000)\n",
      "Shape of mSDA Xs_test_reps h:  (2000, 5000)\n",
      "Shape of mSDA Xt_test_reps h:  (2000, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xs acc with linear SVM on mSDA features:  0.769\n",
      "Xt acc with linear SVM on mSDA features:  0.868\n"
     ]
    }
   ],
   "source": [
    "# test implementation\n",
    "\n",
    "# load dataset1\n",
    "[Xs_train, Ys_train, Xs_test, Ys_test, Xs_unlabeled]=joblib.load(\n",
    "            os.path.join(basepath, 'K-D.pkl'))\n",
    "# load dataset2\n",
    "[Xt_train, Xt_train_label, Xt_test, Xt_test_label, Xt_unlabeled]=joblib.load(\n",
    "            os.path.join(basepath, 'D-K.pkl'))\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC().fit(Xs_train, Ys_train)\n",
    "preds_Xs = clf.predict(Xs_test)\n",
    "acc = np.mean(preds_Xs == Ys_test)\n",
    "print(\"Xs acc on regular X: \", acc)\n",
    "preds_Xt = clf.predict(Xt_test)\n",
    "acc = np.mean(preds_Xt == Xt_test_label)\n",
    "print(\"Xt acc on regular X: \", acc)\n",
    "\n",
    "# set corruption probability, number of layers and bias.\n",
    "pp = 0.3\n",
    "ll = 5\n",
    "bias = True\n",
    "train_X=np.concatenate((Xs_train,Xs_unlabeled,Xt_train,Xt_unlabeled),axis=0)\n",
    "msda=mSDA(p=pp, l=ll,act=np.tanh, Ws=None, bias=True)\n",
    "msda.fit(train_X)\n",
    "Xs_reps = msda.transform(Xs_train)\n",
    "print(\"Shape of mSDA Xs_reps h: \", Xs_reps.shape)\n",
    "Xs_test_reps = msda.transform(Xs_test)\n",
    "print(\"Shape of mSDA Xs_test_reps h: \", Xs_test_reps.shape)\n",
    "Xt_reps = msda.transform(Xt_test)\n",
    "print(\"Shape of mSDA Xt_test_reps h: \", Xt_reps.shape)\n",
    "\n",
    "clf = svm.SVC().fit(Xs_reps, Ys_train)\n",
    "preds_Xs=clf.predict(Xs_test_reps)\n",
    "acc=np.mean(preds_Xs == Ys_test)\n",
    "print(\"Xs acc with linear SVM on mSDA features: \", acc)\n",
    "preds_Xt = clf.predict(Xt_reps)\n",
    "acc = np.mean(preds_Xt == Xt_test_label)\n",
    "print(\"Xt acc with linear SVM on mSDA features: \", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgIxvfM4J9Mc"
   },
   "source": [
    "## Stacked Denoising Autoencoders (SDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sE13iNMFKGJ_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow.compat.v1.keras.backend as K\n",
    "from keras import backend as K\n",
    "os.environ['KERAS_BACKEND'] = \"tensorflow\"\n",
    "#gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "#sess = tf.compat.v1.InteractiveSession(\n",
    "#    config=tf.compat.v1.ConfigProto(\n",
    "#        gpu_options=gpu_options))\n",
    "#K.set_session(sess)\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "class SDA(object):\n",
    "    '''\n",
    "    Implements Stacked Denoising Autoencoders in Keras.\n",
    "    To read more about the SDA, check the following paper:\n",
    "        Vincent P , Larochelle H , Bengio Y , et al.\n",
    "        Extracting and Composing Robust Features with Denoising Autoencoders[C]//\n",
    "        International Conference on Machine Learning. ACM, 2008.\n",
    "    The code is modified according to https://github.com/MadhumitaSushil/SDAE\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            nb_layers=2,\n",
    "            nb_hid=[100],\n",
    "            dropout=[0.1],\n",
    "            enc_act=['tanh'],\n",
    "            dec_act=['linear'],\n",
    "            bias=True,\n",
    "            loss_fn='mse',\n",
    "            batch_size=32,\n",
    "            nb_epoch=300,\n",
    "            optimizer='adam',\n",
    "            verbose=1,\n",
    "            base_classifer=svm.LinearSVC()):\n",
    "        '''\n",
    "        Initializes parameters for stacked denoising autoencoders\n",
    "        :param nb_layers: number of layers, i.e., number of autoencoders to stack on top of each other.\n",
    "        :param nb_hid: list with the number of hidden nodes per layer. If only one value specified, same value is used for all the layers\n",
    "        :param dropout: list with the proportion of X_train nodes to mask at each layer. If only one value is provided, all the layers share the value.\n",
    "        :param enc_act: list with activation function for encoders at each layer. Typically sigmoid.\n",
    "               See also keras.activations for available activation functions.\n",
    "        :param dec_act: list with activation function for decoders at each layer.\n",
    "               Typically the same as encoder for binary X_train, linear for real X_train.\n",
    "               See also keras.activations for available activation functions.\n",
    "        :param bias: True to use bias value.\n",
    "        :param loss_fn: The loss function. Typically 'mse' is used for real values. Options can be found here: https://keras.io/objectives/\n",
    "        :param batch_size: mini batch size for gradient update\n",
    "        :param nb_epoch: number of epochs to train each layer\n",
    "        :param optimizer: The optimizer to use. See also keras.optimizers.\n",
    "        :param verbose: Verbosity mode, 0, 1, or 2.\n",
    "        '''\n",
    "        self.nb_layers = nb_layers\n",
    "        # if only one value specified for nb_hid, dropout, enc_act or dec_act,\n",
    "        # use the same parameters for all layers.\n",
    "        self.nb_hid, self.dropout, self.enc_act, self.dec_act = \\\n",
    "            self._assert_input(nb_layers, nb_hid, dropout, enc_act, dec_act)\n",
    "        self.bias = bias\n",
    "        self.loss_fn = loss_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.encoder_model = None\n",
    "        self.fine_tuned_model = None\n",
    "\n",
    "    def fit(self, X_train=None, X_val=None, patience=1, dropout_all=False, model_layers=None):\n",
    "        '''\n",
    "        Should be called before self.transform and self.fine_tune\n",
    "        Pretrains layers of a stacked denoising autoencoder to generate low-dimensional representation of data.\n",
    "        Returns a list of pretrained sda layers for continue training pre-trained model_layers, if required.\n",
    "        The self.encoder_model can be used in supervised task by adding a classification/regression layer on top,\n",
    "        see also self.fine_tune.\n",
    "        :param X_train: input data (scipy sparse matrix supported). shape:(num_samples,num_features)\n",
    "        :param X_val: validation data (scipy sparse matrix supported). shape:(num_samples,num_features)\n",
    "        :param patience: number of epochs with no improvement after which training will be stopped. Useful when X_val is not None.\n",
    "        :param dropout_all: True to include dropout layer between all layers in the learned encoder model.\n",
    "               By default, dropout is only present for input in the learned encoder model.\n",
    "        :param model_layers: [DA1,DA2,...],Pretrained cur_model layers, to continue training pre-trained model_layers, if required\n",
    "        :return : model_layers for continue training pre-trained model_layers, if required\n",
    "        '''\n",
    "        self._print_sda_config()\n",
    "        if model_layers is not None:\n",
    "            self.nb_layers = len(model_layers)\n",
    "        else:\n",
    "            model_layers = [None] * self.nb_layers\n",
    "\n",
    "        encoders = []\n",
    "        for cur_layer in range(self.nb_layers):\n",
    "            if model_layers[cur_layer] is None:\n",
    "                # same dim of output units as input units (to reconstruct the\n",
    "                # signal)\n",
    "                nb_dim = X_train.shape[1]\n",
    "                input_layer = Input(shape=(nb_dim,))\n",
    "                # masking input data to learn to generalize, and prevent\n",
    "                # identity learning\n",
    "                dropout_layer = Dropout(self.dropout[cur_layer])\n",
    "                in_dropout = dropout_layer(input_layer)\n",
    "                encoder_layer = Dense(\n",
    "                    units=self.nb_hid[cur_layer],\n",
    "                    kernel_initializer='glorot_uniform',\n",
    "                    activation=self.enc_act[cur_layer],\n",
    "                    name='encoder' + str(cur_layer),\n",
    "                    use_bias=self.bias)\n",
    "                encoder = encoder_layer(in_dropout)\n",
    "                decoder_layer = Dense(\n",
    "                    units=nb_dim,\n",
    "                    use_bias=self.bias,\n",
    "                    kernel_initializer='glorot_uniform',\n",
    "                    activation=self.dec_act[cur_layer],\n",
    "                    name='decoder' + str(cur_layer))\n",
    "                decoder = decoder_layer(encoder)\n",
    "                cur_model = Model(input_layer, decoder)\n",
    "                cur_model.compile(loss=self.loss_fn, optimizer=self.optimizer)\n",
    "            else:\n",
    "                cur_model = model_layers[cur_layer]\n",
    "            print(\"Training layer \" + str(cur_layer))\n",
    "            if X_val is not None:\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss', patience=patience, verbose=1)\n",
    "                cur_model.fit_generator(\n",
    "                    generator=data_generator.batch_generator(\n",
    "                        X_train,\n",
    "                        X_train,\n",
    "                        batch_size=self.batch_size,\n",
    "                        shuffle=True),\n",
    "                    callbacks=[early_stopping],\n",
    "                    epochs=self.nb_epoch,\n",
    "                    steps_per_epoch=int(np.ceil(X_train.shape[0] / self.batch_size)),\n",
    "                    verbose=self.verbose,\n",
    "                    validation_data=data_generator.batch_generator(\n",
    "                        X_val,\n",
    "                        X_val,\n",
    "                        batch_size=self.batch_size,\n",
    "                        shuffle=False),\n",
    "                    validation_steps=int(np.ceil(X_val.shape[0] / self.batch_size)))\n",
    "            else:\n",
    "                cur_model.fit_generator(\n",
    "                    generator=data_generator.batch_generator(\n",
    "                        X_train,\n",
    "                        X_train,\n",
    "                        batch_size=self.batch_size,\n",
    "                        shuffle=True),\n",
    "                    epochs=self.nb_epoch,\n",
    "                    steps_per_epoch=int(np.ceil(X_train.shape[0] / self.batch_size)),\n",
    "                    verbose=self.verbose,\n",
    "                )\n",
    "\n",
    "            print(\"Layer \" + str(cur_layer) + \" has been trained.\")\n",
    "\n",
    "            model_layers[cur_layer] = cur_model\n",
    "            encoder_layer = cur_model.layers[-2]\n",
    "            encoders.append(encoder_layer)\n",
    "\n",
    "\n",
    "            # train = 0 because we do not want to use dropout to get hidden node value,since is a train-only behavior,\n",
    "            # used only to learn weights. output of second layer: hidden\n",
    "            # layer(encoder layer)\n",
    "            X_train = self._get_intermediate_output(\n",
    "                cur_model,\n",
    "                X_train,\n",
    "                n_layer=2,\n",
    "                train=0,\n",
    "                n_out=self.nb_hid[cur_layer],\n",
    "                batch_size=self.batch_size)\n",
    "            assert X_train.shape[1] == self.nb_hid[cur_layer], \"Output of hidden layer not retrieved\"\n",
    "            if X_val is not None:\n",
    "                X_val = self._get_intermediate_output(\n",
    "                    cur_model,\n",
    "                    X_val,\n",
    "                    n_layer=2,\n",
    "                    train=0,\n",
    "                    n_out=self.nb_hid[cur_layer],\n",
    "                    batch_size=self.batch_size)\n",
    "        self.encoder_model = self._build_model_from_encoders(\n",
    "            encoders, dropout_all=dropout_all)\n",
    "        return model_layers\n",
    "\n",
    "    def _build_model_from_encoders(self, encoding_layers, dropout_all=False):\n",
    "        '''\n",
    "        Builds a deep NN model that generates low-dimensional representation of input, based on pretrained layers.\n",
    "        :param encoding_layers: pretrained encoder layers\n",
    "        :param dropout_all: True to include dropout layer between all layers. By default, dropout is only present for input.\n",
    "        :return model with each encoding layer as a layer of a NN\n",
    "        '''\n",
    "        input_layer = Input(shape=(encoding_layers[0].input_shape[1],))\n",
    "        dropouted = Dropout(self.dropout[0])(input_layer)\n",
    "\n",
    "        for i in range(len(encoding_layers)):\n",
    "            if i and dropout_all:\n",
    "                dropouted = Dropout(self.dropout[i])(dropouted)\n",
    "\n",
    "            encoding_layers[i].inbound_nodes = []\n",
    "            dropouted = encoding_layers[i](dropouted)\n",
    "        model = Model(input_layer, dropouted)\n",
    "        return model\n",
    "\n",
    "    def fine_tune(\n",
    "            self,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val=None,\n",
    "            y_val=None,\n",
    "            nb_classes=2,\n",
    "            patience=1,\n",
    "            final_act_fn='softmax',\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            batch_size=32,\n",
    "            nb_epoch=300,\n",
    "            verbose=1):\n",
    "        '''\n",
    "        Should be called after self.fit!\n",
    "        The self.encoder_model can be used in supervised task by adding a classification/regression layer on top.\n",
    "        Classification by fine-tuning a pre-trained encoder model for a given task.\n",
    "        :param X_train: input data (scipy sparse matrix supported). shape:(num_samples,num_features)\n",
    "        :param y_train: input data labels. class vector to be converted into a matrix(integers from 0 to num_classes).\n",
    "        :param X_val: validation data (scipy sparse matrix supported). shape:(num_samples,num_features)\n",
    "        :param y_val: validation data labels. class vector to be converted into a matrix(integers from 0 to num_classes).\n",
    "        :param nb_classes: number of classes.\n",
    "        :param patience: number of epochs with no improvement after which training will be stopped. Useful when X_val is not None.\n",
    "        :param final_act_fn: The activation function for classification. Typically 'softmax'.\n",
    "               See also keras.activations for available activation functions.\n",
    "        :param loss: The loss function for classification. Typically 'categorical_crossentropy'.\n",
    "               See also keras.losses for available loss functions.\n",
    "        :param optimizer: The optimizer to use. See also keras.optimizers.\n",
    "        :param batch_size: mini batch size for gradient update\n",
    "        :param nb_epoch: number of epochs to train.\n",
    "        :param verbose: Verbosity mode, 0, 1, or 2.\n",
    "        '''\n",
    "        if self.encoder_model is None:\n",
    "            raise ValueError('Please fit on some data first.')\n",
    "        output=Dense(nb_classes, activation=final_act_fn)(self.encoder_model.output)\n",
    "        model=Model(self.encoder_model.input,output)\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "        if X_val is not None:\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=0)\n",
    "            model.fit_generator(\n",
    "                generator=data_generator.batch_generator(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    nb_classes=nb_classes,\n",
    "                    one_hot=True),\n",
    "                steps_per_epoch=int(np.ceil(X_train.shape[0]/batch_size)),\n",
    "                callbacks=[early_stopping],\n",
    "                epochs=nb_epoch,\n",
    "                verbose=verbose,\n",
    "                validation_data=data_generator.batch_generator(\n",
    "                    X_val,\n",
    "                    y_val,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    nb_classes=nb_classes,\n",
    "                    one_hot=True),\n",
    "                validation_steps=int(np.ceil(X_val.shape[0]/batch_size)))\n",
    "        else:\n",
    "            model.fit_generator(\n",
    "                generator=data_generator.batch_generator(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    nb_classes=nb_classes,\n",
    "                    one_hot=True),\n",
    "                steps_per_epoch=int(np.ceil(X_train.shape[0]/batch_size)),\n",
    "                epochs=nb_epoch,\n",
    "                verbose=verbose)\n",
    "\n",
    "        self.fine_tuned_model=model\n",
    "\n",
    "    def predict(self, X, batch_size=32):\n",
    "        '''\n",
    "        Should be called after self.fit and self.fine_tune!\n",
    "        Generates class probability predictions for the input samples.\n",
    "        :param X: input data (scipy sparse matrix supported). shape:(num_samples,num_features)\n",
    "        :param batch_size: mini batch size for gradient update\n",
    "        :return: probability predictions for X\n",
    "        '''\n",
    "        if self.fine_tuned_model is None:\n",
    "            raise ValueError('Please fine_tune on some data first.')\n",
    "        preds=self.fine_tuned_model.predict_generator(generator=data_generator.batch_generator(\n",
    "                        X,\n",
    "                        None,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False), steps=int(np.ceil(X.shape[0] / batch_size)))\n",
    "        if preds.min() < 0. or preds.max() > 1.:\n",
    "            warnings.warn('Network returning invalid probability values. '\n",
    "                          'The last layer might not normalize predictions '\n",
    "                          'into probabilities '\n",
    "                          '(like softmax or sigmoid would).')\n",
    "        return preds\n",
    "\n",
    "    def transform(self, X, batch_size=32):\n",
    "        \"\"\"\n",
    "        Should be called after self.fit!\n",
    "        Transform the X into the dense representation of the last layer of the learned encoder model.\n",
    "        The dense representation of X can be used in some traditional models, such as LR, SVM, KNN or clustering.\n",
    "        :param X: input data (scipy sparse matrix supported). shape:(num_samples,num_features)\n",
    "        :param batch_size: mini batch size for gradient update\n",
    "        :return : The dense representation of the last layer of the learned encoder model of X.\n",
    "        \"\"\"\n",
    "        if self.encoder_model is None:\n",
    "            raise ValueError('Please fit on some data first.')\n",
    "        transformed_rep = self.encoder_model.predict_generator(\n",
    "            generator=data_generator.batch_generator(\n",
    "                X, None, batch_size=batch_size, shuffle=False), steps=int(np.ceil(X.shape[0]/batch_size)))\n",
    "\n",
    "        return transformed_rep\n",
    "\n",
    "    def fit_predict(self, Xs, Xt, X_test, Ys, Y_test):\n",
    "        ut = self.fit(Xs, Xt)\n",
    "        Xs = self.transform(Xs)\n",
    "        self.base_classifer.fit(Xs, Ys)\n",
    "        X_test = self.transform(X_test)\n",
    "        y_pred = self.base_classifer.predict(X_test)\n",
    "        acc = accuracy_score(Y_test, y_pred)\n",
    "        return acc\n",
    "\n",
    "    def _print_sda_config(self):\n",
    "        \"\"\"\n",
    "        Print the configuration of the SDA\n",
    "        \"\"\"\n",
    "        print(\"Number of layers: \" + str(self.nb_layers))\n",
    "\n",
    "        print(\"Hidden nodes: \")\n",
    "        s = ''\n",
    "        for i in range(self.nb_layers):\n",
    "            s += str(self.nb_hid[i]) + ' '\n",
    "        print(s)\n",
    "\n",
    "        print(\"Dropout: \")\n",
    "        s = ''\n",
    "        for i in range(self.nb_layers):\n",
    "            s += str(self.dropout[i]) + ' '\n",
    "        print(s)\n",
    "\n",
    "        s = ''\n",
    "        print(\"Encoder activation: \")\n",
    "        for i in range(self.nb_layers):\n",
    "            s += str(self.enc_act[i]) + ' '\n",
    "        print(s)\n",
    "\n",
    "        print(\"Decoder activation: \")\n",
    "        s = ''\n",
    "        for i in range(self.nb_layers):\n",
    "            s += str(self.dec_act[i]) + ' '\n",
    "        print(s)\n",
    "\n",
    "        print(\"Epochs: \" + str(self.nb_epoch))\n",
    "        print(\"Bias: \" + str(self.bias))\n",
    "        print(\"Loss: \" + str(self.loss_fn))\n",
    "        print(\"Batch size: \" + str(self.batch_size))\n",
    "        print(\"Optimizer: \" + str(self.optimizer))\n",
    "\n",
    "    def _assert_input(self, nb_layers, nb_hid, dropout, enc_act, dec_act):\n",
    "        '''\n",
    "        If the hidden nodes, dropout proportion, encoder activation function or decoder activation function is given, it uses the same parameter for all the layers.\n",
    "        Errors out if there is a size mismatch between number of layers and parameters for each layer.\n",
    "        '''\n",
    "\n",
    "        if len(nb_hid) == 1:\n",
    "            nb_hid = nb_hid * nb_layers\n",
    "\n",
    "        if len(dropout) == 1:\n",
    "            dropout = dropout * nb_layers\n",
    "\n",
    "        if len(enc_act) == 1:\n",
    "            enc_act = enc_act * nb_layers\n",
    "\n",
    "        if len(dec_act) == 1:\n",
    "            dec_act = dec_act * nb_layers\n",
    "\n",
    "        assert (nb_layers == len(nb_hid) == len(dropout) == len(enc_act) == len(dec_act)), \\\n",
    "            \"Please specify as many hidden nodes, dropout proportion on input, \" \\\n",
    "            \"and encoder and decoder activation function, as many layers are there, using list data structure.\"\n",
    "\n",
    "        return nb_hid, dropout, enc_act, dec_act\n",
    "\n",
    "    def _get_intermediate_output(\n",
    "            self,\n",
    "            model,\n",
    "            X_train,\n",
    "            n_layer,\n",
    "            train,\n",
    "            n_out,\n",
    "            batch_size,\n",
    "            dtype=np.float32):\n",
    "        '''\n",
    "        Returns output of a given intermediate layer in a model\n",
    "        :param model: model to get output from\n",
    "        :param X_train: sparse representation of input data\n",
    "        :param n_layer: the layer number for which output is required\n",
    "        :param train: (0/1) 1 to use training config, like dropout noise.\n",
    "        :param n_out: number of output nodes in the given layer (pre-specify so as to use generator function with sparse matrix to get layer output)\n",
    "        :param batch_size: the num of instances to convert to dense at a time\n",
    "        :return value of intermediate layer\n",
    "        '''\n",
    "        data_out = np.zeros(shape=(X_train.shape[0], n_out))\n",
    "\n",
    "        x_batch_gen = data_generator.x_generator(\n",
    "            X_train, batch_size=batch_size, shuffle=False)\n",
    "        stop_iter = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "        for i in range(stop_iter):\n",
    "            cur_batch, cur_batch_idx = next(x_batch_gen)\n",
    "            data_out[cur_batch_idx, :] = self._get_nth_layer_output(\n",
    "                model, n_layer, X=cur_batch, train=train)\n",
    "\n",
    "        return data_out.astype(dtype, copy=False)\n",
    "\n",
    "    def _get_nth_layer_output(self, model, n_layer, X, train=1):\n",
    "        '''\n",
    "        Returns output of nth layer in a given model.\n",
    "        :param model: keras model to get an intermediate value out of\n",
    "        :param n_layer: the layer number to get the value of\n",
    "        :param X: input data for which layer value should be computed and returned.\n",
    "        :param train: (1/0): 1 to use the same setting as training (for example, with Dropout, etc.), 0 to use the same setting as testing phase for the model.\n",
    "        :return the value of n_layer in the given model, input, and setting\n",
    "        '''\n",
    "        get_nth_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                          [model.layers[n_layer].output])\n",
    "        return get_nth_layer_output([X, train])[0]\n",
    "\n",
    "class data_generator(object):\n",
    "    @classmethod\n",
    "    def batch_generator(\n",
    "            cls,\n",
    "            X,\n",
    "            Y=None,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            nb_classes=2,\n",
    "            one_hot=False,\n",
    "            seed=1337):\n",
    "        '''\n",
    "        Creates batches of data from given dataset, given a batch size. Returns dense representation of sparse input.\n",
    "        :param X: input features, sparse or dense\n",
    "        :param Y: input labels, sparse or dense. If Y is None, return generated X only.\n",
    "        :param batch_size: number of instances in each batch\n",
    "        :param shuffle: If True, shuffle input instances.\n",
    "        :param nb_classes: number of classes for one-hot labels.\n",
    "        :param one_hot: Weather to transform Y to one_hot labels.\n",
    "        :param seed: fixed seed for shuffling data, for replication\n",
    "        :return batch of input features and <labels>\n",
    "        '''\n",
    "        number_of_batches = int(\n",
    "            np.ceil(\n",
    "                X.shape[0] /\n",
    "                batch_size))  # ceil function allows for creating last batch off remaining samples\n",
    "        counter = 0\n",
    "        sample_index = np.arange(X.shape[0])\n",
    "        if shuffle:\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(sample_index)\n",
    "        if Y is not None and one_hot:\n",
    "            Y = to_categorical(Y, nb_classes)\n",
    "        sparse = False\n",
    "        if scp.issparse(X):\n",
    "            sparse = True\n",
    "\n",
    "        while True:\n",
    "            batch_index = sample_index[batch_size *\n",
    "                                       counter:batch_size * (counter + 1)]\n",
    "            if sparse:\n",
    "                # converts to dense array\n",
    "                x_batch = X[batch_index, :].toarray()\n",
    "                if Y is not None:\n",
    "                    # converts to dense array\n",
    "                    y_batch = Y[batch_index, :].toarray()\n",
    "            else:\n",
    "                x_batch = X[batch_index, :]\n",
    "                if Y is not None:\n",
    "                    y_batch = Y[batch_index, :]\n",
    "            counter += 1\n",
    "            if Y is not None:\n",
    "                yield x_batch, y_batch\n",
    "            else:\n",
    "                yield x_batch\n",
    "            if counter == number_of_batches:\n",
    "                if shuffle:\n",
    "                    np.random.shuffle(sample_index)\n",
    "                counter = 0\n",
    "\n",
    "    @classmethod\n",
    "    def x_generator(cls, X, batch_size, shuffle, seed=1337):\n",
    "        '''\n",
    "        Creates batches of data from given input, given a batch size. Returns dense representation of sparse input one batch a time.\n",
    "        :param X: input features, can be sparse or dense\n",
    "        :param batch_size: number of instances in each batch\n",
    "        :param shuffle: If True, shuffle input instances.\n",
    "        :param seed: fixed seed for shuffling data, for replication\n",
    "        :return batch of input data\n",
    "        '''\n",
    "        number_of_batches = int(\n",
    "            np.ceil(\n",
    "                X.shape[0] /\n",
    "                batch_size))  # ceil function allows for creating last batch off remaining samples\n",
    "        counter = 0\n",
    "        sample_index = np.arange(X.shape[0])\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(sample_index)\n",
    "\n",
    "        sparse = False\n",
    "        if scp.issparse(X):\n",
    "            sparse = True\n",
    "\n",
    "        while counter < number_of_batches:\n",
    "            batch_index = sample_index[batch_size *\n",
    "                                       counter:batch_size * (counter + 1)]\n",
    "            if sparse:\n",
    "                # converts to dense array\n",
    "                x_batch = X[batch_index, :].toarray()\n",
    "            else:\n",
    "                x_batch = X[batch_index, :]\n",
    "            yield x_batch, batch_index\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-o40lmv2Ms3D",
    "outputId": "8220a922-bab6-4e3f-c7e7-200e45d5a233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 2\n",
      "Hidden nodes: \n",
      "100 100 \n",
      "Dropout: \n",
      "0.1 0.1 \n",
      "Encoder activation: \n",
      "tanh tanh \n",
      "Decoder activation: \n",
      "linear linear \n",
      "Epochs: 300\n",
      "Bias: True\n",
      "Loss: mse\n",
      "Batch size: 32\n",
      "Optimizer: adam\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0685e948a4fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-90e7242d7632>\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, Xs, Xt, X_test, Ys, Y_test)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[0mXs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_classifer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-90e7242d7632>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, X_val, patience, dropout_all, model_layers)\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;31m# signal)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[0mnb_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0minput_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                 \u001b[1;31m# masking input data to learn to generalize, and prevent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;31m# identity learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[0;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                              input_tensor=tensor)\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'input'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[1;34m(prefix)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "#datasets = ['K', 'D', 'B', 'E']\n",
    "datasets = ['K', 'D']\n",
    "\n",
    "results = {}\n",
    "for dataset1 in datasets:\n",
    "    for dataset2 in datasets:\n",
    "        if dataset1 == dataset2:\n",
    "            continue\n",
    "        [Xs, Ys, X_test, Y_test, Xt]=joblib.load(\n",
    "                    os.path.join(basepath, dataset1+'-'+dataset2+'.pkl'))\n",
    "        Xs = Xs.astype('float')\n",
    "        X_test = X_test.astype('float')\n",
    "        Xt = Xt.astype('float')\n",
    "        model = SDA()\n",
    "        acc = model.fit_predict(Xs, Xt, X_test, Ys, Y_test)\n",
    "        print(dataset1, dataset2, acc)\n",
    "        results[dataset1+dataset2] = acc\n",
    "\n",
    "with open(\"SDA_record.json\",'w') as json_file:\t\n",
    "\t\tjson.dump(results, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ?! HIDC matlab中存在steplen的设置，at matrixproduce\n",
    "class HIDC:\n",
    "    def __init__(self, numIdentical=20, numAlike=20, numDistinct=10, numIter=10):\n",
    "        self.numIdentical = numIdentical\n",
    "        self.numAlike = numAlike\n",
    "        self.numDistinct = numDistinct\n",
    "        self.numIter = numIter\n",
    "        return\n",
    "    \n",
    "    def fit_predict(self, Xs, Xt, Ys, Yt):\n",
    "        inputPath = 'data.mat'\n",
    "        \n",
    "        eng = matlab.engine.start_matlab()\n",
    "        sio.savemat('data.mat',{'TrainData': Xs.T, 'TrainLabel': Ys, 'TestData': Xt.T, 'TestLabel':Yt})\n",
    "        result = eng.HIDC_enterFunc(self.numIdentical, self.numAlike, self.numDistinct, self.numIter, inputPath)\n",
    "        \n",
    "        eng.exit()\n",
    "        # Y_pred = np.asarray(Y_pred)\n",
    "        # Y_pred = np.reshape(Y_pred, (Y_pred.shape[1],)) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5000)\n",
      "(2000,)\n",
      "(2000, 5000)\n",
      "(2000,)\n",
      "(5945, 5000)\n"
     ]
    }
   ],
   "source": [
    "datasets = ['K', 'D']\n",
    "\n",
    "for dataset1 in datasets:\n",
    "    for dataset2 in datasets:\n",
    "        if dataset1 == dataset2:\n",
    "            continue\n",
    "        [Xs, Ys, X_test, Y_test, Xt]=joblib.load(\n",
    "                    os.path.join(basepath, dataset1+'-'+dataset2+'.pkl'))\n",
    "\n",
    "print(Xs.shape)\n",
    "print(Ys.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "MatlabExecutionError",
     "evalue": "\n  File C:\\Users\\stuar\\OneDrive\\Documents\\CSCI115\\presentation1\\GenerativeTriTL.m, line 165, in GenerativeTriTL\n\n  File C:\\Users\\stuar\\OneDrive\\Documents\\CSCI115\\presentation1\\HIDC_enterFunc.m, line 17, in HIDC_enterFunc\nUnable to perform assignment because the size of the left side is 10-by-1 and the size of the right side is 10-by-2.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMatlabExecutionError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-71e190151625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHIDC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-dcb3369d2457>\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, Xs, Xt, Ys, Yt)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0meng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_matlab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0msio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavemat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.mat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'TrainData'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mXs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TrainLabel'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TestData'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TestLabel'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mYt\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIDC_enterFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumIdentical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumAlike\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumDistinct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumIter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0meng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matlab\\engine\\matlabengine.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             return FutureResult(self._engine(), future, nargs, _stdout,\n\u001b[1;32m---> 71\u001b[1;33m                                 _stderr, feval=True).result()\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__validate_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matlab\\engine\\futureresult.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TimeoutCannotBeNegative'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__future\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matlab\\engine\\fevalfuture.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MatlabFunctionTimeout'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetFEvalResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_future\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nargout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retrieved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMatlabExecutionError\u001b[0m: \n  File C:\\Users\\stuar\\OneDrive\\Documents\\CSCI115\\presentation1\\GenerativeTriTL.m, line 165, in GenerativeTriTL\n\n  File C:\\Users\\stuar\\OneDrive\\Documents\\CSCI115\\presentation1\\HIDC_enterFunc.m, line 17, in HIDC_enterFunc\nUnable to perform assignment because the size of the left side is 10-by-1 and the size of the right side is 10-by-2.\n"
     ]
    }
   ],
   "source": [
    "#datasets = ['K', 'D', 'B', 'E']\n",
    "datasets = ['K', 'D']\n",
    "\n",
    "results = {}\n",
    "for dataset1 in datasets:\n",
    "    for dataset2 in datasets:\n",
    "        if dataset1 == dataset2:\n",
    "            continue\n",
    "        [Xs, Ys, X_test, Y_test, Xt]=joblib.load(\n",
    "                    os.path.join(basepath, dataset1+'-'+dataset2+'.pkl'))\n",
    "        Xs = Xs.astype('float')\n",
    "        X_test = X_test.astype('float')\n",
    "        Xt = Xt.astype('float')\n",
    "        model = HIDC()\n",
    "        acc = model.fit_predict(Xs[:10], Xt[:10], Ys[:10], Y_test[:10])\n",
    "        print(dataset1, dataset2, acc)\n",
    "        results[dataset1+dataset2] = acc\n",
    "\n",
    "with open(\"HIDC_record.json\",'w') as json_file:\t\n",
    "\t\tjson.dump(results, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matlab\n",
    "import matlab.engine\n",
    "import scipy.io as sio\n",
    "\n",
    "class TriTL:\n",
    "    def __init__(self, numIdentical=20, numAlike=20, numDistinct=10, numIter=100):\n",
    "        self.numIdentical = numIdentical\n",
    "        self.numAlike = numAlike\n",
    "        self.numDistinct = numDistinct\n",
    "        self.numIter = numIter\n",
    "        return\n",
    "    \n",
    "    def fit_predict(self, Xs, Xt, Ys, Yt):\n",
    "        inputPath = 'data.mat'\n",
    "        \n",
    "        eng = matlab.engine.start_matlab()\n",
    "        sio.savemat('data.mat',{'TrainData': Xs.T, 'TrainLabel': Ys, 'TestData': Xt.T, 'TestLabel':Yt})\n",
    "        result = eng.TriTL_enterFunc(self.numIdentical, self.numAlike, self.numDistinct, self.numIter, inputPath)\n",
    "        eng.exit()\n",
    "        # Y_pred = np.asarray(Y_pred)\n",
    "        # Y_pred = np.reshape(Y_pred, (Y_pred.shape[1],)) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "MatlabExecutionError",
     "evalue": "\n  File C:\\Users\\stuar\\OneDrive\\Documents\\CSCI115\\presentation1\\TriTL.m, line 245, in TriTL\n\n  File C:\\Users\\stuar\\OneDrive\\Documents\\CSCI115\\presentation1\\TriTL_enterFunc.m, line 16, in TriTL_enterFunc\nIncorrect dimensions for matrix multiplication. Check that the number of columns in the first matrix matches the number of rows in the second matrix. To perform elementwise multiplication, use '.*'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMatlabExecutionError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-b054338bd0a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTriTL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-69b7b0d20c87>\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, Xs, Xt, Ys, Yt)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0meng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_matlab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0msio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavemat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.mat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'TrainData'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mXs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TrainLabel'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TestData'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TestLabel'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mYt\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTriTL_enterFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumIdentical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumAlike\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumDistinct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumIter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0meng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Y_pred = np.asarray(Y_pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matlab\\engine\\matlabengine.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             return FutureResult(self._engine(), future, nargs, _stdout,\n\u001b[1;32m---> 71\u001b[1;33m                                 _stderr, feval=True).result()\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__validate_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matlab\\engine\\futureresult.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TimeoutCannotBeNegative'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__future\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matlab\\engine\\fevalfuture.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MatlabFunctionTimeout'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetFEvalResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_future\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nargout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_retrieved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMatlabExecutionError\u001b[0m: \n  File C:\\Users\\stuar\\OneDrive\\Documents\\CSCI115\\presentation1\\TriTL.m, line 245, in TriTL\n\n  File C:\\Users\\stuar\\OneDrive\\Documents\\CSCI115\\presentation1\\TriTL_enterFunc.m, line 16, in TriTL_enterFunc\nIncorrect dimensions for matrix multiplication. Check that the number of columns in the first matrix matches the number of rows in the second matrix. To perform elementwise multiplication, use '.*'.\n"
     ]
    }
   ],
   "source": [
    "#datasets = ['K', 'D', 'B', 'E']\n",
    "datasets = ['K', 'D']\n",
    "\n",
    "results = {}\n",
    "for dataset1 in datasets:\n",
    "    for dataset2 in datasets:\n",
    "        if dataset1 == dataset2:\n",
    "            continue\n",
    "        [Xs, Ys, X_test, Y_test, Xt]=joblib.load(\n",
    "                    os.path.join(basepath, dataset1+'-'+dataset2+'.pkl'))\n",
    "        Xs = Xs.astype('float')\n",
    "        X_test = X_test.astype('float')\n",
    "        Xt = Xt.astype('float')\n",
    "        model = TriTL()\n",
    "        acc = model.fit_predict(Xs[:10], Xt[:10], Ys[:10], Y_test[:10])\n",
    "        print(dataset1, dataset2, acc)\n",
    "        results[dataset1+dataset2] = acc\n",
    "\n",
    "with open(\"TriTL_record.json\",'w') as json_file:\t\n",
    "\t\tjson.dump(results, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Mtrick:\n",
    "    def __init__(self, alpha=2.4, beta=2.4, numCluster=15, maxIter=200):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.numCluster = numCluster\n",
    "        self.maxIter = maxIter\n",
    "        return\n",
    "\n",
    "    def fit_predict(self, Xs, Xt, Ys):\n",
    "        inputPath = 'data.mat'\n",
    "        eng = matlab.engine.start_matlab()\n",
    "        sio.savemat('data.mat',{'TrainData': Xs.T, 'TrainLabel': Ys, 'TestData': Xt.T})\n",
    "        Y_pred = eng.MTrick_enterFunc(self.alpha, self.beta, self.numCluster, self.maxIter, inputPath)\n",
    "        eng.exit()\n",
    "        Y_pred = np.asarray(Y_pred)\n",
    "        print(Y_pred.shape)\n",
    "        Y_pred = np.reshape(Y_pred, (Y_pred.shape[1],)) \n",
    "        return Y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "K D 0.737\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "K B 0.689\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "K E 0.819\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "D K 0.753\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "D B 0.715\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "D E 0.7565\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "B K 0.802\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "B D 0.784\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "B E 0.786\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "E K 0.808\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "E D 0.7265\n",
      "(1, 2000)\n",
      "2000\n",
      "2000\n",
      "E B 0.7055\n"
     ]
    }
   ],
   "source": [
    "datasets = ['K', 'D', 'B', 'E']\n",
    "#datasets = ['K', 'D']\n",
    "\n",
    "results = {}\n",
    "for dataset1 in datasets:\n",
    "    for dataset2 in datasets:\n",
    "        if dataset1 == dataset2:\n",
    "            continue\n",
    "        [Xs, Ys, X_test, Y_test, Xt]=joblib.load(\n",
    "                    os.path.join(basepath, dataset1+'-'+dataset2+'.pkl'))\n",
    "        Xs = Xs.astype('float')\n",
    "        X_test = X_test.astype('float')\n",
    "        Xt = Xt.astype('float')\n",
    "        model = Mtrick()\n",
    "        Y_pred = model.fit_predict(Xs, X_test, Ys)\n",
    "        print(len(Y_pred))\n",
    "        print(len(Y_test))\n",
    "        acc = accuracy_score(Y_pred,Y_test)\n",
    "        print(dataset1, dataset2, acc)\n",
    "        results[dataset1+dataset2] = acc\n",
    "\n",
    "with open(\"Mtrick_record.json\",'w') as json_file:\t\n",
    "\t\tjson.dump(results, json_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Presentation1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
