{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Presentation1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qltYDOKkFYds"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import numpy.matlib\n",
        "import scipy as sp\n",
        "import scipy.io as sio \n",
        "import scipy.sparse as scp\n",
        "from scipy.sparse.linalg import svds as SVD\n",
        "\n",
        "import sklearn \n",
        "from sklearn import svm\n",
        "from sklearn import linear_model\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg4nBe8bHued",
        "outputId": "6c15695e-cf14-4029-d777-6e8788f84a70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "basepath = \"/content/gdrive/My Drive/presentation1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lyZY9WxFfo9"
      },
      "source": [
        "### Sample positive Amazon review for books"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEAqi-Z8CDJa"
      },
      "source": [
        "'woman_the:1 contains_the:1 fan_i:1 alex_ross(superman:1 justice:1 read:1 comics_fan:1 again:1 league_etc:1 fans:1 recieved:1 hanna-barbera!)_a:1 book_fans:1 wonder:1 gift:1 **gorgeous_artwork:1** gift_and:1 contains:1 i_recieved:1 artwork:2 christmas:1 read_it:1 wonder_woman:1 justice_league:1 a_comics:1 again_and:1 even:1 i_read:1 the_most:2 gorgeous:1 of_alex:1 **i:2** **extraordinary:1** most_gorgeous:1 most:2 it_again:1 comic_books:1 and_i:1 ross(superman_batman:1 etc_even:1 etc:1 the_justice:1 **fan:1** **beautiful:1** again.a:1 even_hanna-barbera!):1 comics:1 batman_wonder:1 for_comic:1 in_comic:1 artwork_in:1 books_contains:1 woman:1 a_christmas:1 extraordinary_artwork:1 books:1 christmas_gift:1 ross(superman:1 league:1 artwork_of:1 most_extraordinary:1 comic_book:1 book:1 recieved_this:1 batman:1 must-have_for:1 hanna-barbera!):1 **must-have:1** again.a_must-have:1 alex:1 and_again.a:1 comic:2 #label#:positive'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFPaADeSFlsQ"
      },
      "source": [
        "### Top 20 words by frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7HzdSR-FUG2"
      },
      "source": [
        "'i',\n",
        " 'you',\n",
        " 'not',\n",
        " 'was',\n",
        " '<num>',\n",
        " 'my',\n",
        " 'one',\n",
        " 'book',\n",
        " 'so',\n",
        " 'they',\n",
        " 'all',\n",
        " 'if',\n",
        " 'very',\n",
        " 'about',\n",
        " 'just',\n",
        " 'like',\n",
        " 'great',\n",
        " 'his',\n",
        " 'out',\n",
        " 'good',\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t15Up--rFcFP"
      },
      "source": [
        "## Spectral Feature Alignment (SFA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iU1UJIvFhsk"
      },
      "source": [
        "class SFA:\n",
        "    '''\n",
        "    spectral feature alignment\n",
        "    '''\n",
        "    def __init__(self,l=500,K=100, base_classifer=svm.SVC()):\n",
        "        self.l = l # number of domain-independent features\n",
        "        self.K = K # number of clusters\n",
        "        self.m = 0 # number of domain-specific features\n",
        "        self.ut = None # eigen-vectors from spectral decomposition\n",
        "        self.gamma = 1 # tradeoff parameter\n",
        "        self.base_classifer = base_classifer\n",
        "        self.ix = None # index of domain-independent features\n",
        "        self._ix = None # index of domain-specific features\n",
        "        return\n",
        "\n",
        "    def fit(self, Xs,Xt):\n",
        "\n",
        "        # 1. Select domain-specific and domain-dependent features\n",
        "        # _______________________________________________________\n",
        "\n",
        "        # Sort indices by highest sum of columns (highest word frequency)\n",
        "        ix_s = np.argsort(np.sum(Xs, axis=0))\n",
        "        ix_t = np.argsort(np.sum(Xt, axis=0))\n",
        "        ix_s = ix_s[::-1][:self.l]\n",
        "        ix_t = ix_t[::-1][:self.l]\n",
        "\n",
        "        # Instersect words with highest word frequency in both source and target\n",
        "        ix = np.intersect1d(ix_s, ix_t)\n",
        "\n",
        "        # Complement of previous index\n",
        "        _ix = np.setdiff1d(range(Xs.shape[1]), ix)\n",
        "        self.ix = ix # index of domain-independent features\n",
        "        self._ix = _ix # index of domain-specific features\n",
        "        self.l = len(ix) # number of domain-independent features\n",
        "        self.m = len(_ix) # number of domain-specific features\n",
        "\n",
        "        X = np.concatenate((Xs, Xt), axis=0)\n",
        "        \n",
        "        # 2. Construct co-occurrence matrix between domain specific / independent\n",
        "        #________________________________________________________\n",
        "        \n",
        "        DI = (X[:, ix]>0).astype('float') # Domain independent \n",
        "        DS = (X[:, _ix]>0).astype('float') # Domain specific\n",
        "\n",
        "        M = np.zeros((self.m,self.l))\n",
        "        for i in range(X.shape[0]):\n",
        "            tem1 = np.reshape(DS[i], (1, self.m))\n",
        "            tem2 = np.reshape(DI[i], (1, self.l))\n",
        "            M += np.matmul(tem1.T, tem2)\n",
        "        M = M/np.linalg.norm(M, 'fro')\n",
        "        M = scp.lil_matrix(M)\n",
        "\n",
        "        # 3. Create Laplacian and obtain top K eigenvectors\n",
        "        #________________________________________________________       \n",
        "        D1 = scp.lil_matrix((self.m, self.m))\n",
        "        D2 = scp.lil_matrix((self.l, self.l))\n",
        "        for i in range(self.m):\n",
        "            D1[i,i] = 1.0/np.sqrt(np.sum(M[i,:]))\n",
        "        for i in range(self.l):\n",
        "            D2[i,i] = 1.0/np.sqrt(np.sum(M[:,i]))\n",
        "        B = (D1.tocsr().dot(M.tocsr())).dot(D2.tocsr())\n",
        "        ut, s, vt = SVD(B.tocsc(), k=self.K)\n",
        "        self.ut = ut\n",
        "        return ut\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Feature alignment mapping function\n",
        "        return np.concatenate((X, self.gamma*X[:, self._ix].dot(self.ut)), axis=1)\n",
        "\n",
        "    def fit_predict(self,Xs, Xt, X_test, Ys, Y_test):\n",
        "        # Obtained tranformed features through spectral alignment\n",
        "        ut = self.fit(Xs, Xt)\n",
        "        Xs = self.transform(Xs)\n",
        "        # Build classifier with concatenated Xs features and new features\n",
        "        self.base_classifer.fit(Xs, Ys)\n",
        "        X_test = self.transform(X_test)\n",
        "        y_pred = self.base_classifer.predict(X_test)\n",
        "        acc = accuracy_score(Y_test, y_pred)\n",
        "        return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FykURd88F0j_",
        "outputId": "cccd6240-020d-42fb-dbf5-8b9ed06aa9ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "datasets = ['K', 'D', 'B', 'E']\n",
        "#datasets = ['K', 'D']\n",
        "\n",
        "results = {}\n",
        "for dataset1 in datasets:\n",
        "    for dataset2 in datasets:\n",
        "        if dataset1 == dataset2:\n",
        "            continue\n",
        "        [Xs, Ys, X_test, Y_test, Xt]=joblib.load(\n",
        "                    os.path.join(basepath, dataset1+'-'+dataset2+'.pkl'))\n",
        "        Xs = Xs.astype('float')\n",
        "        X_test = X_test.astype('float')\n",
        "        Xt = Xt.astype('float')\n",
        "        model = SFA()\n",
        "        acc = model.fit_predict(Xs, Xt, X_test, Ys, Y_test)\n",
        "        print(dataset1, dataset2, acc)\n",
        "        results[dataset1+dataset2] = acc\n",
        "\n",
        "with open(\"SFA_record.json\",'w') as json_file:\t\n",
        "\t\tjson.dump(results, json_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K D 0.723\n",
            "K B 0.7105\n",
            "K E 0.819\n",
            "D K 0.7815\n",
            "D B 0.7525\n",
            "D E 0.7585\n",
            "B K 0.787\n",
            "B D 0.7705\n",
            "B E 0.74\n",
            "E K 0.8255\n",
            "E D 0.716\n",
            "E B 0.7005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzeBOUXkHL76"
      },
      "source": [
        "## Structural Correspondence Learning (SCL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZheCaa8mHK5v"
      },
      "source": [
        "class SCL(object):\n",
        "    '''\n",
        "    class of structural correspondence learning \n",
        "    '''\n",
        "    def __init__(self, l2=1.0, num_pivots=10, base_classifer=LinearSVC()):\n",
        "        self.l2 = l2\n",
        "        self.num_pivots = num_pivots\n",
        "        self.W = 0\n",
        "        self.base_classifer = base_classifer\n",
        "        # self.train_data_dim = None\n",
        "\n",
        "    def fit(self, Xs, Xt):\n",
        "        '''\n",
        "        find pivot features and transfer the Xs and Xt\n",
        "        Param Xs: source data\n",
        "        Param Xt: target data\n",
        "        output Xs_new: new source data features\n",
        "        output Xt_new: new target data features\n",
        "        output W: transform matrix\n",
        "        '''\n",
        "        _, ds = Xs.shape\n",
        "        _, dt = Xt.shape\n",
        "        assert ds == dt\n",
        "\n",
        "        # 1. Select pivots (domain-agnostic words in both spaces)\n",
        "        # _______________________________________________________\n",
        "\n",
        "        # Look for num_pivots words with highest frequency in both Xs and Xt\n",
        "        X = np.concatenate((Xs, Xt), axis=0)\n",
        "        ix = np.argsort(np.sum(X, axis=0))\n",
        "        ix = ix[::-1][:self.num_pivots]\n",
        "        pivots = (X[:, ix]>0).astype('float')\n",
        "        p = np.zeros((ds, self.num_pivots))\n",
        "        \n",
        "        # 2. Train classifiers to \"predict\" each pivot. Create num_pivot features\n",
        "        # per pivot. We want to find correlation between pivot and the rest of data\n",
        "        # _______________________________________________________\n",
        "\n",
        "        for i in range(self.num_pivots):\n",
        "            clf = linear_model.SGDClassifier(loss=\"modified_huber\", alpha=self.l2)\n",
        "            clf.fit(X, pivots[:, i])\n",
        "            p[:, i] = clf.coef_\n",
        "\n",
        "        # 3. Obtain top num_pivots eigenvectors of p to reduce dimenstionality\n",
        "        # _______________________________________________________\n",
        "        _, W = np.linalg.eig(np.cov(p))\n",
        "        W = W[:, :self.num_pivots].astype('float')\n",
        "        self.W = W\n",
        "        Xs_new = np.concatenate((np.dot(Xs, W), Xs), axis=1)\n",
        "        Xt_new = np.concatenate((np.dot(Xt, W), Xt), axis=1)\n",
        "\n",
        "        return Xs_new, Xt_new, W\n",
        "\n",
        "    def transform(self, X):\n",
        "        '''\n",
        "        transform the original data by adding new features\n",
        "        Param X: original data\n",
        "        output x_new: X with new features\n",
        "        '''\n",
        "        X_new = np.concatenate((np.dot(X, self.W),X), axis=1)\n",
        "        return X_new\n",
        "    \n",
        "    def fit_predict(self, Xs, Xt, X_test, Ys, Y_test):\n",
        "      # Obtained tranformed features through SCL\n",
        "        self.fit(Xs, Xt)\n",
        "        Xs = self.transform(Xs)\n",
        "        # Build classifier with concatenated Xs features and new features\n",
        "        self.base_classifer.fit(Xs, Ys)\n",
        "        X_test = self.transform(X_test)\n",
        "        y_pred = self.base_classifer.predict(X_test)\n",
        "        acc = accuracy_score(Y_test, y_pred)\n",
        "        return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi-eufjsIRwk",
        "outputId": "fcae8f25-6403-4155-c9da-0bb01968f2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "datasets = ['K', 'D', 'B', 'E']\n",
        "#datasets = ['K', 'D']\n",
        "\n",
        "results = {}\n",
        "for dataset1 in datasets:\n",
        "    for dataset2 in datasets:\n",
        "        if dataset1 == dataset2:\n",
        "            continue\n",
        "        [Xs, Ys, X_test, Y_test, Xt]=joblib.load(\n",
        "                    os.path.join(basepath, dataset1+'-'+dataset2+'.pkl'))\n",
        "        Xs = Xs.astype('float')\n",
        "        X_test = X_test.astype('float')\n",
        "        Xt = Xt.astype('float')\n",
        "        model = SCL()\n",
        "        acc = model.fit_predict(Xs, Xt, X_test, Ys, Y_test)\n",
        "        print(dataset1, dataset2, acc)\n",
        "        results[dataset1+dataset2] = acc\n",
        "\n",
        "with open(\"SCL_record.json\",'w') as json_file:\t\n",
        "\t\tjson.dump(results, json_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K D 0.7225\n",
            "K B 0.6625\n",
            "K E 0.8115\n",
            "D K 0.7455\n",
            "D B 0.712\n",
            "D E 0.726\n",
            "B K 0.7515\n",
            "B D 0.763\n",
            "B E 0.737\n",
            "E K 0.8345\n",
            "E D 0.7005\n",
            "E B 0.6975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMhAZrDuIzcG"
      },
      "source": [
        " ## Marginalized Denoising Autoencoders for Domain Adaptation (mSDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRrWE9OkI-LR"
      },
      "source": [
        "class mSDA(object):\n",
        "    '''\n",
        "    Implement mSDA.\n",
        "    To read more about the SDA, check the following paper:\n",
        "        Chen M , Xu Z , Weinberger K , et al.\n",
        "        Marginalized Denoising Autoencoders for Domain Adaptation[J].\n",
        "        Computer Science, 2012.\n",
        "    This implementation of mSDA is based on both the sample code the authors provided\n",
        "    as well as the equations in the paper.\n",
        "    The code is modified according to https://github.com/douxu896/mSDA\n",
        "    '''\n",
        "    def __init__(self, p=None, l=5, act=np.tanh, Ws=None, bias=True):\n",
        "        '''\n",
        "        :param p: corruption probability\n",
        "        :param l: number of layers\n",
        "        :param act: what nonlinearity to use? if None, not to use nonlinearity.\n",
        "        :param Ws: model parameters. Can optionally pass in precomputed Ws to use to transform X.\n",
        "                (e.g. if transforming test X with Ws learned from training X)\n",
        "        :param bias: Whether to use bias?\n",
        "        '''\n",
        "        self.p = p\n",
        "        self.l = l\n",
        "        self.act = act\n",
        "        self.Ws = Ws\n",
        "        self.bias = bias\n",
        "\n",
        "    def mDA(self, X, W=None):\n",
        "        '''\n",
        "        One layer Marginalized Denoising Autoencoder.\n",
        "        Learn a representation h of X by reconstructing \"corrupted\" input but marginalizing out corruption\n",
        "        :param X: input features, shape:(num_samples,num_features)\n",
        "        :param W: model parameters. Can optionally pass in precomputed W to use to transform X.\n",
        "                (e.g. if transforming test X with W learned from training X)\n",
        "        :return: model parameters, reconstructed representation.\n",
        "        '''\n",
        "        if self.bias:\n",
        "            X=np.hstack((X, np.ones((X.shape[0], 1))))\n",
        "        if W is None:\n",
        "            W = self._compute_reconstruction_W(X)\n",
        "        h = np.dot(X, W)  # no nonlinearity\n",
        "        if self.act is not None:\n",
        "            h = self.act(h)  # inject nonlinearity\n",
        "        return W, h\n",
        "\n",
        "    def _compute_reconstruction_W(self, X):\n",
        "        '''\n",
        "        Learn reconstruction parameters.\n",
        "        :param X: input features, shape:(num_samples,num_features)\n",
        "        :return: model parameters.\n",
        "        '''\n",
        "        # typecast to correct Xtype\n",
        "        X.dtype = \"float64\"\n",
        "        d = X.shape[1]\n",
        "        # Represents the probability that a given feature will be corrupted\n",
        "        if self.bias:\n",
        "            q = np.ones(\n",
        "                (d-1, 1)) * (1 - self.p)\n",
        "            # add bias probability\n",
        "            q=np.vstack((q,1))\n",
        "        else:\n",
        "            q = np.ones(\n",
        "                (d, 1)) * (1 - self.p)\n",
        "\n",
        "        S = np.dot(X.transpose(), X)\n",
        "        Q = S * (np.dot(q, q.transpose()))\n",
        "        Q[np.diag_indices_from(Q)] = q[:,0] * np.diag(S)\n",
        "        P = S * numpy.matlib.repmat(q, 1, d)\n",
        "\n",
        "        # solve equation of the form W = BA^-1\n",
        "        A = Q + 10**-5 * np.eye(d)\n",
        "        B = P[:-1,:]\n",
        "        W = np.linalg.solve(A.transpose(), B.transpose())\n",
        "        return W\n",
        "\n",
        "    def fit(self, X):\n",
        "        '''\n",
        "        Stack mDA layers on top of each other, using previous layer as input for the next\n",
        "        :param X: input features, shape:(num_samples,num_features)\n",
        "        :return: None\n",
        "        '''\n",
        "        Ws = list()\n",
        "        hs = list()\n",
        "        hs.append(X)\n",
        "        for layer in range(0, self.l):\n",
        "            W, h = self.mDA(hs[-1])\n",
        "            Ws.append(W)\n",
        "            hs.append(h)\n",
        "        self.Ws = Ws\n",
        "\n",
        "    def transform(self, X):\n",
        "        '''\n",
        "        Should be called after fit!\n",
        "        Stack mDA layers on top of each other, using previous layer as input for the next\n",
        "        :param X: input features, shape:(num_samples,num_features)\n",
        "        :return: reconstructed representation of the last layer.\n",
        "        '''\n",
        "        if self.Ws is None:\n",
        "            raise ValueError('Please fit on some data first.')\n",
        "        hs = list()\n",
        "        hs.append(X)\n",
        "        for layer in range(0, self.l):\n",
        "            _, h = self.mDA(hs[-1], self.Ws[layer])\n",
        "            hs.append(h)\n",
        "        return hs[-1]\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        '''\n",
        "        Stack mDA layers on top of each other, using previous layer as input for the next\n",
        "        :param X: input features, shape:(num_samples,num_features)\n",
        "        :return: reconstructed representation of the last layer.\n",
        "        '''\n",
        "        Ws = list()\n",
        "        hs = list()\n",
        "        hs.append(X)\n",
        "        for layer in range(0, self.l):\n",
        "            W, h = self.mDA(hs[-1])\n",
        "            Ws.append(W)\n",
        "            hs.append(h)\n",
        "        self.Ws = Ws\n",
        "        return hs[-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZpDwheGJRQJ",
        "outputId": "2b3bd3fe-1046-4e48-e53e-84d07c2c7777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# test implementation\n",
        "\n",
        "# load dataset1\n",
        "[Xs_train, Ys_train, Xs_test, Ys_test, Xs_unlabeled]=joblib.load(\n",
        "            os.path.join(basepath, 'K-D.pkl'))\n",
        "# load dataset2\n",
        "[Xt_train, Xt_train_label, Xt_test, Xt_test_label, Xt_unlabeled]=joblib.load(\n",
        "            os.path.join(basepath, 'D-K.pkl'))\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC().fit(Xs_train, Ys_train)\n",
        "preds_Xs = clf.predict(Xs_test)\n",
        "acc = np.mean(preds_Xs == Ys_test)\n",
        "print(\"Xs acc on regular X: \", acc)\n",
        "preds_Xt = clf.predict(Xt_test)\n",
        "acc = np.mean(preds_Xt == Xt_test_label)\n",
        "print(\"Xt acc on regular X: \", acc)\n",
        "\n",
        "# set corruption probability, number of layers and bias.\n",
        "pp = 0.3\n",
        "ll = 5\n",
        "bias = True\n",
        "train_X=np.concatenate((Xs_train,Xs_unlabeled,Xt_train,Xt_unlabeled),axis=0)\n",
        "msda=mSDA(p=pp, l=ll,act=np.tanh, Ws=None, bias=True)\n",
        "msda.fit(train_X)\n",
        "Xs_reps = msda.transform(Xs_train)\n",
        "print(\"Shape of mSDA Xs_reps h: \", Xs_reps.shape)\n",
        "Xs_test_reps = msda.transform(Xs_test)\n",
        "print(\"Shape of mSDA Xs_test_reps h: \", Xs_test_reps.shape)\n",
        "Xt_reps = msda.transform(Xt_test)\n",
        "print(\"Shape of mSDA Xt_test_reps h: \", Xt_reps.shape)\n",
        "\n",
        "clf = svm.SVC().fit(Xs_reps, Ys_train)\n",
        "preds_Xs=clf.predict(Xs_test_reps)\n",
        "acc=np.mean(preds_Xs == Ys_test)\n",
        "print(\"Xs acc with linear SVM on mSDA features: \", acc)\n",
        "preds_Xt = clf.predict(Xt_reps)\n",
        "acc = np.mean(preds_Xt == Xt_test_label)\n",
        "print(\"Xt acc with linear SVM on mSDA features: \", acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xs acc on regular X:  0.711\n",
            "Xt acc on regular X:  0.9725\n",
            "Shape of mSDA Xs_reps h:  (2000, 5000)\n",
            "Shape of mSDA Xs_test_reps h:  (2000, 5000)\n",
            "Shape of mSDA Xt_test_reps h:  (2000, 5000)\n",
            "Xs acc with linear SVM on mSDA features:  0.8\n",
            "Xt acc with linear SVM on mSDA features:  0.965\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}