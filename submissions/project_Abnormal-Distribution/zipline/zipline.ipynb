{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "zipline.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nFh292Fen7jQ",
        "outputId": "9f5b3c4a-2ca1-435a-8aa5-b97947a82b9d"
      },
      "source": [
        "!pip3 install zipline==1.3.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting zipline==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/59/8c5802a7897c1095fdc409fb557f04df8f75c37174e80d2ba58c8d8a6488/zipline-1.3.0.tar.gz (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pip>=7.1.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (19.3.1)\n",
            "Requirement already satisfied: setuptools>18.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (50.3.2)\n",
            "Collecting Logbook>=0.12.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/d9/16ac346f7c0102835814cc9e5b684aaadea101560bb932a2403bd26b2320/Logbook-1.5.3.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2016.4 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.18.5)\n",
            "Collecting requests-file>=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.4.1)\n",
            "Collecting pandas<=0.22,>=0.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/c6/0936bc5814b429fddb5d6252566fe73a3e40372e6ceaf87de3dec1326f28/pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl (26.2MB)\n",
            "\u001b[K     |████████████████████████████████| 26.3MB 64.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas-datareader>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.9.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.10.2)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2.23.0)\n",
            "Requirement already satisfied: Cython>=0.25.2 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.29.21)\n",
            "Collecting cyordereddict>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/1a/364cbfd927be1b743c7f0a985a7f1f7e8a51469619f9fefe4ee9240ba210/cyordereddict-1.0.0.tar.gz (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: bottleneck>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.3.2)\n",
            "Requirement already satisfied: contextlib2>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.5.5)\n",
            "Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (4.4.2)\n",
            "Collecting networkx<2.0,>=1.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2c/e473e54afc9fae58dfa97066ef6709a7e35a1dd1c28c5a3842989322be00/networkx-1.11-py2.py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2.7.1)\n",
            "Collecting bcolz<1,>=0.12.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/8b/1ffa01f872cac36173c5eb95b58c01040d8d25f1b242c48577f4104cd3ab/bcolz-0.12.1.tar.gz (622kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (7.1.2)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.11.1)\n",
            "Collecting multipledispatch>=0.4.8\n",
            "  Downloading https://files.pythonhosted.org/packages/89/79/429ecef45fd5e4504f7474d4c3c3c4668c267be3370e4c2fd33e61506833/multipledispatch-0.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.1.1)\n",
            "Collecting Mako>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.3.20)\n",
            "Collecting alembic>=0.7.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers>=1.4.4 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2.2.2)\n",
            "Requirement already satisfied: intervaltree>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2.1.0)\n",
            "Collecting lru-dict>=1.1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/00/a5/32ed6e10246cd341ca8cc205acea5d208e4053f48a4dced2b1b31d45ba3f/lru-dict-1.1.6.tar.gz\n",
            "Collecting empyrical>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/43/1b997c21411c6ab7c96dc034e160198272c7a785aeea7654c9bcf98bec83/empyrical-0.5.5.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tables>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (3.4.4)\n",
            "Collecting trading-calendars>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/07/ea64b7acb14ca7db166d509cd43acc3548c2c2809e94730dfd9bb6546cb4/trading_calendars-2.0.0.tar.gz (102kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas-datareader>=0.2.1->zipline==1.3.0) (4.2.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->zipline==1.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->zipline==1.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->zipline==1.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->zipline==1.3.0) (2020.6.20)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Building wheels for collected packages: zipline, Logbook, cyordereddict, bcolz, lru-dict, empyrical, trading-calendars\n",
            "  Building wheel for zipline (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zipline: filename=zipline-1.3.0-cp36-cp36m-linux_x86_64.whl size=5008445 sha256=500c2c8b5a3eb5f9bf95dab049216db2c061fb0283b14fa668e45735b226ac2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/d6/67/f303ab028b004bf8e00c05b5b04fba83d8ec238b6547becdb7\n",
            "  Building wheel for Logbook (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Logbook: filename=Logbook-1.5.3-cp36-cp36m-linux_x86_64.whl size=66378 sha256=4668a32fa34fe790367f263eb61ebe4a3414ad5ed64f9f5d2573135a37c45250\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/70/07/68b99a8e05dcd1ab194a8e0ccb9e4d0ac5dd6d8d139c7149b4\n",
            "  Building wheel for cyordereddict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cyordereddict: filename=cyordereddict-1.0.0-cp36-cp36m-linux_x86_64.whl size=168126 sha256=5d514ddc3223a2923308109c460d0fa13c3cf3757757a5557ccf4560c989133d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/9d/8b/5bf3e22c1edd59b50f11bb19dec9dfcfe5a479fc7ace02b61f\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-0.12.1-cp36-cp36m-linux_x86_64.whl size=997557 sha256=66102938ab0dd4e02168563e9ff846c1bea730a53041c02f4f79616ba05f60b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/cc/1b/2cf1f88959af5d7f4d449b7fc6c9452d0ecbd86fd61a9ee376\n",
            "  Building wheel for lru-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lru-dict: filename=lru_dict-1.1.6-cp36-cp36m-linux_x86_64.whl size=25880 sha256=30cb756669757a797dab6b3ea5b54c05510e461a491b805740bf69cbbacf9390\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/ef/06/fbdd555907a7d438fb33e4c8675f771ff1cf41917284c51ebf\n",
            "  Building wheel for empyrical (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empyrical: filename=empyrical-0.5.5-cp36-none-any.whl size=39763 sha256=8b86da4445f50cf1bc900557955d1c1f410cac03302235554cc11ce0b8f8f68c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/b2/c8/6769d8444d2f2e608fae2641833110668d0ffd1abeb2e9f3fc\n",
            "  Building wheel for trading-calendars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for trading-calendars: filename=trading_calendars-2.0.0-cp36-none-any.whl size=135649 sha256=b36608b72177536048637b9779913145d2cc95eb1625aa1c04f1fdb16e111786\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/fb/89/d71a90b9dd2c51fad1b5f6d240deb0d5051e80402f9fc3a6b9\n",
            "Successfully built zipline Logbook cyordereddict bcolz lru-dict empyrical trading-calendars\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.0 has requirement pandas>=0.23, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement networkx>=2.0, but you'll have networkx 1.11 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas-datareader 0.9.0 has requirement pandas>=0.23, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Logbook, requests-file, pandas, cyordereddict, networkx, bcolz, multipledispatch, Mako, python-editor, alembic, lru-dict, empyrical, trading-calendars, zipline\n",
            "  Found existing installation: pandas 1.1.4\n",
            "    Uninstalling pandas-1.1.4:\n",
            "      Successfully uninstalled pandas-1.1.4\n",
            "  Found existing installation: networkx 2.5\n",
            "    Uninstalling networkx-2.5:\n",
            "      Successfully uninstalled networkx-2.5\n",
            "Successfully installed Logbook-1.5.3 Mako-1.1.3 alembic-1.4.3 bcolz-0.12.1 cyordereddict-1.0.0 empyrical-0.5.5 lru-dict-1.1.6 multipledispatch-0.6.0 networkx-1.11 pandas-0.22.0 python-editor-1.0.4 requests-file-1.5.1 trading-calendars-2.0.0 zipline-1.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JR-_lPDeBIC"
      },
      "source": [
        "import zipline\n",
        "\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "import sys\n",
        "\n",
        "import os\n",
        "\n",
        "from zipline.data import bundles\n",
        "from zipline.pipeline import Pipeline\n",
        "from zipline.utils.calendars import get_calendar\n",
        "from zipline.pipeline.engine import SimplePipelineEngine\n",
        "from zipline.pipeline.factors import CustomFactor, DailyReturns, AverageDollarVolume\n",
        "\n",
        "\n",
        "from zipline.pipeline.data import USEquityPricing\n",
        "from zipline.pipeline.loaders import USEquityPricingLoader\n",
        "from zipline.assets._assets import Equity\n",
        "from zipline.api import symbol"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6RtvTI4g1jy",
        "outputId": "d874dc1f-44d9-43bf-92dc-d11b1bef03aa"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUJYPG-VhIq7"
      },
      "source": [
        "zipline_dir = '/content/drive/MyDrive/abnormal-distribution-project-data/zipline'\n",
        "os.environ['ZIPLINE_ROOT'] = zipline_dir"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9lOy3owgHKY"
      },
      "source": [
        "Ingest Zipline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPFxFTLaeEDp"
      },
      "source": [
        "\n",
        "#!zipline ingest -b 'sep'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A7zrlOBHkzw"
      },
      "source": [
        "\n",
        "\n",
        "METADATA_HEADERS = ['start_date', 'end_date', 'auto_close_date',\n",
        "                    'symbol', 'exchange', 'asset_name']\n",
        "\n",
        "\n",
        "def check_for_abnormal_returns(df, thresh=3.0):\n",
        "    \"\"\"Checks to see if any days have abnormal returns\"\"\"\n",
        "    returns = df['close'].pct_change()\n",
        "    abnormal_rets = returns[returns > thresh]\n",
        "    if abnormal_rets.shape[0] > 0:\n",
        "        sys.stderr.write('Abnormal returns for: {}\\n'.format(df.ix[0]['ticker']))\n",
        "        sys.stderr.write('{}\\n'.format(str(abnormal_rets)))\n",
        "\n",
        "\n",
        "def from_sep_dump(file_name, start=None, end=None):\n",
        "    \"\"\"\n",
        "    ticker,date,open,high,low,close,volume,dividends,lastupdated\n",
        "    A,2008-01-02,36.67,36.8,36.12,36.3,1858900.0,0.0,2017-11-01\n",
        "\n",
        "    To use this make your ~/.zipline/extension.py look similar this:\n",
        "\n",
        "    from zipline.data.bundles import register\n",
        "    from alphacompiler.data.loaders.sep_quandl import from_sep_dump\n",
        "\n",
        "    register(\"sep\",\n",
        "         from_sep_dump(\"/path/to/your/SEP/dump/SHARADAR_SEP_69.csv\"),)\n",
        "\n",
        "    \"\"\"\n",
        "    us_calendar = get_calendar(\"NYSE\").all_sessions\n",
        "    ticker2sid_map = {}\n",
        "\n",
        "    def ingest(environ,\n",
        "               asset_db_writer,\n",
        "               minute_bar_writer,  # unused\n",
        "               daily_bar_writer,\n",
        "               adjustment_writer,\n",
        "               calendar,\n",
        "               cache,\n",
        "               show_progress,\n",
        "               output_dir,\n",
        "               # pass these as defaults to make them 'nonlocal' in py2\n",
        "               start=start,\n",
        "               end=end):\n",
        "\n",
        "        print(\"starting ingesting data from: {}\".format(file_name))\n",
        "\n",
        "        # read in the whole dump (will require ~7GB of RAM)\n",
        "        df = pd.read_csv(file_name, index_col='date',\n",
        "                         parse_dates=['date'], na_values=['NA'])\n",
        "\n",
        "        # drop unused columns, dividends will be used later\n",
        "        df = df.drop(['lastupdated', 'dividends', 'closeunadj'], axis=1)\n",
        "\n",
        "        # counter of valid securites, this will be our primary key\n",
        "        sec_counter = 0\n",
        "        data_list = []  # list to send to daily_bar_writer\n",
        "        metadata_list = []  # list to send to asset_db_writer (metadata)\n",
        "\n",
        "        # iterate over all the unique securities and pack data, and metadata\n",
        "        # for writing\n",
        "        for tkr, df_tkr in df.groupby('ticker'):\n",
        "            df_tkr = df_tkr.sort_index()\n",
        "\n",
        "            row0 = df_tkr.ix[0]  # get metadata from row\n",
        "\n",
        "            print(\" preparing {}\".format(row0[\"ticker\"]))\n",
        "            check_for_abnormal_returns(df_tkr)\n",
        "\n",
        "            # check to see if there are missing dates in the middle\n",
        "            this_cal = us_calendar[(us_calendar >= df_tkr.index[0]) & (us_calendar <= df_tkr.index[-1])]\n",
        "            if len(this_cal) != df_tkr.shape[0]:\n",
        "                print('MISSING interstitial dates for: %s using forward fill' % row0[\"ticker\"])\n",
        "                print('number of dates missing: {}'.format(len(this_cal) - df_tkr.shape[0]))\n",
        "                df_desired = pd.DataFrame(index=this_cal.tz_localize(None))\n",
        "                df_desired = df_desired.join(df_tkr)\n",
        "                df_tkr = df_desired.fillna(method='ffill')\n",
        "\n",
        "            # update metadata; 'start_date', 'end_date', 'auto_close_date',\n",
        "            # 'symbol', 'exchange', 'asset_name'\n",
        "            metadata_list.append((df_tkr.index[0],\n",
        "                                  df_tkr.index[-1],\n",
        "                                  df_tkr.index[-1] + pd.Timedelta(days=1),\n",
        "                                  row0[\"ticker\"],\n",
        "                                  \"SEP\",  # all have exchange = SEP\n",
        "                                  row0[\"ticker\"]  # TODO: can we delete this?\n",
        "                                  )\n",
        "                                 )\n",
        "\n",
        "            # drop metadata columns\n",
        "            df_tkr = df_tkr.drop(['ticker'], axis=1)\n",
        "\n",
        "            # pack data to be written by daily_bar_writer\n",
        "            data_list.append((sec_counter, df_tkr))\n",
        "            ticker2sid_map[tkr] = sec_counter  # record the sid for use later\n",
        "            sec_counter += 1\n",
        "\n",
        "        print(\"writing data for {} securities\".format(len(metadata_list)))\n",
        "        daily_bar_writer.write(data_list, show_progress=False)\n",
        "\n",
        "        # write metadata\n",
        "        asset_db_writer.write(equities=pd.DataFrame(metadata_list,\n",
        "                                                    columns=METADATA_HEADERS))\n",
        "        print(\"a total of {} securities were loaded into this bundle\".format(\n",
        "            sec_counter))\n",
        "\n",
        "        # read in Dividend History\n",
        "        dfd = pd.read_csv(file_name, index_col='date',\n",
        "                         parse_dates=['date'], na_values=['NA'])\n",
        "        # drop rows where dividends == 0.0\n",
        "        dfd = dfd[dfd[\"dividends\"] != 0.0]\n",
        "        dfd = dfd.dropna()\n",
        "\n",
        "        dfd.loc[:, 'ex_date'] = dfd.loc[:, 'record_date'] = dfd.index\n",
        "        dfd.loc[:, 'declared_date'] = dfd.loc[:, 'pay_date'] = dfd.index\n",
        "        dfd.loc[:, 'sid'] = dfd.loc[:, 'ticker'].apply(lambda x: ticker2sid_map[x])\n",
        "        dfd = dfd.rename(columns={'dividends': 'amount'})\n",
        "        dfd = dfd.drop(['open', 'high', 'low', 'close', 'volume', 'lastupdated', 'ticker', 'closeunadj'], axis=1)\n",
        "\n",
        "        # # format dfd to have sid\n",
        "        adjustment_writer.write(dividends=dfd)\n",
        "\n",
        "    return ingest"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR9qZO-8HO2x"
      },
      "source": [
        "\n",
        "def register_data(start_date, end_date, bundle_name, address):\n",
        "\n",
        "    start_session = pd.Timestamp(start_date, tz='utc')\n",
        "    end_session = pd.Timestamp(end_date, tz='utc')\n",
        "\n",
        "    register(bundle_name, csvdir_equities(['daily'],address,),\n",
        "    calendar_name='NYSE', start_session=start_session,\n",
        "    end_session=end_session)\n",
        "\n",
        "\n",
        "class PricingLoader(object):\n",
        "    def __init__(self, bundle_data):\n",
        "        self.loader = USEquityPricingLoader(\n",
        "            bundle_data.equity_daily_bar_reader,\n",
        "            bundle_data.adjustment_reader)\n",
        "\n",
        "    def get_loader(self, column):\n",
        "        if column not in USEquityPricing.columns:\n",
        "            raise Exception('Column not in USEquityPricing')\n",
        "        return self.loader\n",
        "\n",
        "def build_pipeline_engine(bundle_data, trading_calendar):\n",
        "    pricing_loader = PricingLoader(bundle_data)\n",
        "\n",
        "    engine = SimplePipelineEngine(\n",
        "        get_loader=pricing_loader.get_loader,\n",
        "        calendar=trading_calendar.all_sessions,\n",
        "        asset_finder=bundle_data.asset_finder)\n",
        "\n",
        "    return engine\n",
        "\n",
        "# Loading stock list from file\n",
        "def stock_list(file_name):\n",
        "    all_stocks = []\n",
        "    with open(file_name, 'r') as f:\n",
        "        for line in f:\n",
        "            # remove linebreak which is the last character of the string\n",
        "            currentPlace = line[:-1]\n",
        "            # add item to the list\n",
        "            all_stocks.append(currentPlace)\n",
        "        return all_stocks\n",
        "\n",
        "def get_universe_tickers(engine, universe, end_date):\n",
        "    universe_end_date = pd.Timestamp(end_date, tz='UTC')\n",
        "\n",
        "    universe_tickers = engine \\\n",
        "        .run_pipeline(\n",
        "        Pipeline(screen=universe),\n",
        "        universe_end_date,\n",
        "        universe_end_date) \\\n",
        "        .index.get_level_values(1) \\\n",
        "        .values.tolist()\n",
        "\n",
        "    return universe_tickers"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFLrzNPrjIM4"
      },
      "source": [
        "def get_tickers_from_bundle(bundle_name):\n",
        "    \"\"\"Gets a list of tickers from a given bundle\"\"\"\n",
        "    bundle_data = bundles.load(bundle_name, os.environ, None)\n",
        "\n",
        "    # get a list of all sids\n",
        "    lifetimes = bundle_data.asset_finder._compute_asset_lifetimes()\n",
        "    all_sids = lifetimes.sid\n",
        "\n",
        "    # retreive all assets in the bundle\n",
        "    all_assets = bundle_data.asset_finder.retrieve_all(all_sids)\n",
        "\n",
        "    # return only tickers\n",
        "    return map(lambda x: (x.symbol, x.sid), all_assets)\n",
        "\n",
        "\n",
        "def get_all_assets_for_bundle(bundle_name):\n",
        "    \"\"\"For a given bundle get a list of all assets\"\"\"\n",
        "    bundle_data = load(bundle_name, os.environ, None)\n",
        "\n",
        "    # get a list of all sids\n",
        "    lifetimes = bundle_data.asset_finder._compute_asset_lifetimes()\n",
        "    all_sids = lifetimes.sid\n",
        "\n",
        "    print('all_sids: ', all_sids)\n",
        "\n",
        "    # retreive all assets in the bundle\n",
        "    return bundle_data.asset_finder.retrieve_all(sids=all_sids)\n",
        "\n",
        "\n",
        "def get_ticker_sid_dict_from_bundle(bundle_name):\n",
        "    \"\"\"Packs the (ticker,sid) tuples into a dict.\"\"\"\n",
        "    all_equities = get_tickers_from_bundle(bundle_name)\n",
        "    return dict(all_equities)\n",
        "\n",
        "def pack_sparse_data(N, rawpath, fields, filename):\n",
        "    \"\"\"pack data into np.recarray and persists it to a file to be\n",
        "    used by SparseDataFactor\"\"\"\n",
        "\n",
        "\n",
        "    # create buffer to hold data for all tickers\n",
        "    dfs = [None] * N\n",
        "\n",
        "    max_len = -1\n",
        "    print(\"Packing sids\")\n",
        "    for fn in listdir(rawpath):\n",
        "        if not fn.endswith(\".csv\"):\n",
        "            continue\n",
        "        df = pd.read_csv(os.path.join(rawpath,fn), index_col=\"Date\", parse_dates=True)\n",
        "        df = df.sort_index()\n",
        "        sid = int(fn.split('.')[0])\n",
        "        #print(\"packing sid: %d\" % sid)\n",
        "        dfs[sid] = df\n",
        "\n",
        "        # width is max number of rows in any file\n",
        "        max_len = max(max_len, df.shape[0])\n",
        "    print(\"Finished packing sids\")\n",
        "\n",
        "    # temp workaround for `Array Index Out of Bound` bug\n",
        "    max_len = max_len + 1\n",
        "\n",
        "    # pack up data as buffer\n",
        "    num_fundamentals = len(fields)\n",
        "    buff = np.full((num_fundamentals + 1, N, max_len), np.nan)\n",
        "\n",
        "    dtypes = [('date', '<f8')]\n",
        "    for field in fields:\n",
        "        dtypes.append((field, '<f8'))\n",
        "\n",
        "    # pack self.data as np.recarray\n",
        "    data = np.recarray(shape=(N, max_len), buf=buff, dtype=dtypes)\n",
        "\n",
        "    # iterate over loaded data and populate self.data\n",
        "    for i, df in enumerate(dfs):\n",
        "        if df is None:\n",
        "            continue\n",
        "        ind_len = df.index.shape[0]\n",
        "        data.date[i, :ind_len] = df.index\n",
        "        for field in fields:\n",
        "            data[field][i, :ind_len] = df[field]\n",
        "\n",
        "    data.dump(filename)  # can be read back with np.load()\n",
        "\n",
        "\n",
        "def load_sf1(sf1_dir, fields, dimensions=None):\n",
        "    \"\"\"\n",
        "    Loads SF1 data into a npy compressed file SF1.npy\n",
        "    :param sf1_dir: Sharadar SF1 bulk file\n",
        "    :param fields: fields to load\n",
        "    :param dimensions: dimensions to load. One-to-one with fields. If None, assume ARQ if data available,\n",
        "    ART if not\n",
        "    \"\"\"\n",
        "    stocks_dir = '/content/drive/MyDrive/abnormal-distribution-project-data/stocks/dummy'\n",
        "\n",
        "    bundles.register('sep', from_sep_dump('.', '.'), )\n",
        "    num_tickers = len(get_ticker_sid_dict_from_bundle('sep'))\n",
        "    print('number of tickers: ', num_tickers)\n",
        "\n",
        "    data = pd.read_csv(sf1_dir)\n",
        "\n",
        "    tickers = get_ticker_sid_dict_from_bundle('sep')\n",
        "    \n",
        "    counter = 0\n",
        "    for ticker, sid in tickers.items():\n",
        "        counter += 1\n",
        "        if counter % 100 == 0:\n",
        "            print(\"Working on {}-th file\".format(counter))\n",
        "\n",
        "        df = data[(data.ticker == ticker)]\n",
        "        df = df.rename(columns={'datekey': 'Date'}).set_index('Date')\n",
        "        df.index = df.index.rename('Date')\n",
        "        series = []\n",
        "        for i, field in enumerate(fields):\n",
        "            if dimensions is None:\n",
        "                if df[df.dimension == 'ARQ'][field].isna().sum() == df[df.dimension == 'ARQ'].shape[0]:\n",
        "                    s = df[df.dimension == 'ART'][field]\n",
        "                else:\n",
        "                    s = df[df.dimension == 'ARQ'][field]\n",
        "            else:\n",
        "                s = df[df.dimension == dimensions[i]][field]\n",
        "            series.append(s)\n",
        "\n",
        "        df = pd.concat(series, axis=1)\n",
        "        df = df.sort_index()\n",
        "        df.index = df.index.rename('Date')\n",
        "        df.to_csv(os.path.join(stocks_dir, \"{}.csv\".format(sid)))\n",
        "    \n",
        "    pack_sparse_data(num_tickers + 1,  # number of tickers in bundle + 1\n",
        "                     stocks_dir,\n",
        "                     fields,\n",
        "                     zipline_dir + '/data/' + 'SF1.npy')  # write directly to the zipline data dir"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Kwj6nun_Yr",
        "outputId": "7005a08c-15a3-45c4-cd9f-15eb529fd41c"
      },
      "source": [
        "sf1_dir = '/content/drive/MyDrive/abnormal-distribution-project-data/stocks/SHARADAR_SF1.csv'\n",
        "fields = ['marketcap', 'assets', 'liabilities', 'pe', 'currentratio', 'netmargin', 'capex', 'fcf', 'roic']\n",
        "load_sf1(sf1_dir, fields, dimensions=None)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:94: UserWarning: Overwriting bundle with name 'sep'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of tickers:  17841\n",
            "Packing sids\n",
            "Finished packing sids\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmzRHJ829CSu"
      },
      "source": [
        "from zipline.pipeline.factors import CustomFactor\n",
        "\n",
        "\n",
        "class SparseDataFactor(CustomFactor):\n",
        "    \"\"\"Abstract Base Class to be used for computing sparse data.\n",
        "    The data is packed and persisted into a NumPy binary data file\n",
        "    in a previous step.\n",
        "    This class must be subclassed with class variable 'outputs' set.  The fields\n",
        "    in 'outputs' should match those persisted.\"\"\"\n",
        "    inputs = []\n",
        "    window_length = 1\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.time_index = None\n",
        "        self.curr_date = None # date for which time_index is accurate\n",
        "        self.data = None\n",
        "        self.data_path = \"please_specify_.npy_file\"\n",
        "\n",
        "    def bs(self, arr):\n",
        "        \"\"\"Binary Search\"\"\"\n",
        "        if len(arr) == 1:\n",
        "            if self.curr_date < arr[0]:\n",
        "                return 0\n",
        "            else: return 1\n",
        "\n",
        "        mid = int(len(arr) / 2)\n",
        "        if self.curr_date < arr[mid]:\n",
        "            return self.bs(arr[:mid])\n",
        "        else:\n",
        "            return mid + self.bs(arr[mid:])\n",
        "\n",
        "    def bs_sparse_time(self, sid):\n",
        "        \"\"\"For each security find the best range in the sparse data.\"\"\"\n",
        "        dates_for_sid = self.data.date[sid]\n",
        "        if np.isnan(dates_for_sid[0]):\n",
        "            return 0\n",
        "\n",
        "        # do a binary search of the dates array finding the index\n",
        "        # where self.curr_date will lie.\n",
        "        non_nan_dates = dates_for_sid[~np.isnan(dates_for_sid)]\n",
        "        return self.bs(non_nan_dates) - 1\n",
        "\n",
        "    def cold_start(self, today, assets):\n",
        "        if self.data is None:\n",
        "            self.data = np.load(self.data_path, allow_pickle=True)\n",
        "\n",
        "        self.M = self.data.date.shape[1]\n",
        "\n",
        "        # for each sid, do binary search of date array to find current index\n",
        "        # the results can be shared across all factors that inherit from SparseDataFactor\n",
        "        # this sets an array of ints: time_index\n",
        "        self.time_index = np.full(self.N, -1, np.dtype('int64'))\n",
        "        self.curr_date = today.value\n",
        "        for asset in assets:  # asset is numpy.int64\n",
        "            self.time_index[asset] = self.bs_sparse_time(asset)\n",
        "\n",
        "    def update_time_index(self, today, assets):\n",
        "        \"\"\"Ratchet update.\n",
        "        for each asset check if today >= dates[self.time_index]\n",
        "        if so then increment self.time_index[asset.sid] += 1\"\"\"\n",
        "\n",
        "        ind_p1 = self.time_index.copy()\n",
        "        np.add.at(ind_p1, ind_p1 != (self.M - 1), 1)\n",
        "        sids_to_increment = today.value >= self.data.date[np.arange(self.N), ind_p1]\n",
        "        sids_not_max = self.time_index != (self.M - 1)   # create mask of non-maxed\n",
        "        self.time_index[sids_to_increment & sids_not_max] += 1\n",
        "\n",
        "        self.curr_date = today.value\n",
        "\n",
        "    def compute(self, today, assets, out, *arrays):\n",
        "        # for each asset in assets determine index from date (today)\n",
        "        if self.time_index is None:\n",
        "            self.cold_start(today, assets)\n",
        "        else:\n",
        "            self.update_time_index(today, assets)\n",
        "\n",
        "        ti_used_today = self.time_index[assets]\n",
        "\n",
        "        for field in self.__class__.outputs:\n",
        "            out[field][:] = self.data[field][assets, ti_used_today]\n",
        "\n",
        "\n",
        "class Fundamentals(SparseDataFactor):\n",
        "    outputs = ['marketcap', 'assets', 'liabilities', 'pe', 'currentratio', 'netmargin', 'capex', 'fcf', 'roic']\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Fundamentals, self).__init__(*args, **kwargs)\n",
        "        self.N = len(get_ticker_sid_dict_from_bundle(\"sep\")) + 1  # max(sid)+1 get this from the bundle\n",
        "        self.data_path = zipline_dir + '/data/' + 'SF1.npy'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qMxO_-g_OZ8"
      },
      "source": [
        "data = pd.read_csv(sf1_dir)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4urcOtJ_tFI",
        "outputId": "8b000ba8-c717-4e1e-e261-632b64501228"
      },
      "source": [
        "data.columns[0:50]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ticker', 'dimension', 'calendardate', 'datekey', 'reportperiod',\n",
              "       'lastupdated', 'accoci', 'assets', 'assetsavg', 'assetsc', 'assetsnc',\n",
              "       'assetturnover', 'bvps', 'capex', 'cashneq', 'cashnequsd', 'cor',\n",
              "       'consolinc', 'currentratio', 'de', 'debt', 'debtc', 'debtnc', 'debtusd',\n",
              "       'deferredrev', 'depamor', 'deposits', 'divyield', 'dps', 'ebit',\n",
              "       'ebitda', 'ebitdamargin', 'ebitdausd', 'ebitusd', 'ebt', 'eps',\n",
              "       'epsdil', 'epsusd', 'equity', 'equityavg', 'equityusd', 'ev', 'evebit',\n",
              "       'evebitda', 'fcf', 'fcfps', 'fxusd', 'gp', 'grossmargin',\n",
              "       'intangibles'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7wJJhTv_4uC",
        "outputId": "78a096e5-b8e8-42d4-e5e1-214b57f0aee4"
      },
      "source": [
        "data.columns[50:]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['intexp', 'invcap', 'invcapavg', 'inventory', 'investments',\n",
              "       'investmentsc', 'investmentsnc', 'liabilities', 'liabilitiesc',\n",
              "       'liabilitiesnc', 'marketcap', 'ncf', 'ncfbus', 'ncfcommon', 'ncfdebt',\n",
              "       'ncfdiv', 'ncff', 'ncfi', 'ncfinv', 'ncfo', 'ncfx', 'netinc',\n",
              "       'netinccmn', 'netinccmnusd', 'netincdis', 'netincnci', 'netmargin',\n",
              "       'opex', 'opinc', 'payables', 'payoutratio', 'pb', 'pe', 'pe1',\n",
              "       'ppnenet', 'prefdivis', 'price', 'ps', 'ps1', 'receivables', 'retearn',\n",
              "       'revenue', 'revenueusd', 'rnd', 'roa', 'roe', 'roic', 'ros', 'sbcomp',\n",
              "       'sgna', 'sharefactor', 'sharesbas', 'shareswa', 'shareswadil', 'sps',\n",
              "       'tangibles', 'taxassets', 'taxexp', 'taxliabilities', 'tbvps',\n",
              "       'workingcapital'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPy1rDlShGe1"
      },
      "source": [
        "def run_pipeline(engine, pipeline, start_date, end_date):\n",
        "\n",
        "    # TODO: adjust for trading days\n",
        "    end_dt = pd.Timestamp(end_date.strftime('%Y-%m-%d'), tz='UTC')\n",
        "    start_dt = pd.Timestamp(start_date.strftime('%Y-%m-%d'), tz='UTC')\n",
        "    return engine.run_pipeline(pipeline, start_dt, end_dt)\n",
        "\n",
        "def get_pipeline_tickers(factors):\n",
        "\n",
        "    return factors.index.levels[1].values.tolist()\n",
        "\n",
        "def make_pipeline(factors, universe):\n",
        "    factors_pipe = OrderedDict()\n",
        "        \n",
        "    for name, f in factors.items():\n",
        "        factors_pipe[name] = f\n",
        "                    \n",
        "    pipe = Pipeline(screen=universe, columns=factors_pipe)\n",
        "    \n",
        "    return pipe\n",
        "\n",
        "\n",
        "def make_factors():\n",
        "    \n",
        "    all_factors = {\n",
        "        '1Y_return': DailyReturns(window_length=252)\n",
        "    }\n",
        "    \n",
        "    return all_factors"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM_spQ00HPxY",
        "outputId": "21aa64dc-2e3f-40b1-bef2-adf5e564888f"
      },
      "source": [
        "\n",
        "trading_calendar = get_calendar('NYSE') \n",
        "ingest_func = bundles.csvdir.csvdir_equities(['daily'], 'sep')\n",
        "bundles.register('sep', from_sep_dump('.'))\n",
        "bundle_data = bundles.load('sep')\n",
        "engine = build_pipeline_engine(bundle_data, trading_calendar)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Overwriting bundle with name 'sep'\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u0Hj4JM9vAZ"
      },
      "source": [
        "start_date = '2016-1-4'\n",
        "end_date = '2020-5-19'\n",
        "universe_start_date = pd.Timestamp(start_date, tz='UTC')\n",
        "universe_end_date = pd.Timestamp(end_date, tz='UTC')\n",
        "# Select universe of stocks\n",
        "universe = Fundamentals().marketcap.top(500) & AverageDollarVolume(window_length=120).top(500)\n",
        "pipeline = Pipeline(screen=universe)\n",
        "#pipeline.add(Fundamentals().marketcap, 'universe')\n",
        "all_factors = run_pipeline(engine, pipeline, universe_start_date, universe_end_date)\n",
        "# Get all tickers for the stocks we're looking at\n",
        "all_assets = get_pipeline_tickers(all_factors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R4aEhJmZ-dkk",
        "outputId": "84fc335f-5c9c-4ff6-c1cf-1408318a9778"
      },
      "source": [
        "all_factors"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"30\" valign=\"top\">2016-01-04 00:00:00+00:00</th>\n",
              "      <th>Equity(0 [A])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(28 [AAL])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(40 [AAPL])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(61 [ABBV])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(62 [ABC])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(80 [ABEV])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(119 [ABT])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(207 [ACN])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(261 [ADBE])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(277 [ADI])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(286 [ADM])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(298 [ADP])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(306 [ADS])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(309 [ADSK])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(366 [AEP])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(383 [AET])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(415 [AFL])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(481 [AGN])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(570 [AIG])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(703 [ALL])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(781 [ALXN])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(796 [AMAT])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(833 [AMGN])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(872 [AMP])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(907 [AMT])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(933 [AMZN])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(960 [ANDV])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(1013 [ANTM])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(1030 [AON])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(1044 [APA])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"30\" valign=\"top\">2020-05-19 00:00:00+00:00</th>\n",
              "      <th>Equity(16515 [UPS])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(16547 [USB])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(16645 [V])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(16654 [VALE])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(16721 [VEEV])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(16748 [VFC])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(16845 [VLO])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(16871 [VMW])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(16960 [VRSK])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(16961 [VRSN])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(16976 [VRTX])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17069 [VZ])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17070 [W])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17111 [WBA])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17154 [WDAY])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17174 [WEC])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17182 [WELL])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17202 [WFC])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17336 [WLTW])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17338 [WM])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17340 [WMB])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17354 [WMT])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17560 [XEL])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17587 [XLNX])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17604 [XOM])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17726 [YUM])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17730 [YUMC])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17746 [ZBH])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17793 [ZM])</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Equity(17827 [ZTS])</th>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>396591 rows × 0 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: [(2016-01-04 00:00:00+00:00, Equity(0 [A])), (2016-01-04 00:00:00+00:00, Equity(28 [AAL])), (2016-01-04 00:00:00+00:00, Equity(40 [AAPL])), (2016-01-04 00:00:00+00:00, Equity(61 [ABBV])), (2016-01-04 00:00:00+00:00, Equity(62 [ABC])), (2016-01-04 00:00:00+00:00, Equity(80 [ABEV])), (2016-01-04 00:00:00+00:00, Equity(119 [ABT])), (2016-01-04 00:00:00+00:00, Equity(207 [ACN])), (2016-01-04 00:00:00+00:00, Equity(261 [ADBE])), (2016-01-04 00:00:00+00:00, Equity(277 [ADI])), (2016-01-04 00:00:00+00:00, Equity(286 [ADM])), (2016-01-04 00:00:00+00:00, Equity(298 [ADP])), (2016-01-04 00:00:00+00:00, Equity(306 [ADS])), (2016-01-04 00:00:00+00:00, Equity(309 [ADSK])), (2016-01-04 00:00:00+00:00, Equity(366 [AEP])), (2016-01-04 00:00:00+00:00, Equity(383 [AET])), (2016-01-04 00:00:00+00:00, Equity(415 [AFL])), (2016-01-04 00:00:00+00:00, Equity(481 [AGN])), (2016-01-04 00:00:00+00:00, Equity(570 [AIG])), (2016-01-04 00:00:00+00:00, Equity(703 [ALL])), (2016-01-04 00:00:00+00:00, Equity(781 [ALXN])), (2016-01-04 00:00:00+00:00, Equity(796 [AMAT])), (2016-01-04 00:00:00+00:00, Equity(833 [AMGN])), (2016-01-04 00:00:00+00:00, Equity(872 [AMP])), (2016-01-04 00:00:00+00:00, Equity(907 [AMT])), (2016-01-04 00:00:00+00:00, Equity(933 [AMZN])), (2016-01-04 00:00:00+00:00, Equity(960 [ANDV])), (2016-01-04 00:00:00+00:00, Equity(1013 [ANTM])), (2016-01-04 00:00:00+00:00, Equity(1030 [AON])), (2016-01-04 00:00:00+00:00, Equity(1044 [APA])), (2016-01-04 00:00:00+00:00, Equity(1049 [APC])), (2016-01-04 00:00:00+00:00, Equity(1054 [APD])), (2016-01-04 00:00:00+00:00, Equity(1124 [APTV])), (2016-01-04 00:00:00+00:00, Equity(1216 [ARMH])), (2016-01-04 00:00:00+00:00, Equity(1339 [ASML])), (2016-01-04 00:00:00+00:00, Equity(1495 [ATVI])), (2016-01-04 00:00:00+00:00, Equity(1535 [AVB])), (2016-01-04 00:00:00+00:00, Equity(1554 [AVGO])), (2016-01-04 00:00:00+00:00, Equity(1638 [AXP])), (2016-01-04 00:00:00+00:00, Equity(1675 [AZO])), (2016-01-04 00:00:00+00:00, Equity(1688 [BA])), (2016-01-04 00:00:00+00:00, Equity(1691 [BABA])), (2016-01-04 00:00:00+00:00, Equity(1693 [BAC])), (2016-01-04 00:00:00+00:00, Equity(1752 [BAX])), (2016-01-04 00:00:00+00:00, Equity(1843 [BCR])), (2016-01-04 00:00:00+00:00, Equity(1893 [BDX])), (2016-01-04 00:00:00+00:00, Equity(1923 [BEN])), (2016-01-04 00:00:00+00:00, Equity(2023 [BHC])), (2016-01-04 00:00:00+00:00, Equity(2029 [BHI])), (2016-01-04 00:00:00+00:00, Equity(2033 [BHP])), (2016-01-04 00:00:00+00:00, Equity(2048 [BIDU])), (2016-01-04 00:00:00+00:00, Equity(2056 [BIIB])), (2016-01-04 00:00:00+00:00, Equity(2103 [BK])), (2016-01-04 00:00:00+00:00, Equity(2127 [BKNG])), (2016-01-04 00:00:00+00:00, Equity(2164 [BLK])), (2016-01-04 00:00:00+00:00, Equity(2218 [BMRN])), (2016-01-04 00:00:00+00:00, Equity(2228 [BMY])), (2016-01-04 00:00:00+00:00, Equity(2326 [BP])), (2016-01-04 00:00:00+00:00, Equity(2368 [BRCM])), (2016-01-04 00:00:00+00:00, Equity(2392 [BRK.B])), (2016-01-04 00:00:00+00:00, Equity(2476 [BSX])), (2016-01-04 00:00:00+00:00, Equity(2525 [BUD])), (2016-01-04 00:00:00+00:00, Equity(2579 [BX])), (2016-01-04 00:00:00+00:00, Equity(2586 [BXLT])), (2016-01-04 00:00:00+00:00, Equity(2589 [BXP])), (2016-01-04 00:00:00+00:00, Equity(2620 [C])), (2016-01-04 00:00:00+00:00, Equity(2660 [CAG])), (2016-01-04 00:00:00+00:00, Equity(2667 [CAH])), (2016-01-04 00:00:00+00:00, Equity(2692 [CAM])), (2016-01-04 00:00:00+00:00, Equity(2753 [CAT])), (2016-01-04 00:00:00+00:00, Equity(2778 [CB])), (2016-01-04 00:00:00+00:00, Equity(2779 [CB1])), (2016-01-04 00:00:00+00:00, Equity(2902 [CCI])), (2016-01-04 00:00:00+00:00, Equity(2912 [CCL])), (2016-01-04 00:00:00+00:00, Equity(3037 [CELG])), (2016-01-04 00:00:00+00:00, Equity(3074 [CERN])), (2016-01-04 00:00:00+00:00, Equity(3116 [CFG])), (2016-01-04 00:00:00+00:00, Equity(3244 [CHKP])), (2016-01-04 00:00:00+00:00, Equity(3296 [CHTR])), (2016-01-04 00:00:00+00:00, Equity(3311 [CI])), (2016-01-04 00:00:00+00:00, Equity(3396 [CL])), (2016-01-04 00:00:00+00:00, Equity(3517 [CLX])), (2016-01-04 00:00:00+00:00, Equity(3535 [CMCSA])), (2016-01-04 00:00:00+00:00, Equity(3545 [CME])), (2016-01-04 00:00:00+00:00, Equity(3551 [CMG])), (2016-01-04 00:00:00+00:00, Equity(3556 [CMI])), (2016-01-04 00:00:00+00:00, Equity(3676 [CNQ])), (2016-01-04 00:00:00+00:00, Equity(3741 [COF])), (2016-01-04 00:00:00+00:00, Equity(3800 [COP])), (2016-01-04 00:00:00+00:00, Equity(3822 [COST])), (2016-01-04 00:00:00+00:00, Equity(3838 [CP])), (2016-01-04 00:00:00+00:00, Equity(3977 [CRM])), (2016-01-04 00:00:00+00:00, Equity(4056 [CSCO])), (2016-01-04 00:00:00+00:00, Equity(4114 [CSX])), (2016-01-04 00:00:00+00:00, Equity(4178 [CTSH])), (2016-01-04 00:00:00+00:00, Equity(4279 [CVS])), (2016-01-04 00:00:00+00:00, Equity(4286 [CVX])), (2016-01-04 00:00:00+00:00, Equity(4315 [CX])), (2016-01-04 00:00:00+00:00, Equity(4320 [CXO])), (2016-01-04 00:00:00+00:00, Equity(4394 [D])), ...]\n",
              "\n",
              "[396591 rows x 0 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}