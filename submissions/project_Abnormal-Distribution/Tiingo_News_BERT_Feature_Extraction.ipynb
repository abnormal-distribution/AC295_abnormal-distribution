{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFRecords_TiingoNews_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9bCgHXwUz7LQ",
        "86C4-p4lR6jE",
        "lUWgbGb4v8vf",
        "VrzCdVtUtK17",
        "fdx3SkzBXNzD",
        "dfalDv7PwAix",
        "EDHE_Nd6SCy7",
        "5FKm8Sral-Nj",
        "SPWyyDZ90yFT",
        "pTccF_71wKVb",
        "P2i5mWCnbrqZ",
        "i2QaSsOR6G6b",
        "JN47CAbOwPjJ",
        "B42rdFY8wNDy",
        "on3Z22b5xi_k",
        "RL1d93i-IOOz",
        "gJo9Hzi3xCw0",
        "_zYYs3JKG3nc",
        "FXzzEEOdLyo1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch6vpxrwQdcc"
      },
      "source": [
        "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
        "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> AC295: Advanced Practical Data Science </h1>\n",
        "\n",
        "## Project: News Analytics for Stock Return Prediction\n",
        "\n",
        "**Harvard University, Fall 2020**  \n",
        "**Instructors**: Pavlos Protopapas  \n",
        "\n",
        "### **Team: $\\alpha\\beta normal$ $Distri\\beta ution$**\n",
        "#### **Roht Beri, Eduardo Peynetti, Jessica Wijaya, Stuart Neilson**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86zGynCVRsdD"
      },
      "source": [
        "## Creating TFRecords Datasets & BERT Pipeline for Tiingo News Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR4nF7VSwXQy"
      },
      "source": [
        "## Disks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swwiExQ8r2dp"
      },
      "source": [
        "### Connect Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMZ_Iph5r26F",
        "outputId": "f5a99fd5-3ca8-406f-f119-2b7dc5143275"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q89uICCXv4kc"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bCgHXwUz7LQ"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjIZStJn9NyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026f06c8-f332-4260-f22d-dd12d941839d"
      },
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install --upgrade pymongo[srv]==3.10.1\n",
        "!pip3 install dask[dataframe]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already up-to-date: pymongo[srv]==3.10.1 in /usr/local/lib/python3.6/dist-packages (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: dnspython<2.0.0,>=1.16.0; extra == \"srv\" in /usr/local/lib/python3.6/dist-packages (from pymongo[srv]==3.10.1) (1.16.0)\n",
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.6/dist-packages (2.12.0)\n",
            "Requirement already satisfied: pandas>=0.23.0; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]) (1.1.4)\n",
            "Requirement already satisfied: numpy>=1.13.0; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]) (1.18.5)\n",
            "Requirement already satisfied: partd>=0.3.10; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]) (1.1.0)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]) (0.11.1)\n",
            "Requirement already satisfied: fsspec>=0.6.0; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]) (0.8.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0; extra == \"dataframe\"->dask[dataframe]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0; extra == \"dataframe\"->dask[dataframe]) (2.8.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.6/dist-packages (from partd>=0.3.10; extra == \"dataframe\"->dask[dataframe]) (0.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0; extra == \"dataframe\"->dask[dataframe]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86C4-p4lR6jE"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXHPXTLpQZ4V"
      },
      "source": [
        "import os\n",
        "import ast\n",
        "import requests\n",
        "import tarfile\n",
        "import tempfile\n",
        "import zipfile\n",
        "import shutil\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import subprocess\n",
        "import logging\n",
        "\n",
        "import pymongo\n",
        "import bson\n",
        "import dns\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from collections import Counter\n",
        "from glob import glob\n",
        "from threading import Thread\n",
        "\n",
        "from bson import BSON, ObjectId\n",
        "from pymongo import MongoClient\n",
        "from tabulate import tabulate\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from datetime import datetime, timedelta, date, time\n",
        "\n",
        "from transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUWgbGb4v8vf"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrzCdVtUtK17"
      },
      "source": [
        "### Useful Constants and Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWTGV25dtOzH",
        "outputId": "2dd0c871-6936-45d5-d97c-e17b3081ae0b"
      },
      "source": [
        "# Set google drive path for pipeline storage\n",
        "PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/headlines/'\n",
        "TICKERS_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/components/sp1500.csv'\n",
        "IND_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/components/industries.csv'\n",
        "IND_GROUP_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/components/industry_groups.csv'\n",
        "SECTOR_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/components/sectors.csv'\n",
        "BERT_HIDDEN_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/headlines/bert_hs/'\n",
        "\n",
        "#Helpful Constants\n",
        "START = datetime(2000,1,1)\n",
        "END = datetime.now()\n",
        "WINDOW = 365\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Mongo Atlas keys & host name\n",
        "PASSWORD = '47PXdQpbJKFTLGTJ'\n",
        "DBNAME = 'abnormalDistribution'\n",
        "COLLECTION = 'tiingo'\n",
        "HOST = f'mongodb+srv://abnormal-distribution:{PASSWORD}@cluster0.friwl.mongodb.net/{DBNAME}?retryWrites=true&w=majority'\n",
        "print(HOST)\n",
        "\n",
        "# Pipeline variables\n",
        "batch_size = 512\n",
        "prefetch = AUTOTUNE"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mongodb+srv://abnormal-distribution:47PXdQpbJKFTLGTJ@cluster0.friwl.mongodb.net/abnormalDistribution?retryWrites=true&w=majority\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdx3SkzBXNzD"
      },
      "source": [
        "### Get Tickers and Sectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP2wbJgDXIB2"
      },
      "source": [
        "tickers = pd.read_csv(TICKERS_PATH).values.flatten()\n",
        "sectors = pd.read_csv(SECTOR_PATH).sector.values\n",
        "\n",
        "#industry = pd.read_csv(IND_PATH).industry.values\n",
        "#ind_group = pd.read_csv(IND_GROUP_PATH).industry_group.values\n",
        "\n",
        "tickers.sort()\n",
        "sectors.sort()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfalDv7PwAix"
      },
      "source": [
        "## TFRecords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDHE_Nd6SCy7"
      },
      "source": [
        "### Utils for Creating TFRecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TbePIRhYPDw"
      },
      "source": [
        "# The following functions can be used to convert a value to a type compatible\n",
        "# with tf.train.Example.\n",
        "# Credit: https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYEjBDr1w1Ye"
      },
      "source": [
        "# Function to extract news from Mongo\n",
        "def extract_news(news_collection, ticker='AAPL', tags = False,\n",
        "                 fields={'description': 1, '_id': 1}):\n",
        "    \n",
        "    if not tags:\n",
        "        result = news_collection.find(\n",
        "            {'tickers': ticker.lower()}, \n",
        "            fields, no_cursor_timeout=True)\n",
        "    else:\n",
        "        result = news_collection.find(\n",
        "            {'tickers': { '$eq': [] }, 'tags': ticker}, \n",
        "            fields, no_cursor_timeout=True)\n",
        "    \n",
        "    return result "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3b4DDhYQLs-"
      },
      "source": [
        "# Creates shard for the given ticker\n",
        "def thread_tfrecord(ticker, tags, fields,\n",
        "                    collection, shard_path, \n",
        "                    tokenizer):\n",
        "    \n",
        "    # Extract news from mongo\n",
        "    results = extract_news(\n",
        "        collection,\n",
        "        ticker, \n",
        "        tags,\n",
        "        fields\n",
        "    )\n",
        "\n",
        "    # TFRecord writer initialized\n",
        "    with tf.io.TFRecordWriter(shard_path) as writer:\n",
        "\n",
        "        print(f\"Creating TFRecords for {ticker}\")\n",
        "\n",
        "        # Tokenize the record and write to TFRecords\n",
        "        for item in tqdm(results):\n",
        "\n",
        "            id = item['_id'].binary\n",
        "\n",
        "            description = item['description']\n",
        "            if description==\"\" or description==None or description==[]:\n",
        "                continue\n",
        "            description_token = tokenizer.encode_plus(\n",
        "                description, \n",
        "                add_special_tokens = True, # add [CLS], [SEP]\n",
        "                max_length = 512, # max length of the text that can go to BERT (<=512)\n",
        "                padding='max_length',\n",
        "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "                truncation='longest_first',\n",
        "                return_tensors=\"tf\"\n",
        "            )\n",
        "            description_input = description_token['input_ids'].numpy().tostring()\n",
        "            description_type = description_token['token_type_ids'].numpy().tostring()\n",
        "            description_attention = description_token['attention_mask'].numpy().tostring()\n",
        "\n",
        "            # Create tf.train.Example\n",
        "            feature={\n",
        "                '_id' : _bytes_feature(id),\n",
        "                'input_ids': _bytes_feature(description_input),\n",
        "                'token_type_ids': _bytes_feature(description_type),\n",
        "                'attention_mask': _bytes_feature(description_attention), \n",
        "            }\n",
        "            features=tf.train.Features(feature=feature)\n",
        "            example = tf.train.Example(features=features)\n",
        "\n",
        "            # Wrtie the TFRecord\n",
        "            writer.write(example.SerializeToString())\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o36jlGgUhVQU"
      },
      "source": [
        "# Function to create TFRecords for the dataset\n",
        "def create_TFRecords(tickers = ['AAPL'], tags=False ,host=HOST, \n",
        "                     dbname=DBNAME, collection=COLLECTION, path=PATH):\n",
        "    \"\"\"\n",
        "    Creates a TFRecords file whose elements are (_id, description)\n",
        "    \"\"\"\n",
        "    #Setup connection to Mongo\n",
        "    client = pymongo.MongoClient(host=host)\n",
        "    db = client[dbname]\n",
        "    news_collection = db[collection]\n",
        "\n",
        "    # Number of Shards\n",
        "    num_shards = len(tickers)\n",
        "\n",
        "    if not os.path.exists(path + 'tf_records_new'):\n",
        "        os.mkdir(path + 'tf_records_new')\n",
        "        os.mkdir(path + 'tf_records_new/tags')\n",
        "        os.mkdir(path + 'tf_records_new/tickers')  \n",
        "\n",
        "    if not tags:\n",
        "        done_path = path + 'tf_records_new/tickers/*.records'\n",
        "        shard_path = path + \"tf_records_new/tickers/{}.records\"\n",
        "    else:\n",
        "        done_path = path + 'tf_records_new/tags/*.records'\n",
        "        shard_path = path + \"tf_records_new/tags/{}.records\"\n",
        "\n",
        "    # Files alread done   \n",
        "    records_done = []\n",
        "    records_done.extend(glob(done_path))\n",
        "\n",
        "    #Initalize BERT Tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
        "\n",
        "    # Data fields to be extracted\n",
        "    fields={'description': 1, '_id': 1}\n",
        "\n",
        "    threads = [[] for _ in range(1000)]\n",
        "    i = 0\n",
        "\n",
        "    # Create TF Records for \n",
        "    for j in trange(num_shards):\n",
        "        \n",
        "        # Create the new file path\n",
        "        file_path = shard_path.format(tickers[j])\n",
        "\n",
        "        # Create the thread if the file does not exist\n",
        "        if file_path not in records_done:\n",
        "            \n",
        "            # Counter for running maximum 5 threads at a time\n",
        "            i += 1 if len(threads[i]) >= 5 else 0\n",
        "\n",
        "            # Create thread for creation of ticker/tag TFRecords\n",
        "            threads[i].append(\n",
        "                Thread(\n",
        "                    target=thread_tfrecord,\n",
        "                    args = (\n",
        "                        tickers[j], tags, fields, \n",
        "                        news_collection, file_path, \n",
        "                        tokenizer\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "    \n",
        "    # Run the threads\n",
        "    for thread in threads:\n",
        "        for inner in thread:\n",
        "            inner.start()\n",
        "        for inner in thread:\n",
        "            inner.join()\n",
        "\n",
        "    client.close()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FKm8Sral-Nj"
      },
      "source": [
        "### Create TFRecords Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef1qgJ1HrCog"
      },
      "source": [
        "# Create TFRecords for Stocks\n",
        "create_TFRecords(tickers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7fFgW3xZiIR"
      },
      "source": [
        "# Create TFRecords for Sectors\n",
        "create_TFRecords(sectors, tags=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPWyyDZ90yFT"
      },
      "source": [
        "### Read TFRecords Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3KZJCV9Y2zi"
      },
      "source": [
        "# Check data integrity\n",
        "tick_path = PATH + 'tf_records_new/tickers/*.records'\n",
        "tag_path = PATH + 'tf_records_new/tags/*.records'\n",
        "\n",
        "files = glob(tag_path) + glob(tick_path)\n",
        "i = 0\n",
        "for file in tqdm(files):\n",
        "    raw_dataset = tf.data.TFRecordDataset(file)\n",
        "    try:\n",
        "        for raw_record in raw_dataset.take(10):\n",
        "            example = tf.train.Example()\n",
        "            example.ParseFromString(raw_record.numpy())\n",
        "            example = ObjectId(\n",
        "                example.features.feature['_id'].bytes_list.value[0]\n",
        "            )\n",
        "        del raw_dataset\n",
        "    except:\n",
        "        os.remove(file)\n",
        "        print('removed file: ', file)\n",
        "        i += 1\n",
        "\n",
        "if i:\n",
        "    print(f\"{i} files were deleted, please rerun Create TFRecords\")\n",
        "else:\n",
        "    print(\"TFRecords are in good shape\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbEY61HinbFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "184f19b2-8e6f-4762-cc0d-1c7fd0b748a5"
      },
      "source": [
        "filenames = ['/content/drive/MyDrive/abnormal-distribution-project-data/headlines/tf_records_new/tickers/ALGN.records']\n",
        "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
        "raw_dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TFRecordDatasetV2 shapes: (), types: tf.string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IilIiY6Id29q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee06fda0-22db-469e-b015-cae7f70d1b96"
      },
      "source": [
        "for raw_record in raw_dataset.take(1):\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(raw_record.numpy())\n",
        "    print(example)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features {\n",
            "  feature {\n",
            "    key: \"_id\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"_\\274\\206\\370\\204i\\031\\330\\300[\\\"Z\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    key: \"attention_mask\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    key: \"input_ids\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"e\\000\\000\\000?\\030\\000\\000\\312\\014\\000\\000\\362\\003\\000\\000\\2577\\000\\000\\374H\\000\\0003\\010\\000\\000\\316\\007\\000\\000\\024\\016\\000\\000\\265Z\\000\\000\\251\\016\\000\\000\\314\\013\\000\\000\\327\\007\\000\\000\\314\\007\\000\\000P0\\000\\000\\276\\013\\000\\0000\\010\\000\\000\\005\\032\\000\\000\\315\\007\\000\\000id\\000\\000\\236\\013\\000\\000\\366\\013\\000\\000\\364\\003\\000\\000f\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    key: \"token_type_ids\"\n",
            "    value {\n",
            "      bytes_list {\n",
            "        value: \"\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhRN5pKG3so2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b5b682-fe87-4b86-9a0f-01e3035a18d6"
      },
      "source": [
        "# Create a dictionary describing the features.\n",
        "features={\n",
        "        '_id': tf.io.FixedLenFeature([], tf.string), \n",
        "        'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "        'token_type_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "        'attention_mask': tf.io.FixedLenFeature([], tf.string), \n",
        "    }\n",
        "\n",
        "def _parse_image_function(example_proto):\n",
        "  # Parse the input tf.train.Example proto using the dictionary above.\n",
        "  return tf.io.parse_single_example(example_proto, features)\n",
        "\n",
        "parsed_image_dataset = raw_dataset.map(_parse_image_function)\n",
        "parsed_image_dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: {_id: (), attention_mask: (), input_ids: (), token_type_ids: ()}, types: {_id: tf.string, attention_mask: tf.string, input_ids: tf.string, token_type_ids: tf.string}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaURptBM5TwT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafa99b8-53a0-4ead-b37f-b2f42e32fd72"
      },
      "source": [
        "i = 0\n",
        "for data in parsed_image_dataset:\n",
        "    input_ids = tf.io.decode_raw(data['input_ids'], tf.int32)\n",
        "    print(ObjectId(data['_id'].numpy()))\n",
        "    print(input_ids.numpy()[:10])\n",
        "    print(input_ids.numpy().shape)\n",
        "    break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5fbc86f8846919d8c05b225a\n",
            "[  101  6207  3274  1010 14255 18684  2099  1998  3604 23221]\n",
            "(512,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTccF_71wKVb"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2i5mWCnbrqZ"
      },
      "source": [
        "### Utils for BERT Pieline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2XAP5JubqaO"
      },
      "source": [
        "# Function to parse data features\n",
        "def _parse_features_function(example):\n",
        "    # Parse the input tf.train.Example proto using the dictionary above.\n",
        "    tf_records_features = {\n",
        "        '_id': tf.io.FixedLenFeature([], tf.string), \n",
        "        'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "        'token_type_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "        'attention_mask': tf.io.FixedLenFeature([], tf.string), \n",
        "    }\n",
        "    return tf.io.parse_single_example(example, tf_records_features)\n",
        "\n",
        "\n",
        "# Structure the data for training\n",
        "def structure_data(data):\n",
        "    id = data['_id']\n",
        "    input_ids = tf.io.decode_raw(data['input_ids'], tf.int32)\n",
        "    attention_mask = tf.io.decode_raw(data['attention_mask'], tf.int32)\n",
        "    token_type_ids = tf.io.decode_raw(data['token_type_ids'], tf.int32)\n",
        "\n",
        "    return ((input_ids, token_type_ids, attention_mask), id)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBJLiX7UqCw3"
      },
      "source": [
        "# Function to build pipeline\n",
        "def build_pipeline(ticker=None, tags=False):\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_deterministic = True\n",
        "\n",
        "    if tags:\n",
        "        tfrecords_pattern_path = (\n",
        "            PATH + f\"tf_records_new/tags/{ticker}.records\"\n",
        "        )\n",
        "    else:\n",
        "        tfrecords_pattern_path = (\n",
        "            PATH + f\"tf_records_new/tickers/{ticker}.records\"\n",
        "        )\n",
        "    \n",
        "    train_files = tf.io.matching_files(tfrecords_pattern_path)\n",
        "    train_shards = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "\n",
        "    train = train_shards.interleave(tf.data.TFRecordDataset, cycle_length=1)\n",
        "    train = train.with_options(options)\n",
        "    train = train.map(_parse_features_function, num_parallel_calls=AUTOTUNE)\n",
        "    train = train.map(structure_data, num_parallel_calls=AUTOTUNE)\n",
        "    train = train.batch(batch_size)\n",
        "    #train = train.cache().prefetch(prefetch)\n",
        "\n",
        "    return train"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SuPvmX1fJHW"
      },
      "source": [
        "def generate_multiple_pipelines(start, num_tickers=12, tags=False, \n",
        "                                tickers=tickers, sectors=sectors):\n",
        "    data = []\n",
        "\n",
        "    if tags:\n",
        "        start = min(len(sectors), start)\n",
        "        stop = min(len(sectors), start+num_tickers)\n",
        "        tickers = sectors[start: stop]\n",
        "    else:\n",
        "        start = min(len(tickers), start)\n",
        "        stop = min(len(tickers), start+num_tickers)\n",
        "        tickers = tickers[start: stop]\n",
        "    \n",
        "    print(\"Generating Pipeline....\")\n",
        "\n",
        "    for ticker in tqdm(tickers):\n",
        "        data.append(build_pipeline(ticker, tags))\n",
        "    \n",
        "    return data, tickers"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2QaSsOR6G6b"
      },
      "source": [
        "### Test Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU9ZEjJgxzLb"
      },
      "source": [
        "tags_data, tags = generate_multiple_pipelines(0, tags=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbWq3mbbfsyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b76dfc-aea4-467f-e732-483a9906a90e"
      },
      "source": [
        "for item in tags_data[0].take(1):\n",
        "    print('answer: ', item[1][0].numpy())\n",
        "    print()\n",
        "    print('input_ids: ',item[0][0][0].numpy().shape)\n",
        "print(tags)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "answer:  b'_\\xbc\\x94^\\x84i\\x19\\xd8\\xc0d\\x1e\\x96'\n",
            "\n",
            "input_ids:  (512,)\n",
            "['Basic Materials' 'Communication Services' 'Consumer Cyclical'\n",
            " 'Consumer Defensive' 'Energy' 'Financial Services' 'Healthcare'\n",
            " 'Industrials' 'Misc' 'Real Estate' 'Technology' 'Utilities']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7CgHg7UlSud"
      },
      "source": [
        "tickers_data, ticks = generate_multiple_pipelines(0, tags=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-el3_E97lSud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e545b09-16a5-465a-a92e-7b3d3bba591d"
      },
      "source": [
        "for item in tickers_data[0].take(1):\n",
        "    print('answer: ', item[1][0].numpy())\n",
        "    print()\n",
        "    print('input_ids: ',item[0][0][0].numpy().shape)\n",
        "print(ticks)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "answer:  b'_\\xbc\\x81\\xe8\\x84i\\x19\\xd8\\xc0Z\\xbd0'\n",
            "\n",
            "input_ids:  (512,)\n",
            "['A' 'AA' 'AAAGY' 'AAL' 'AAMRQ' 'AAP' 'AAPL' 'AAXN' 'AB' 'ABB' 'ABBV'\n",
            " 'ABC']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN47CAbOwPjJ"
      },
      "source": [
        "## BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B42rdFY8wNDy"
      },
      "source": [
        "### Utils to build model to get BERT Hidden Layers & BERT Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_phW9OwF_RwC"
      },
      "source": [
        "# Build Model to get hidden layers\n",
        "def get_BERT_hidden():\n",
        "\n",
        "    # Inputs layers\n",
        "    input_ids = layers.Input(shape=(512,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(512,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(512,), dtype=tf.int32)\n",
        "\n",
        "    # BERT model\n",
        "    bert = TFBertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "\n",
        "    # BERT is not trainable\n",
        "    bert.trainable = False\n",
        "\n",
        "    # BERT output\n",
        "    question = bert(\n",
        "        input_ids, \n",
        "        token_type_ids=token_type_ids, \n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    # Pooled Output\n",
        "    output = layers.Flatten()(question[1])\n",
        "\n",
        "    # Build the model\n",
        "    model = Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask], \n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Blmd-z6Ddm_"
      },
      "source": [
        "# Build Model to get sentiment\n",
        "def get_BERT_sentiment():\n",
        "    \n",
        "    # Inputs layers\n",
        "    input_ids = layers.Input(shape=(512,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(512,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(512,), dtype=tf.int32)\n",
        "    \n",
        "    # BERT classification model\n",
        "    bert = TFBertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased', \n",
        "        return_dict=True,\n",
        "        output_hidden_states=True\n",
        "    )\n",
        "\n",
        "    # BERT is not trainable\n",
        "    bert.trainable = False\n",
        "    \n",
        "    # BERT output\n",
        "    logits = bert(\n",
        "        input_ids, \n",
        "        token_type_ids=token_type_ids, \n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    # Apply softmax as BERT output is logits\n",
        "    output = layers.Softmax()(logits.logits)\n",
        "\n",
        "    # Build the model\n",
        "    model = Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask], \n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgko-htY7aRc"
      },
      "source": [
        "# Function for threading model\n",
        "def thread_model(model_hs, model_sentiment, \n",
        "                 input, id, path_results):\n",
        "\n",
        "    pred_y_hs = model_hs.predict(input)\n",
        "    pred_y_sent = model_sentiment.predict(input)\n",
        "    \n",
        "    temp_df = pd.DataFrame(\n",
        "        {'_id': id, \n",
        "         'bert_features': list(pred_y_hs),\n",
        "         'bert_sentiment': list(pred_y_sent[:,1])\n",
        "         }\n",
        "    )\n",
        "    \n",
        "    temp_df.to_csv(path_results, index=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHRYgNHP1X4w"
      },
      "source": [
        "# Function to generate and save BERT Representations\n",
        "def generate_BERT_features(start, num_tickers=100, tags=False, tickers=tickers, \n",
        "                           sectors=sectors, path=BERT_HIDDEN_PATH):\n",
        "    K.clear_session()\n",
        "\n",
        "    model_hs = get_BERT_hidden()\n",
        "    model_sentiment = get_BERT_sentiment()\n",
        "\n",
        "    if not os.path.exists(path+'tags'):\n",
        "        os.mkdir(path+'tags')\n",
        "    if not os.path.exists(path+'tickers'):\n",
        "        os.mkdir(path+'tickers')\n",
        "\n",
        "    if tags:\n",
        "        path = path + 'tags/'\n",
        "        path_results = path + '{}/{}_bert_tiingo.csv'\n",
        "    else:\n",
        "        path = path + 'tickers/'\n",
        "        path_results = path + '{}/{}_bert_tiingo.csv'\n",
        "\n",
        "    train, ticks = generate_multiple_pipelines(start, num_tickers, tags)\n",
        "\n",
        "    # Run prediction loop for each ticker\n",
        "    for i, tick in tqdm(enumerate(ticks)):\n",
        "        \n",
        "        # Create directories if they don't exist\n",
        "        if not os.path.exists(path+tick):\n",
        "            os.mkdir(path+tick)\n",
        "\n",
        "        # Initalize batch number & Threads\n",
        "        batch_num = 0\n",
        "        threads = []\n",
        "\n",
        "        print('Processing: ', tick)\n",
        "        # Run predictor for each batch\n",
        "        for input, id in tqdm(train[i]):\n",
        "\n",
        "            # File path to save prediction outcomes\n",
        "            temp_path = path_results.format(tick, batch_num)\n",
        "\n",
        "            # Thread prediction only if batch prediction doesn't exists\n",
        "            if not os.path.exists(temp_path):\n",
        "                threads.append(\n",
        "                    Thread(\n",
        "                        target=thread_model, \n",
        "                        args = (model_hs, model_sentiment, input, id, temp_path)\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # Update batch number\n",
        "            batch_num += 1\n",
        "\n",
        "            # Run maximum of 2 threads at a time\n",
        "            if len(threads) == 2:\n",
        "                for thread in threads:\n",
        "                    thread.start()\n",
        "                for thread in threads:\n",
        "                    thread.join()\n",
        "                threads = []\n",
        "\n",
        "        # Run remaining threads if any\n",
        "        if threads != []:\n",
        "            for thread in threads:\n",
        "                thread.start()\n",
        "            for thread in threads:\n",
        "                thread.join()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on3Z22b5xi_k"
      },
      "source": [
        "### Get and Save BERT Hidden Layers & Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts7x9_m021qE"
      },
      "source": [
        "# BERT Features for Stocks\n",
        "generate_BERT_features(start=0, num_tickers=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_1krrqkuey"
      },
      "source": [
        "# BERT Features for Sectors\n",
        "generate_BERT_features(start=0, tags=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL1d93i-IOOz"
      },
      "source": [
        "## MongoDB Update with BERT Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJo9Hzi3xCw0"
      },
      "source": [
        "### Utils to Update MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtOmHmCNd6J6"
      },
      "source": [
        "def data_update(news_collection, file, new_path):\n",
        "\n",
        "    # Read the data\n",
        "    df = pd.read_csv(file)\n",
        "    \n",
        "    # Get the number of updates\n",
        "    count = df.shape[0]\n",
        "\n",
        "    # Update the data and get results\n",
        "    df = df.apply(\n",
        "        lambda x: news_collection.update_one(\n",
        "            {\"_id\": ObjectId(ast.literal_eval(x['_id']))},\n",
        "            {\"$set\": \n",
        "             {\n",
        "                 \"bert_HS\": list(map(float, x['bert_features'].strip('[]').split())), \n",
        "                 \"bert_sentiment\": x[\"bert_sentiment\"] \n",
        "             }\n",
        "            }).matched_count,\n",
        "        axis=1\n",
        "    ).sum()\n",
        "    \n",
        "    # Check if update was successful and copy the DataRecords to processed folder\n",
        "    if df==count:\n",
        "        shutil.copy2(file, new_path)\n",
        "    \n",
        "    return df==count"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m7DlNrlbN9Z"
      },
      "source": [
        "def update_mongo(start=0, num_tickers=100, tags=False, tickers=tickers, \n",
        "                 sectors=sectors, host=HOST, dbname=DBNAME, \n",
        "                 collection=COLLECTION, path=BERT_HIDDEN_PATH):\n",
        "\n",
        "    # Create Directory to move files which have been processed\n",
        "    if not os.path.exists(path+'tags/processed'):\n",
        "        os.mkdir(path+'tags/processed')\n",
        "    if not os.path.exists(path+'tickers/processed'):\n",
        "        os.mkdir(path+'tickers/processed')\n",
        "\n",
        "    # Set the paths and the tickers\n",
        "    if tags:\n",
        "        path = path + 'tags/'\n",
        "        path_data = path + '{}/*_bert_tiingo.csv'\n",
        "        path_results = path + 'processed/{}'\n",
        "        interim_path = path + '{}/{}_bert_tiingo.csv'\n",
        "        start = min(len(sectors), start)\n",
        "        stop = min(len(sectors), start+num_tickers)\n",
        "        tickers = sectors[start: stop]\n",
        "    else:\n",
        "        path = path + 'tickers/'\n",
        "        path_data = path + '{}/*_bert_tiingo.csv'\n",
        "        path_results = path + 'processed/{}'\n",
        "        interim_path = path + '{}/{}_bert_tiingo.csv'\n",
        "        start = min(len(tickers), start)\n",
        "        stop = min(len(tickers), start+num_tickers)\n",
        "        tickers = tickers[start: stop]\n",
        "\n",
        "    #Setup connection to Mongo\n",
        "    client = pymongo.MongoClient(host=host)\n",
        "    db = client[dbname]\n",
        "    news_collection = db[collection]\n",
        "\n",
        "    print('Processing Tickers.....')\n",
        "    # Create threads for data update\n",
        "    for tick in tqdm(tickers):\n",
        "        # Set temp path to ticker data\n",
        "        temp_path = path_data.format(tick)\n",
        "        temp_path_results = path_results.format(tick)\n",
        "        if not os.path.exists(temp_path_results):\n",
        "            os.mkdir(temp_path_results)\n",
        "        \n",
        "        # Get the files already processed\n",
        "        processed_files = glob(temp_path_results + '/*_bert_tiingo.csv')\n",
        "        processed_files = [\n",
        "            file[:-16].lstrip(path_results[:-2]).lstrip(tick+'/')\n",
        "            for file in processed_files\n",
        "        ]\n",
        "        processed_files = [\n",
        "            interim_path.format(tick, file) for file in processed_files\n",
        "        ]\n",
        "\n",
        "        threads = []\n",
        "        print(\"Generating Threads for: \", tick)\n",
        "        for file in tqdm(glob(temp_path)):\n",
        "\n",
        "            # Check if file alread processed\n",
        "            if not file in processed_files:\n",
        "                # Generate Thread\n",
        "                threads.append(\n",
        "                    Thread(\n",
        "                        target=data_update, \n",
        "                        args=(news_collection, file, temp_path_results)\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                if len(threads) >= 50:\n",
        "                    for thread in threads:\n",
        "                        thread.start()\n",
        "                    for thread in threads:\n",
        "                        thread.join()\n",
        "                    threads = []\n",
        "        \n",
        "        if len(threads) > 0:\n",
        "            for thread in threads:\n",
        "                thread.start()\n",
        "            for thread in threads:\n",
        "                thread.join()\n",
        "    \n",
        "    # Create Index in mongoDb on \n",
        "    news_collection.create_index(\"bert_sentiment\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zYYs3JKG3nc"
      },
      "source": [
        "### Update MongoDB with Hidden Layers and Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwN1wS6nvc82"
      },
      "source": [
        "# Update Mongo with sector BERT features\n",
        "update_mongo(start=0, tags=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho-LbdxWmhAZ"
      },
      "source": [
        "# Update Mongo with stock BERT features\n",
        "update_mongo(start=0, num_tickers=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9BFFgPrLnKQ"
      },
      "source": [
        "## Mongo BERT Feature Extraction for Time Series Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXzzEEOdLyo1"
      },
      "source": [
        "### Mongo Feature Extraction Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e79LTMyFLxcm"
      },
      "source": [
        "def get_BERT_features_from_mongo(\n",
        "    ticker, start=None, end=None, tags=False,\n",
        "    fields={\n",
        "        \"publishedDate\": 1, \n",
        "        \"bert_hs\": 1, \n",
        "        \"bert_sentiment\": 1, \n",
        "        \"_id\": 0\n",
        "    },\n",
        "    count=False, path=BERT_HIDDEN_PATH, host=HOST, \n",
        "    dbname=DBNAME, collection=COLLECTION):\n",
        "    \n",
        "    # Setup the dates\n",
        "    if start==None:\n",
        "        start = datetime(1900,1,1).isoformat()\n",
        "    else:\n",
        "        start = start.isoformat()\n",
        "\n",
        "    if end==None:\n",
        "        end = datetime.now().isoformat()\n",
        "    else:\n",
        "        end = end.isoformat()\n",
        "\n",
        "    #Setup connection to Mongo\n",
        "    client = pymongo.MongoClient(host=host)\n",
        "    db = client[dbname]\n",
        "    news_collection = db[collection]\n",
        "\n",
        "    # Construct Query\n",
        "    if tags:\n",
        "        query = {\n",
        "            \"tickers\": [], \"tags\": ticker.lower(),\n",
        "            \"publishedDate\": {\"$gte\": start, \"$lte\": end},\n",
        "            \"bert_sentiment\": { \"$exists\": True }\n",
        "        }\n",
        "    else:\n",
        "        query = {\n",
        "            \"tickers\": ticker.lower(), \n",
        "            \"publishedDate\": {\"$gte\": start, \"$lte\": end},\n",
        "            \"bert_sentiment\": { \"$exists\": True }\n",
        "        }\n",
        "    \n",
        "    # Run Query\n",
        "    if count:\n",
        "        data = news_collection.count_documents(query)\n",
        "    else:\n",
        "        data = news_collection.find(query,fields).sort(\"publishedDate\")\n",
        "\n",
        "    return data"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0qTGpIb9x9K"
      },
      "source": [
        "### MongoDB BERT Features Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raoK6DOzoTzi",
        "outputId": "b50002c0-3274-4e53-8f86-402e528519ca"
      },
      "source": [
        "get_BERT_features_from_mongo(\"AAPL\", count=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "220521"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLI-Uda7O6TH"
      },
      "source": [
        "## Scrap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0jKLgTj2rCw"
      },
      "source": [
        "client = pymongo.MongoClient(HOST)\n",
        "db = client[DBNAME]\n",
        "news_collection = db[COLLECTION]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36BiLDcu4F_r"
      },
      "source": [
        "start = datetime(1900,1,1).isoformat()\n",
        "end = datetime.now().isoformat()\n",
        "query = {\n",
        "    \"tickers\": 'aapl',\n",
        "    \"publishedDate\": {\"$gte\": start, \"$lte\": end},\n",
        "    \"bert_sentiment\": { \"$exists\": True }\n",
        "}\n",
        "data = news_collection.count_documents(query)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h9TTT1D4iHS",
        "outputId": "4f1bc37d-6127-44c8-9e75-4337011e4907"
      },
      "source": [
        "data"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "220521"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gV5mk-j9sQG"
      },
      "source": [
        "start = datetime(1900,1,1).isoformat()\n",
        "end = datetime.now().isoformat()\n",
        "query = {\n",
        "    \"tickers\": 'aapl',\n",
        "    \"publishedDate\": {\"$gte\": start, \"$lte\": end},\n",
        "    \"bert_sentiment\": { \"$exists\": False }\n",
        "}\n",
        "data = news_collection.count_documents(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUPwjD5Z4joJ",
        "outputId": "5f290df3-2187-44e0-8a66-1e08eb02e70d"
      },
      "source": [
        "data"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12700"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBUEigne8v39"
      },
      "source": [
        "start = datetime(1900,1,1).isoformat()\n",
        "end = datetime.now().isoformat()\n",
        "query = {\n",
        "    \"bert_sentiment\": { \"$exists\": True }\n",
        "}\n",
        "data = news_collection.count_documents(query)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlDfbwgF7e6W"
      },
      "source": [
        "# news_collection.create_index(\"bert_sentiment\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}