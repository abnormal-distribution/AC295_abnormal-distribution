{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "zipline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SKokiCXp9Nc4",
        "fAiGrv85A5CD",
        "__Qxmxwc_u_q",
        "ucIJTE9GB0wZ"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bHqSKcH9CZi"
      },
      "source": [
        "# Stock Data Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKokiCXp9Nc4"
      },
      "source": [
        "## 0. Install Zipline\n",
        "\n",
        "You will need to restart your runtime after installing since zipline uses an older version of pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nFh292Fen7jQ",
        "outputId": "91413330-1346-4f01-f665-5755c3fc0071"
      },
      "source": [
        "!pip3 install zipline==1.3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting zipline==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/59/8c5802a7897c1095fdc409fb557f04df8f75c37174e80d2ba58c8d8a6488/zipline-1.3.0.tar.gz (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pip>=7.1.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (19.3.1)\n",
            "Requirement already satisfied: setuptools>18.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (50.3.2)\n",
            "Collecting Logbook>=0.12.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/d9/16ac346f7c0102835814cc9e5b684aaadea101560bb932a2403bd26b2320/Logbook-1.5.3.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2016.4 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.18.5)\n",
            "Collecting requests-file>=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.4.1)\n",
            "Collecting pandas<=0.22,>=0.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/c6/0936bc5814b429fddb5d6252566fe73a3e40372e6ceaf87de3dec1326f28/pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl (26.2MB)\n",
            "\u001b[K     |████████████████████████████████| 26.3MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas-datareader>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.9.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.10.2)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2.23.0)\n",
            "Requirement already satisfied: Cython>=0.25.2 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.29.21)\n",
            "Collecting cyordereddict>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/1a/364cbfd927be1b743c7f0a985a7f1f7e8a51469619f9fefe4ee9240ba210/cyordereddict-1.0.0.tar.gz (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: bottleneck>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.3.2)\n",
            "Requirement already satisfied: contextlib2>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.5.5)\n",
            "Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (4.4.2)\n",
            "Collecting networkx<2.0,>=1.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2c/e473e54afc9fae58dfa97066ef6709a7e35a1dd1c28c5a3842989322be00/networkx-1.11-py2.py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2.7.1)\n",
            "Collecting bcolz<1,>=0.12.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/8b/1ffa01f872cac36173c5eb95b58c01040d8d25f1b242c48577f4104cd3ab/bcolz-0.12.1.tar.gz (622kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (7.1.2)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (0.11.1)\n",
            "Collecting multipledispatch>=0.4.8\n",
            "  Downloading https://files.pythonhosted.org/packages/89/79/429ecef45fd5e4504f7474d4c3c3c4668c267be3370e4c2fd33e61506833/multipledispatch-0.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.1.1)\n",
            "Collecting Mako>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (1.3.20)\n",
            "Collecting alembic>=0.7.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers>=1.4.4 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2.3.0)\n",
            "Requirement already satisfied: intervaltree>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (2.1.0)\n",
            "Collecting lru-dict>=1.1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/00/a5/32ed6e10246cd341ca8cc205acea5d208e4053f48a4dced2b1b31d45ba3f/lru-dict-1.1.6.tar.gz\n",
            "Collecting empyrical>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/43/1b997c21411c6ab7c96dc034e160198272c7a785aeea7654c9bcf98bec83/empyrical-0.5.5.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tables>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from zipline==1.3.0) (3.4.4)\n",
            "Collecting trading-calendars>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/07/ea64b7acb14ca7db166d509cd43acc3548c2c2809e94730dfd9bb6546cb4/trading_calendars-2.0.0.tar.gz (102kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas-datareader>=0.2.1->zipline==1.3.0) (4.2.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->zipline==1.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->zipline==1.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->zipline==1.3.0) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->zipline==1.3.0) (3.0.4)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Building wheels for collected packages: zipline, Logbook, cyordereddict, bcolz, lru-dict, empyrical, trading-calendars\n",
            "  Building wheel for zipline (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zipline: filename=zipline-1.3.0-cp36-cp36m-linux_x86_64.whl size=5008459 sha256=e90e51f6e07ce11bcc45976e074e649de3b4e68d682d1860b0cfa6de73b75f7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/d6/67/f303ab028b004bf8e00c05b5b04fba83d8ec238b6547becdb7\n",
            "  Building wheel for Logbook (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Logbook: filename=Logbook-1.5.3-cp36-cp36m-linux_x86_64.whl size=66381 sha256=ab00212a28e8ca519182e362f77a59df53b624e08ce3ed69b38c2db7c0540d03\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/70/07/68b99a8e05dcd1ab194a8e0ccb9e4d0ac5dd6d8d139c7149b4\n",
            "  Building wheel for cyordereddict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cyordereddict: filename=cyordereddict-1.0.0-cp36-cp36m-linux_x86_64.whl size=168122 sha256=8df8c50edb9d5a19d8076fff3d997ec9a01fcf1cd8e86665d17bcce1b2b9c76f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/9d/8b/5bf3e22c1edd59b50f11bb19dec9dfcfe5a479fc7ace02b61f\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-0.12.1-cp36-cp36m-linux_x86_64.whl size=996130 sha256=49ed79a4334f33d02be502c7bd62ced356fdb77cec476631f52a4bd978290674\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/cc/1b/2cf1f88959af5d7f4d449b7fc6c9452d0ecbd86fd61a9ee376\n",
            "  Building wheel for lru-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lru-dict: filename=lru_dict-1.1.6-cp36-cp36m-linux_x86_64.whl size=25872 sha256=e4f510dddbf6329937b83058488cbb7e9d6fc5927d5d8eff11b6d4a2d8ce0b7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/ef/06/fbdd555907a7d438fb33e4c8675f771ff1cf41917284c51ebf\n",
            "  Building wheel for empyrical (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empyrical: filename=empyrical-0.5.5-cp36-none-any.whl size=39763 sha256=4c0ea60cefe3433b49612ae1399692067879d45704fc0d6b7800db5e8c72a8da\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/b2/c8/6769d8444d2f2e608fae2641833110668d0ffd1abeb2e9f3fc\n",
            "  Building wheel for trading-calendars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for trading-calendars: filename=trading_calendars-2.0.0-cp36-none-any.whl size=135649 sha256=2061431c6af13d78d808b2d4eb8b534119acd4a700f2d019da2872df20924cd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/fb/89/d71a90b9dd2c51fad1b5f6d240deb0d5051e80402f9fc3a6b9\n",
            "Successfully built zipline Logbook cyordereddict bcolz lru-dict empyrical trading-calendars\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.0 has requirement pandas>=0.23, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement networkx>=2.0, but you'll have networkx 1.11 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas-datareader 0.9.0 has requirement pandas>=0.23, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Logbook, requests-file, pandas, cyordereddict, networkx, bcolz, multipledispatch, Mako, python-editor, alembic, lru-dict, empyrical, trading-calendars, zipline\n",
            "  Found existing installation: pandas 1.1.4\n",
            "    Uninstalling pandas-1.1.4:\n",
            "      Successfully uninstalled pandas-1.1.4\n",
            "  Found existing installation: networkx 2.5\n",
            "    Uninstalling networkx-2.5:\n",
            "      Successfully uninstalled networkx-2.5\n",
            "Successfully installed Logbook-1.5.3 Mako-1.1.3 alembic-1.4.3 bcolz-0.12.1 cyordereddict-1.0.0 empyrical-0.5.5 lru-dict-1.1.6 multipledispatch-0.6.0 networkx-1.11 pandas-0.22.0 python-editor-1.0.4 requests-file-1.5.1 trading-calendars-2.0.0 zipline-1.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQHkne-N9gBi"
      },
      "source": [
        "## 1. Load libraries data directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JR-_lPDeBIC"
      },
      "source": [
        "import zipline\n",
        "\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "import os\n",
        "\n",
        "from zipline.data import bundles\n",
        "from zipline.pipeline import Pipeline\n",
        "from zipline.utils.calendars import get_calendar\n",
        "from zipline.pipeline.engine import SimplePipelineEngine\n",
        "from zipline.pipeline.factors import CustomFactor, DailyReturns, AverageDollarVolume, Returns\n",
        "\n",
        "\n",
        "from zipline.pipeline.data import USEquityPricing\n",
        "from zipline.pipeline.loaders import USEquityPricingLoader\n",
        "from zipline.assets._assets import Equity\n",
        "from zipline.api import symbol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6RtvTI4g1jy",
        "outputId": "1f0ba1fc-458c-4152-da0c-a871821082eb"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUJYPG-VhIq7"
      },
      "source": [
        "# Zipline root directory\n",
        "zipline_dir = '/content/drive/MyDrive/abnormal-distribution-project-data/zipline'\n",
        "\n",
        "os.environ['ZIPLINE_ROOT'] = zipline_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atGCZ9_--6zU"
      },
      "source": [
        "## 2. Ingestion and raw data processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAiGrv85A5CD"
      },
      "source": [
        "### 2.1 Ingestion and Data Processing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A7zrlOBHkzw"
      },
      "source": [
        "# Portions of this code adapted from https://github.com/pbharrin/alpha-compiler\n",
        "\n",
        "\n",
        "METADATA_HEADERS = ['start_date', 'end_date', 'auto_close_date',\n",
        "                    'symbol', 'exchange', 'asset_name']\n",
        "\n",
        "\n",
        "def check_for_abnormal_returns(df, thresh=3.0):\n",
        "    \"\"\"Checks to see if any days have abnormal returns\"\"\"\n",
        "    returns = df['close'].pct_change()\n",
        "    abnormal_rets = returns[returns > thresh]\n",
        "    if abnormal_rets.shape[0] > 0:\n",
        "        sys.stderr.write('Abnormal returns for: {}\\n'.format(df.ix[0]['ticker']))\n",
        "        sys.stderr.write('{}\\n'.format(str(abnormal_rets)))\n",
        "\n",
        "\n",
        "def from_sep_dump(file_name, start=None, end=None):\n",
        "    \"\"\"\n",
        "    Function that reads full Sharadar stock price csv file\n",
        "    sharadar_sep.csv, and returns an ingest function for Zipline\n",
        "\n",
        "    \"\"\"\n",
        "    us_calendar = get_calendar(\"NYSE\").all_sessions\n",
        "    ticker2sid_map = {}\n",
        "\n",
        "    def ingest(environ,\n",
        "               asset_db_writer,\n",
        "               minute_bar_writer,  # unused\n",
        "               daily_bar_writer,\n",
        "               adjustment_writer,\n",
        "               calendar,\n",
        "               cache,\n",
        "               show_progress,\n",
        "               output_dir,\n",
        "               # pass these as defaults to make them 'nonlocal' in py2\n",
        "               start=start,\n",
        "               end=end):\n",
        "\n",
        "        print(\"starting ingesting data from: {}\".format(file_name))\n",
        "\n",
        "        # read in the whole dump (will require ~7GB of RAM)\n",
        "        df = pd.read_csv(file_name, index_col='date',\n",
        "                         parse_dates=['date'], na_values=['NA'])\n",
        "\n",
        "        # drop unused columns, dividends will be used later\n",
        "        df = df.drop(['lastupdated', 'dividends', 'closeunadj'], axis=1)\n",
        "\n",
        "        # counter of valid securites, this will be our primary key\n",
        "        sec_counter = 0\n",
        "        data_list = []  # list to send to daily_bar_writer\n",
        "        metadata_list = []  # list to send to asset_db_writer (metadata)\n",
        "\n",
        "        # iterate over all the unique securities and pack data, and metadata\n",
        "        # for writing\n",
        "        for tkr, df_tkr in df.groupby('ticker'):\n",
        "            df_tkr = df_tkr.sort_index()\n",
        "\n",
        "            row0 = df_tkr.ix[0]  # get metadata from row\n",
        "\n",
        "            print(\" preparing {}\".format(row0[\"ticker\"]))\n",
        "            check_for_abnormal_returns(df_tkr)\n",
        "\n",
        "            # check to see if there are missing dates in the middle\n",
        "            this_cal = us_calendar[(us_calendar >= df_tkr.index[0]) & (us_calendar <= df_tkr.index[-1])]\n",
        "            if len(this_cal) != df_tkr.shape[0]:\n",
        "                print('MISSING interstitial dates for: %s using forward fill' % row0[\"ticker\"])\n",
        "                print('number of dates missing: {}'.format(len(this_cal) - df_tkr.shape[0]))\n",
        "                df_desired = pd.DataFrame(index=this_cal.tz_localize(None))\n",
        "                df_desired = df_desired.join(df_tkr)\n",
        "                df_tkr = df_desired.fillna(method='ffill')\n",
        "\n",
        "            # update metadata; 'start_date', 'end_date', 'auto_close_date',\n",
        "            # 'symbol', 'exchange', 'asset_name'\n",
        "            metadata_list.append((df_tkr.index[0],\n",
        "                                  df_tkr.index[-1],\n",
        "                                  df_tkr.index[-1] + pd.Timedelta(days=1),\n",
        "                                  row0[\"ticker\"],\n",
        "                                  \"SEP\",  # all have exchange = SEP\n",
        "                                  row0[\"ticker\"]  # TODO: can we delete this?\n",
        "                                  )\n",
        "                                 )\n",
        "\n",
        "            # drop metadata columns\n",
        "            df_tkr = df_tkr.drop(['ticker'], axis=1)\n",
        "\n",
        "            # pack data to be written by daily_bar_writer\n",
        "            data_list.append((sec_counter, df_tkr))\n",
        "            ticker2sid_map[tkr] = sec_counter  # record the sid for use later\n",
        "            sec_counter += 1\n",
        "\n",
        "        print(\"writing data for {} securities\".format(len(metadata_list)))\n",
        "        daily_bar_writer.write(data_list, show_progress=False)\n",
        "\n",
        "        # write metadata\n",
        "        asset_db_writer.write(equities=pd.DataFrame(metadata_list,\n",
        "                                                    columns=METADATA_HEADERS))\n",
        "        print(\"a total of {} securities were loaded into this bundle\".format(\n",
        "            sec_counter))\n",
        "\n",
        "        # read in Dividend History\n",
        "        dfd = pd.read_csv(file_name, index_col='date',\n",
        "                         parse_dates=['date'], na_values=['NA'])\n",
        "        # drop rows where dividends == 0.0\n",
        "        dfd = dfd[dfd[\"dividends\"] != 0.0]\n",
        "        dfd = dfd.dropna()\n",
        "\n",
        "        dfd.loc[:, 'ex_date'] = dfd.loc[:, 'record_date'] = dfd.index\n",
        "        dfd.loc[:, 'declared_date'] = dfd.loc[:, 'pay_date'] = dfd.index\n",
        "        dfd.loc[:, 'sid'] = dfd.loc[:, 'ticker'].apply(lambda x: ticker2sid_map[x])\n",
        "        dfd = dfd.rename(columns={'dividends': 'amount'})\n",
        "        dfd = dfd.drop(['open', 'high', 'low', 'close', 'volume', 'lastupdated', 'ticker', 'closeunadj'], axis=1)\n",
        "\n",
        "        # # format dfd to have sid\n",
        "        adjustment_writer.write(dividends=dfd)\n",
        "\n",
        "    return ingest\n",
        "\n",
        "def get_tickers_from_bundle(bundle_name):\n",
        "    \"\"\"Gets a list of tickers from a given bundle\"\"\"\n",
        "    bundle_data = bundles.load(bundle_name, os.environ, None)\n",
        "\n",
        "    # get a list of all sids\n",
        "    lifetimes = bundle_data.asset_finder._compute_asset_lifetimes()\n",
        "    all_sids = lifetimes.sid\n",
        "\n",
        "    # retreive all assets in the bundle\n",
        "    all_assets = bundle_data.asset_finder.retrieve_all(all_sids)\n",
        "\n",
        "    # return only tickers\n",
        "    return map(lambda x: (x.symbol, x.sid), all_assets)\n",
        "\n",
        "\n",
        "def get_all_assets_for_bundle(bundle_name):\n",
        "    \"\"\"For a given bundle get a list of all assets\"\"\"\n",
        "    bundle_data = load(bundle_name, os.environ, None)\n",
        "\n",
        "    # get a list of all sids\n",
        "    lifetimes = bundle_data.asset_finder._compute_asset_lifetimes()\n",
        "    all_sids = lifetimes.sid\n",
        "\n",
        "    print('all_sids: ', all_sids)\n",
        "\n",
        "    # retreive all assets in the bundle\n",
        "    return bundle_data.asset_finder.retrieve_all(sids=all_sids)\n",
        "\n",
        "\n",
        "def get_ticker_sid_dict_from_bundle(bundle_name):\n",
        "    \"\"\"Packs the (ticker,sid) tuples into a dict.\"\"\"\n",
        "    all_equities = get_tickers_from_bundle(bundle_name)\n",
        "    return dict(all_equities)\n",
        "\n",
        "def pack_sparse_data(N, rawpath, fields, filename):\n",
        "    \"\"\"pack data into np.recarray and persists it to a file to be\n",
        "    used by SparseDataFactor\"\"\"\n",
        "\n",
        "\n",
        "    # create buffer to hold data for all tickers\n",
        "    dfs = [None] * N\n",
        "\n",
        "    max_len = -1\n",
        "    print(\"Packing sids\")\n",
        "    for fn in listdir(rawpath):\n",
        "        if not fn.endswith(\".csv\"):\n",
        "            continue\n",
        "        df = pd.read_csv(os.path.join(rawpath,fn), index_col=\"Date\", parse_dates=True)\n",
        "        df = df.sort_index()\n",
        "        sid = int(fn.split('.')[0])\n",
        "        #print(\"packing sid: %d\" % sid)\n",
        "        dfs[sid] = df\n",
        "\n",
        "        # width is max number of rows in any file\n",
        "        max_len = max(max_len, df.shape[0])\n",
        "    print(\"Finished packing sids\")\n",
        "\n",
        "    # temp workaround for `Array Index Out of Bound` bug\n",
        "    max_len = max_len + 1\n",
        "\n",
        "    # pack up data as buffer\n",
        "    num_fundamentals = len(fields)\n",
        "    buff = np.full((num_fundamentals + 1, N, max_len), np.nan)\n",
        "\n",
        "    dtypes = [('date', '<f8')]\n",
        "    for field in fields:\n",
        "        dtypes.append((field, '<f8'))\n",
        "\n",
        "    # pack self.data as np.recarray\n",
        "    data = np.recarray(shape=(N, max_len), buf=buff, dtype=dtypes)\n",
        "\n",
        "    # iterate over loaded data and populate self.data\n",
        "    for i, df in enumerate(dfs):\n",
        "        if df is None:\n",
        "            continue\n",
        "        ind_len = df.index.shape[0]\n",
        "        data.date[i, :ind_len] = df.index\n",
        "        for field in fields:\n",
        "            data[field][i, :ind_len] = df[field]\n",
        "\n",
        "    data.dump(filename)  # can be read back with np.load()\n",
        "\n",
        "\n",
        "def load_sf1(sf1_dir, \n",
        "             fields, \n",
        "             bund ='sep', \n",
        "             npy_name='SF1', \n",
        "             stocks_dir = '/content/drive/MyDrive/abnormal-distribution-project-data/stocks/dummy',\n",
        "             dimensions=None):\n",
        "    \"\"\"\n",
        "    Loads SF1 data into a npy compressed file SF1.npy\n",
        "    :param sf1_dir: Sharadar SF1 bulk file\n",
        "    :param fields: fields to load\n",
        "    :param dimensions: dimensions to load. One-to-one with fields. If None, assume ARQ if data available,\n",
        "    ART if not\n",
        "    \"\"\"\n",
        "\n",
        "    bundles.register(bund, from_sep_dump('.', '.'), )\n",
        "    num_tickers = len(get_ticker_sid_dict_from_bundle(bund))\n",
        "    print('number of tickers: ', num_tickers)\n",
        "\n",
        "    data = pd.read_csv(sf1_dir)\n",
        "\n",
        "    tickers = get_ticker_sid_dict_from_bundle(bund)\n",
        "    \n",
        "    counter = 0\n",
        "    for ticker, sid in tickers.items():\n",
        "        counter += 1\n",
        "        if counter % 100 == 0:\n",
        "            print(\"Working on {}-th file\".format(counter))\n",
        "\n",
        "        df = data[(data.ticker == ticker)]\n",
        "        df = df.rename(columns={'datekey': 'Date'}).set_index('Date')\n",
        "        df.index = df.index.rename('Date')\n",
        "        series = []\n",
        "        for i, field in enumerate(fields):\n",
        "            if dimensions is None:\n",
        "                if df[df.dimension == 'ARQ'][field].isna().sum() == df[df.dimension == 'ARQ'].shape[0]:\n",
        "                    s = df[df.dimension == 'ART'][field]\n",
        "                else:\n",
        "                    s = df[df.dimension == 'ARQ'][field]\n",
        "            else:\n",
        "                s = df[df.dimension == dimensions[i]][field]\n",
        "            series.append(s)\n",
        "\n",
        "        df = pd.concat(series, axis=1)\n",
        "        df = df.sort_index()\n",
        "        df.index = df.index.rename('Date')\n",
        "        df.to_csv(os.path.join(stocks_dir, \"{}.csv\".format(sid)))\n",
        "    \n",
        "    pack_sparse_data(num_tickers + 1,  # number of tickers in bundle + 1\n",
        "                     stocks_dir,\n",
        "                     fields,\n",
        "                     zipline_dir + '/data/' + npy_name +  '.npy')  # write directly to the zipline data dir\n",
        "\n",
        "SECTOR_CODING = {'Technology': 0,\n",
        "                 'Industrials': 1,\n",
        "                 'Energy': 2,\n",
        "                 'Utilities': 3,\n",
        "                 'Consumer Cyclical': 4,\n",
        "                 'Healthcare': 5,\n",
        "                 'Financial Services': 6,\n",
        "                 'Basic Materials': 7,\n",
        "                 'Consumer Defensive': 8,\n",
        "                 'Real Estate': 9,\n",
        "                 'Communication Services': 10,\n",
        "                 np.nan: -1}  # a few tickers are missing sectors, these should be ignored\n",
        "\n",
        "EXCHANGE_CODING = {'NYSE': 0,\n",
        "                   'NASDAQ': 1,\n",
        "                   'NYSEMKT': 2,  # previously AMEX\n",
        "                   'OTC': 3,\n",
        "                   'NYSEARCA': 4,\n",
        "                   'BATS': 5}\n",
        "\n",
        "\n",
        "def load_static(filepath):\n",
        "    \"\"\"Stores static items to a persisted np array.\n",
        "    The following static fields are currently persisted.\n",
        "    -Sector\n",
        "    -exchange\n",
        "    -industry: GICS\n",
        "    \"\"\"\n",
        "    bundles.register('sep', int, )\n",
        "\n",
        "    df = pd.read_csv(filepath, index_col=\"ticker\")\n",
        "    df = df[df.exchange != 'None']\n",
        "    df = df[df.exchange != 'INDEX']\n",
        "    df = df[df.table == 'SEP']\n",
        "\n",
        "    coded_sectors_for_ticker = df['sector'].map(SECTOR_CODING)\n",
        "    coded_exchange_for_ticker = df['exchange'].map(EXCHANGE_CODING)\n",
        "    coded_industry_for_ticker = df['siccode'].fillna(-1).astype('int')\n",
        "\n",
        "    ae_d = get_ticker_sid_dict_from_bundle('sep')\n",
        "    N = max(ae_d.values()) + 1\n",
        "\n",
        "    # create 2-D array to hold data where index = SID\n",
        "    static_data = np.full((3, N), -1, np.dtype('int64'))\n",
        "\n",
        "    # iterate over Assets in the bundle, and fill in static fields\n",
        "    print('Creating static data')\n",
        "    for ticker, sid in ae_d.items():\n",
        "        #print(ticker, sid, coded_sectors_for_ticker.get(ticker, -1))\n",
        "        static_data[0, sid] = coded_sectors_for_ticker.get(ticker, -1)\n",
        "        static_data[1, sid] = coded_exchange_for_ticker.get(ticker, -1)\n",
        "        static_data[2, sid] = coded_industry_for_ticker.get(ticker, -1)\n",
        "    print('Finished creating static data')\n",
        "\n",
        "    # finally save the file to disk\n",
        "    np.save(zipline_dir + '/data/' + \"SHARDAR_static.npy\", static_data)\n",
        "\n",
        "\n",
        "class SparseDataFactor(CustomFactor):\n",
        "    \"\"\"Abstract Base Class to be used for computing sparse data.\n",
        "    The data is packed and persisted into a NumPy binary data file\n",
        "    in a previous step.\n",
        "    This class must be subclassed with class variable 'outputs' set.  The fields\n",
        "    in 'outputs' should match those persisted.\"\"\"\n",
        "    inputs = []\n",
        "    window_length = 1\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.time_index = None\n",
        "        self.curr_date = None # date for which time_index is accurate\n",
        "        self.data = None\n",
        "        self.data_path = \"please_specify_.npy_file\"\n",
        "\n",
        "    def bs(self, arr):\n",
        "        \"\"\"Binary Search\"\"\"\n",
        "        if len(arr) == 1:\n",
        "            if self.curr_date < arr[0]:\n",
        "                return 0\n",
        "            else: return 1\n",
        "\n",
        "        mid = int(len(arr) / 2)\n",
        "        if self.curr_date < arr[mid]:\n",
        "            return self.bs(arr[:mid])\n",
        "        else:\n",
        "            return mid + self.bs(arr[mid:])\n",
        "\n",
        "    def bs_sparse_time(self, sid):\n",
        "        \"\"\"For each security find the best range in the sparse data.\"\"\"\n",
        "        dates_for_sid = self.data.date[sid]\n",
        "        if np.isnan(dates_for_sid[0]):\n",
        "            return 0\n",
        "\n",
        "        # do a binary search of the dates array finding the index\n",
        "        # where self.curr_date will lie.\n",
        "        non_nan_dates = dates_for_sid[~np.isnan(dates_for_sid)]\n",
        "        return self.bs(non_nan_dates) - 1\n",
        "\n",
        "    def cold_start(self, today, assets):\n",
        "        if self.data is None:\n",
        "            self.data = np.load(self.data_path, allow_pickle=True)\n",
        "\n",
        "        self.M = self.data.date.shape[1]\n",
        "\n",
        "        # for each sid, do binary search of date array to find current index\n",
        "        # the results can be shared across all factors that inherit from SparseDataFactor\n",
        "        # this sets an array of ints: time_index\n",
        "        self.time_index = np.full(self.N, -1, np.dtype('int64'))\n",
        "        self.curr_date = today.value\n",
        "        for asset in assets:  # asset is numpy.int64\n",
        "            self.time_index[asset] = self.bs_sparse_time(asset)\n",
        "\n",
        "    def update_time_index(self, today, assets):\n",
        "        \"\"\"Ratchet update.\n",
        "        for each asset check if today >= dates[self.time_index]\n",
        "        if so then increment self.time_index[asset.sid] += 1\"\"\"\n",
        "\n",
        "        ind_p1 = self.time_index.copy()\n",
        "        np.add.at(ind_p1, ind_p1 != (self.M - 1), 1)\n",
        "        sids_to_increment = today.value >= self.data.date[np.arange(self.N), ind_p1]\n",
        "        sids_not_max = self.time_index != (self.M - 1)   # create mask of non-maxed\n",
        "        self.time_index[sids_to_increment & sids_not_max] += 1\n",
        "\n",
        "        self.curr_date = today.value\n",
        "\n",
        "    def compute(self, today, assets, out, *arrays):\n",
        "        # for each asset in assets determine index from date (today)\n",
        "        if self.time_index is None:\n",
        "            self.cold_start(today, assets)\n",
        "        else:\n",
        "            self.update_time_index(today, assets)\n",
        "\n",
        "        ti_used_today = self.time_index[assets]\n",
        "\n",
        "        for field in self.__class__.outputs:\n",
        "            out[field][:] = self.data[field][assets, ti_used_today]\n",
        "\n",
        "\n",
        "class Fundamentals(SparseDataFactor):\n",
        "    outputs = ['marketcap', 'assets', 'liabilities', 'pe', 'currentratio', 'netmargin', 'capex', 'fcf', 'roic']\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Fundamentals, self).__init__(*args, **kwargs)\n",
        "        self.N = len(get_ticker_sid_dict_from_bundle(\"sep\")) + 1  # max(sid)+1 get this from the bundle\n",
        "        self.data_path = zipline_dir + '/data/' + 'SF1.npy'\n",
        "\n",
        "class FundamentalsSP500(SparseDataFactor):\n",
        "    outputs = ['marketcap', 'assets', 'liabilities', 'pe', 'currentratio', 'netmargin', 'capex', 'fcf', 'roic']\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(FundamentalsSP500, self).__init__(*args, **kwargs)\n",
        "        self.N = len(get_ticker_sid_dict_from_bundle(\"sp500\")) + 1  # max(sid)+1 get this from the bundle\n",
        "        self.data_path = zipline_dir + '/data/' + 'SF1_SP500.npy'\n",
        "\n",
        "class FundamentalsSP1500(SparseDataFactor):\n",
        "    outputs = ['marketcap', 'assets', 'liabilities', 'pe', 'currentratio', 'netmargin', 'capex', 'fcf', 'roic']\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(FundamentalsSP1500, self).__init__(*args, **kwargs)\n",
        "        self.N = len(get_ticker_sid_dict_from_bundle(\"sp1500\")) + 1  # max(sid)+1 get this from the bundle\n",
        "        self.data_path = zipline_dir + '/data/' + 'SF1.npy'\n",
        "\n",
        "class StaticData(CustomFactor):\n",
        "    \"\"\"Returns static values for an SID.\n",
        "    This holds static data (does not change with time) like: exchange, sector, industry\"\"\"\n",
        "    inputs = []\n",
        "    window_length = 1\n",
        "    outputs = ['sector', 'exchange', 'industry']\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.data = np.load(zipline_dir + '/data/' + 'SHARDAR_static.npy', allow_pickle=True)\n",
        "\n",
        "    def compute(self, today, assets, out):\n",
        "        # out[:] = self.data[assets]\n",
        "        out['sector'][:] = self.data[0, assets]\n",
        "        out['exchange'][:] = self.data[1, assets]\n",
        "        out['industry'][:] = self.data[2, assets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__Qxmxwc_u_q"
      },
      "source": [
        "### 2.2 Ingest stock and fundamental data into Zipine\n",
        "\n",
        "Warning: Running this code will overwrite database and can take almost an hour to run!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1tkYFaLBG9w"
      },
      "source": [
        "# Stock price data ingestion\n",
        "\n",
        "#!zipline ingest -b 'sep'\n",
        "#zipline ingest -b 'sp500'\n",
        "#!zipline ingest -b 'sp1500'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df738XoVBHAo"
      },
      "source": [
        "# Fundamental data ingestion\n",
        "\n",
        "# Choose which fundamental fields you want to be able to process through the pipeline.\n",
        "# A list of  of all available fields can be found below.\n",
        "\n",
        "# Fundamental data directory\n",
        "#sf1_dir = '/content/drive/MyDrive/abnormal-distribution-project-data/stocks/SHARADAR_SF1.csv'\n",
        "#fields = ['marketcap', 'assets', 'liabilities', 'pe', 'currentratio', 'netmargin', 'capex', 'fcf', 'roic']\n",
        "#load_sf1(sf1_dir, fields, dimensions=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BedPTieq8UJE",
        "outputId": "315ffbeb-0a6d-42a8-f180-dfb36c5c18b2"
      },
      "source": [
        "# Static data ingestions\n",
        "#static_file = '/content/drive/MyDrive/abnormal-distribution-project-data/stocks/SHARADAR_TICKERS.zip'\n",
        "#load_static(static_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:281: UserWarning: Overwriting bundle with name 'sep'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating static data\n",
            "Finished creating static data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPU8UX7JD1o7"
      },
      "source": [
        "An explanation of database related fields can be found in :\n",
        "\n",
        "  https://www.quandl.com/databases/SF1/documentation?anchor=dimensions\n",
        "\n",
        "A summary of each fundamental field can be found in:\n",
        "\n",
        "https://docs-1-8--quantrocket.netlify.app/docs/data/fundamental/sharadar/\n",
        "        \n",
        "        ['ticker', 'dimension', 'calendardate', 'datekey', 'reportperiod',\n",
        "       'lastupdated', 'accoci', 'assets', 'assetsavg', 'assetsc', 'assetsnc',\n",
        "       'assetturnover', 'bvps', 'capex', 'cashneq', 'cashnequsd', 'cor',\n",
        "       'consolinc', 'currentratio', 'de', 'debt', 'debtc', 'debtnc', 'debtusd',\n",
        "       'deferredrev', 'depamor', 'deposits', 'divyield', 'dps', 'ebit',\n",
        "       'ebitda', 'ebitdamargin', 'ebitdausd', 'ebitusd', 'ebt', 'eps',\n",
        "       'epsdil', 'epsusd', 'equity', 'equityavg', 'equityusd', 'ev', 'evebit',\n",
        "       'evebitda', 'fcf', 'fcfps', 'fxusd', 'gp', 'grossmargin']\n",
        "\n",
        "       ['intexp', 'invcap', 'invcapavg', 'inventory', 'investments',\n",
        "       'investmentsc', 'investmentsnc', 'liabilities', 'liabilitiesc',\n",
        "       'liabilitiesnc', 'marketcap', 'ncf', 'ncfbus', 'ncfcommon', 'ncfdebt',\n",
        "       'ncfdiv', 'ncff', 'ncfi', 'ncfinv', 'ncfo', 'ncfx', 'netinc',\n",
        "       'netinccmn', 'netinccmnusd', 'netincdis', 'netincnci', 'netmargin',\n",
        "       'opex', 'opinc', 'payables', 'payoutratio', 'pb', 'pe', 'pe1',\n",
        "       'ppnenet', 'prefdivis', 'price', 'ps', 'ps1', 'receivables', 'retearn',\n",
        "       'revenue', 'revenueusd', 'rnd', 'roa', 'roe', 'roic', 'ros', 'sbcomp',\n",
        "       'sgna', 'sharefactor', 'sharesbas', 'shareswa', 'shareswadil', 'sps',\n",
        "       'tangibles', 'taxassets', 'taxexp', 'taxliabilities', 'tbvps',\n",
        "       'workingcapital']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6uWiFpJBh4u"
      },
      "source": [
        "# 3. Equity factors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucIJTE9GB0wZ"
      },
      "source": [
        "## 3.1 Load helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR9qZO-8HO2x"
      },
      "source": [
        "# Portions of these code adapted from https://www.udacity.com/course/ai-for-trading--nd880 \n",
        "\n",
        "\n",
        "def register_data(start_date, end_date, bundle_name, address):\n",
        "\n",
        "    start_session = pd.Timestamp(start_date, tz='utc')\n",
        "    end_session = pd.Timestamp(end_date, tz='utc')\n",
        "\n",
        "    register(bundle_name, csvdir_equities(['daily'],address,),\n",
        "    calendar_name='NYSE', start_session=start_session,\n",
        "    end_session=end_session)\n",
        "\n",
        "\n",
        "class PricingLoader(object):\n",
        "    def __init__(self, bundle_data):\n",
        "        self.loader = USEquityPricingLoader(\n",
        "            bundle_data.equity_daily_bar_reader,\n",
        "            bundle_data.adjustment_reader)\n",
        "\n",
        "    def get_loader(self, column):\n",
        "        if column not in USEquityPricing.columns:\n",
        "            raise Exception('Column not in USEquityPricing')\n",
        "        return self.loader\n",
        "\n",
        "def build_pipeline_engine(bundle_data, trading_calendar):\n",
        "    pricing_loader = PricingLoader(bundle_data)\n",
        "\n",
        "    engine = SimplePipelineEngine(\n",
        "        get_loader=pricing_loader.get_loader,\n",
        "        calendar=trading_calendar.all_sessions,\n",
        "        asset_finder=bundle_data.asset_finder)\n",
        "\n",
        "    return engine\n",
        "\n",
        "# Loading stock list from file\n",
        "def stock_list(file_name):\n",
        "    all_stocks = []\n",
        "    with open(file_name, 'r') as f:\n",
        "        for line in f:\n",
        "            # remove linebreak which is the last character of the string\n",
        "            currentPlace = line[:-1]\n",
        "            # add item to the list\n",
        "            all_stocks.append(currentPlace)\n",
        "        return all_stocks\n",
        "\n",
        "def get_universe_tickers(engine, universe, end_date):\n",
        "    universe_end_date = pd.Timestamp(end_date, tz='UTC')\n",
        "\n",
        "    universe_tickers = engine \\\n",
        "        .run_pipeline(\n",
        "        Pipeline(screen=universe),\n",
        "        universe_end_date,\n",
        "        universe_end_date) \\\n",
        "        .index.get_level_values(1) \\\n",
        "        .values.tolist()\n",
        "\n",
        "    return universe_tickers\n",
        "\n",
        "\n",
        "def run_pipeline(engine, pipeline, start_date, end_date):\n",
        "\n",
        "    # TODO: adjust for trading days\n",
        "    end_dt = pd.Timestamp(end_date.strftime('%Y-%m-%d'), tz='UTC')\n",
        "    start_dt = pd.Timestamp(start_date.strftime('%Y-%m-%d'), tz='UTC')\n",
        "    return engine.run_pipeline(pipeline, start_dt, end_dt)\n",
        "\n",
        "\n",
        "def get_pipeline_tickers(factors):\n",
        "\n",
        "    return factors.index.levels[1].values.tolist()\n",
        "\n",
        "\n",
        "def make_pipeline(factors, universe):\n",
        "    factors_pipe = OrderedDict()\n",
        "        \n",
        "    for name, f in factors.items():\n",
        "        factors_pipe[name] = f\n",
        "                    \n",
        "    pipe = Pipeline(screen=universe, columns=factors_pipe)\n",
        "    \n",
        "    return pipe\n",
        "\n",
        "#pd.DataFrame(sorted([asset.security_name for asset in all_assets]), \n",
        "#             columns =['ticker']).to_csv('sp1500.csv', index=False)\n",
        "\n",
        "#data = pd.read_csv('/content/drive/MyDrive/abnormal-distribution-project-data/stocks/SHARADAR_SEP.csv')\n",
        "#sp500_tickers = pd.read_csv('/content/drive/MyDrive/abnormal-distribution-project-data/components/sp500.csv')\n",
        "#data_sp500 = data[data.ticker.isin(sp500_tickers.ticker)]\n",
        "#data_sp500.reset_index(inplace=True, drop=True)\n",
        "#data_sp500.to_csv('/content/drive/MyDrive/abnormal-distribution-project-data/stocks/SHARADAR_SP500.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pkEClPRDMPL"
      },
      "source": [
        "## 3.2 Custom Factors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPy1rDlShGe1"
      },
      "source": [
        "\n",
        "def ForwardReturns(long_window, short_window, mask, asset=None):\n",
        "    if asset == None:\n",
        "        return ((1 + Returns(window_length=long_window, mask=mask))/\\\n",
        "                (1 + Returns(window_length=short_window, mask=mask)) - 1)\n",
        "    else:\n",
        "        return ((1 + Returns(inputs=[asset], window_length=long_window, mask=mask))/\\\n",
        "                (1 + Returns(inputs=[asset], window_length=short_window, mask=mask)) - 1)\n",
        "\n",
        "\n",
        "def make_returns():\n",
        "    \n",
        "    all_factors = {\n",
        "        \n",
        "        '1D_ret_open': DailyReturns(inputs=[USEquityPricing.open]),\n",
        "        '1W_ret_open': Returns(inputs=[USEquityPricing.open], window_length=5),\n",
        "        '1M_ret_open': Returns(inputs=[USEquityPricing.open], window_length=21), \n",
        "        '3M_ret_open': Returns(inputs=[USEquityPricing.open], window_length=63),\n",
        "        '6M_ret_open': Returns(inputs=[USEquityPricing.open], window_length=126), \n",
        "        '1Y_ret_open': Returns(inputs=[USEquityPricing.open], window_length=252),\n",
        "        \n",
        "        '1D_ret_close': DailyReturns() ,\n",
        "        '1W_ret_close': Returns(window_length=5),\n",
        "        '1M_ret_close': Returns(window_length=21), \n",
        "        '3M_ret_close': Returns(window_length=63),\n",
        "        '6M_ret_close': Returns(window_length=126),\n",
        "        '1Y_ret_close': Returns(window_length=252),\n",
        "    }\n",
        "    \n",
        "    return all_factors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgdxEWg8FyZH"
      },
      "source": [
        "## 3.3 Run pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM_spQ00HPxY"
      },
      "source": [
        "# Obtain data, choose trading calendar and build pipeline engine\n",
        "\n",
        "trading_calendar = get_calendar('NYSE') \n",
        "bundles.register('sp1500', from_sep_dump('.'))\n",
        "bundle_data = bundles.load('sp1500')\n",
        "engine = build_pipeline_engine(bundle_data, trading_calendar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u0Hj4JM9vAZ"
      },
      "source": [
        "# Set date range\n",
        "start_date = '1999-12-30'\n",
        "end_date = '2020-11-19'\n",
        "universe_start_date = pd.Timestamp(start_date, tz='UTC')\n",
        "universe_end_date = pd.Timestamp(end_date, tz='UTC')\n",
        "\n",
        "# Select universe of stocks\n",
        "universe = FundamentalsSP1500().marketcap.top(1500) \n",
        "\n",
        "pipeline = make_pipeline(make_returns(), universe)\n",
        "\n",
        "# Define pipeline\n",
        "#pipeline = Pipeline(screen=universe)\n",
        "#pipeline.add(StaticData().sector, 'sector')\n",
        "#pipeline.add(DailyReturns(window_length=252), '1Y_return')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHJXOgB7GMni",
        "outputId": "b8012f4c-7886-4d6a-a951-2af8af5c090a"
      },
      "source": [
        "# Run pipeline \n",
        "all_factors = run_pipeline(engine, pipeline, universe_start_date, universe_end_date)\n",
        "# Convert tickers from zipline class to string\n",
        "all_factors.index = all_factors.index.set_levels(all_factors.index.levels[1].map(lambda x: x.asset_name), level=1)\n",
        "# Get all tickers for the stocks we're looking at\n",
        "all_assets = get_pipeline_tickers(all_factors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:371: RuntimeWarning: invalid value encountered in less_equal\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j68o-fa-W7r7"
      },
      "source": [
        "pickle.dump(all_factors, open('/content/drive/MyDrive/abnormal-distribution-project-data/components/returns_sp1500.p','wb'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIr1mu30T5Go"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "A = pickle.load(open('/content/drive/MyDrive/abnormal-distribution-project-data/components/returns_sp500.p','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "WZCH1zpvsTBt",
        "outputId": "7728b0f5-f4e6-4fca-cb4c-40f20bd8c0a8"
      },
      "source": [
        "A.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>1D_ret_close</th>\n",
              "      <th>1D_ret_open</th>\n",
              "      <th>1M_ret_close</th>\n",
              "      <th>1M_ret_open</th>\n",
              "      <th>1W_ret_close</th>\n",
              "      <th>1W_ret_open</th>\n",
              "      <th>1Y_ret_close</th>\n",
              "      <th>1Y_ret_open</th>\n",
              "      <th>3M_ret_close</th>\n",
              "      <th>3M_ret_open</th>\n",
              "      <th>6M_ret_close</th>\n",
              "      <th>6M_ret_open</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">1999-12-30 00:00:00+00:00</th>\n",
              "      <th>AAPL</th>\n",
              "      <td>0.025086</td>\n",
              "      <td>-0.022599</td>\n",
              "      <td>0.028604</td>\n",
              "      <td>-0.011429</td>\n",
              "      <td>0.007848</td>\n",
              "      <td>-0.057734</td>\n",
              "      <td>1.511173</td>\n",
              "      <td>1.416201</td>\n",
              "      <td>0.591150</td>\n",
              "      <td>0.625940</td>\n",
              "      <td>1.219753</td>\n",
              "      <td>1.094431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACF</th>\n",
              "      <td>-0.009973</td>\n",
              "      <td>-0.027027</td>\n",
              "      <td>0.091941</td>\n",
              "      <td>0.062699</td>\n",
              "      <td>0.042104</td>\n",
              "      <td>0.028571</td>\n",
              "      <td>0.455922</td>\n",
              "      <td>0.364877</td>\n",
              "      <td>0.242670</td>\n",
              "      <td>0.252174</td>\n",
              "      <td>0.151194</td>\n",
              "      <td>0.112003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACGL</th>\n",
              "      <td>0.014894</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.004868</td>\n",
              "      <td>-0.085734</td>\n",
              "      <td>0.073518</td>\n",
              "      <td>-0.025585</td>\n",
              "      <td>-0.390545</td>\n",
              "      <td>-0.432283</td>\n",
              "      <td>-0.175691</td>\n",
              "      <td>-0.134977</td>\n",
              "      <td>-0.111732</td>\n",
              "      <td>-0.143316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACIIQ</th>\n",
              "      <td>0.093760</td>\n",
              "      <td>0.012821</td>\n",
              "      <td>0.135371</td>\n",
              "      <td>-0.000559</td>\n",
              "      <td>0.165249</td>\n",
              "      <td>0.110490</td>\n",
              "      <td>-0.334591</td>\n",
              "      <td>-0.403606</td>\n",
              "      <td>-0.100932</td>\n",
              "      <td>-0.167132</td>\n",
              "      <td>-0.199177</td>\n",
              "      <td>-0.273716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADBE</th>\n",
              "      <td>-0.002843</td>\n",
              "      <td>0.011614</td>\n",
              "      <td>-0.039662</td>\n",
              "      <td>-0.077913</td>\n",
              "      <td>-0.013701</td>\n",
              "      <td>0.027788</td>\n",
              "      <td>1.873291</td>\n",
              "      <td>1.808691</td>\n",
              "      <td>0.162314</td>\n",
              "      <td>0.153583</td>\n",
              "      <td>0.548259</td>\n",
              "      <td>0.584315</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 1D_ret_close  1D_ret_open  1M_ret_close  \\\n",
              "1999-12-30 00:00:00+00:00 AAPL       0.025086    -0.022599      0.028604   \n",
              "                          ACF       -0.009973    -0.027027      0.091941   \n",
              "                          ACGL       0.014894     0.000000     -0.004868   \n",
              "                          ACIIQ      0.093760     0.012821      0.135371   \n",
              "                          ADBE      -0.002843     0.011614     -0.039662   \n",
              "\n",
              "                                 1M_ret_open  1W_ret_close  1W_ret_open  \\\n",
              "1999-12-30 00:00:00+00:00 AAPL     -0.011429      0.007848    -0.057734   \n",
              "                          ACF       0.062699      0.042104     0.028571   \n",
              "                          ACGL     -0.085734      0.073518    -0.025585   \n",
              "                          ACIIQ    -0.000559      0.165249     0.110490   \n",
              "                          ADBE     -0.077913     -0.013701     0.027788   \n",
              "\n",
              "                                 1Y_ret_close  1Y_ret_open  3M_ret_close  \\\n",
              "1999-12-30 00:00:00+00:00 AAPL       1.511173     1.416201      0.591150   \n",
              "                          ACF        0.455922     0.364877      0.242670   \n",
              "                          ACGL      -0.390545    -0.432283     -0.175691   \n",
              "                          ACIIQ     -0.334591    -0.403606     -0.100932   \n",
              "                          ADBE       1.873291     1.808691      0.162314   \n",
              "\n",
              "                                 3M_ret_open  6M_ret_close  6M_ret_open  \n",
              "1999-12-30 00:00:00+00:00 AAPL      0.625940      1.219753     1.094431  \n",
              "                          ACF       0.252174      0.151194     0.112003  \n",
              "                          ACGL     -0.134977     -0.111732    -0.143316  \n",
              "                          ACIIQ    -0.167132     -0.199177    -0.273716  \n",
              "                          ADBE      0.153583      0.548259     0.584315  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRdy9YvdH7Fg"
      },
      "source": [
        "A = A.reset_index().rename(columns={'level_0':'date','level_1':'ticker'})[['date','ticker','1D_ret_close', '1M_ret_close']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtorVfiUHKgR"
      },
      "source": [
        "A.to_csv('/content/drive/MyDrive/abnormal-distribution-project-data/components/returns_sp500.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lATuCd-2Hru9"
      },
      "source": [
        "A = pd.read_csv('/content/drive/MyDrive/abnormal-distribution-project-data/components/returns_sp500.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "IaDNihbtHwWG",
        "outputId": "ea77ea49-c0d6-4dce-fbd7-e62bd630918c"
      },
      "source": [
        "A.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>1D_ret_close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1999-12-30 00:00:00+00:00</td>\n",
              "      <td>A</td>\n",
              "      <td>0.170732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1999-12-30 00:00:00+00:00</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>0.025086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1999-12-30 00:00:00+00:00</td>\n",
              "      <td>ABC</td>\n",
              "      <td>-0.029342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1999-12-30 00:00:00+00:00</td>\n",
              "      <td>ABMD</td>\n",
              "      <td>0.014085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1999-12-30 00:00:00+00:00</td>\n",
              "      <td>ABS</td>\n",
              "      <td>-0.028287</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        date ticker  1D_ret_close\n",
              "0  1999-12-30 00:00:00+00:00      A      0.170732\n",
              "1  1999-12-30 00:00:00+00:00   AAPL      0.025086\n",
              "2  1999-12-30 00:00:00+00:00    ABC     -0.029342\n",
              "3  1999-12-30 00:00:00+00:00   ABMD      0.014085\n",
              "4  1999-12-30 00:00:00+00:00    ABS     -0.028287"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    }
  ]
}