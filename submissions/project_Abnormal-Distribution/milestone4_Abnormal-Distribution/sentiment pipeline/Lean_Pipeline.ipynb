{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lean_Pipeline.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKbBebxXpnH3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta, datetime\n",
        "from pytz import timezone\n",
        "import pickle\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeGH9jJux9nD"
      },
      "source": [
        "# Pipeline to calculate sentiment scores, targets, and aggregate news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x290NqO0pylJ",
        "outputId": "5f8353d6-e734-4aa7-c57d-d72e9dd770b2"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hE7BL60qIvS"
      },
      "source": [
        "CALENDAR_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/components/trading_calendar.csv'\n",
        "\n",
        "DICT_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/loughran_mcdonald/loughran_mcdonald.csv'\n",
        "EXTRA_DICT_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/loughran_mcdonald/sestm.csv'\n",
        "\n",
        "STOCKS_SP1500_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/components/returns_sp1500.csv'\n",
        "FACTORS_SP1500_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/components/factors_sp1500.csv'\n",
        "\n",
        "FINNHUB_SP1500_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/news_reduced/finnhub_sp1500.p'\n",
        "NEWS_SP1500_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/news_reduced/news_sp1500.p'\n",
        "\n",
        "TIINGO_LEAN_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/lean/tiingo.p'\n",
        "FINNHUB_LEAN_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/lean/finnhub.p'\n",
        "\n",
        "TARGETS_SP1500_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/loughran_mcdonald/targets_sp1500.p'\n",
        "TARGETS_SP1500_CSV_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/loughran_mcdonald/targets_sp1500.csv'\n",
        "MASTER_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/lean/master.p'\n",
        "\n",
        "TRAIN_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/lean/train.p'\n",
        "TEST_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/lean/test.p'\n",
        "VALID_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/lean/valid.p'\n",
        "\n",
        "INFO_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/lean/info.p'\n",
        "TRAIN_TXT_PATH = '/content/drive/MyDrive/abnormal-distribution-project-data/lean/train.txt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQQMB6bsp-vG"
      },
      "source": [
        "# Load trading calendar\n",
        "\n",
        "calendar = pd.read_csv(CALENDAR_PATH)\n",
        "calendar.index = pd.to_datetime(calendar.date).dt.date\n",
        "calendar.drop(['date'], inplace=True, axis=1)\n",
        "calendar.market_date = pd.to_datetime(calendar.market_date).dt.date\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4h2MQdLyLqZ"
      },
      "source": [
        "## Targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teBScidpTdfu"
      },
      "source": [
        "# Create targets \r\n",
        "\r\n",
        "targets = pd.read_csv(STOCKS_SP1500_PATH)\r\n",
        "targets.date = pd.to_datetime(targets.date).dt.date\r\n",
        "targets.ticker = targets.ticker.str.lower()\r\n",
        "\r\n",
        "factors = pd.read_csv(FACTORS_SP1500_PATH)\r\n",
        "factors.date = pd.to_datetime(factors.date).dt.date\r\n",
        "factors.ticker = factors.ticker.str.lower()\r\n",
        "\r\n",
        "targets['residual_close'] = factors.Residuals\r\n",
        "targets = targets[['date','ticker','1D_ret_close', '1D_ret_open', 'residual_close']]\r\n",
        "\r\n",
        "# Quantile\r\n",
        "targets_2means = targets.groupby('date').transform(lambda x: pd.qcut(x, q=[0, 0.5, 1], labels=range(0,2)))\r\n",
        "targets_3means = targets.groupby('date').transform(lambda x: pd.qcut(x, q=[0, 0.25, 0.75, 1], labels=range(0,3)))\r\n",
        "\r\n",
        "targets['ret_close_2'] = targets_2means['1D_ret_close']\r\n",
        "targets['ret_open_2'] = targets_2means['1D_ret_open']\r\n",
        "targets['res_close_2'] = targets_2means['residual_close']\r\n",
        "targets['ret_close_3'] = targets_3means['1D_ret_close']\r\n",
        "targets['ret_open_3'] = targets_3means['1D_ret_open']\r\n",
        "targets['res_close_3'] = targets_3means['residual_close']\r\n",
        "\r\n",
        "#targets = targets.drop(['1D_ret_close', '1D_ret_open', 'residual_close'], axis=1)\r\n",
        "\r\n",
        "with open(TARGETS_SP1500_PATH,'wb') as pkl_file:\r\n",
        "  pickle.dump(targets, pkl_file)\r\n",
        "\r\n",
        "targets.to_csv(TARGETS_SP1500_CSV_PATH, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYHewEqOyN1N"
      },
      "source": [
        "## Lougrhan-McDonald"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKKYXYVNRwR8"
      },
      "source": [
        "# Load pre-calculated pickle file\n",
        "with open(TARGETS_SP1500_PATH,'rb') as pkl_file:\n",
        "  targets = pickle.load(pkl_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2A17JRaqHEw"
      },
      "source": [
        "def loughran_mcdonald(text, pos_vocab, neg_vocab):\n",
        "  \"\"\"\n",
        "  Function that takes in a list of documents, a positive vocabulary and a negative \n",
        "  vocabulary, and returns an array of scores which is the sum of postive TF-IDF \n",
        "  values minus negative TF-IDF values.\n",
        "  \"\"\"\n",
        "  \n",
        "  # Calculate TF-IDF for positive and negative dictionaries\n",
        "  pos_tfidf = TfidfVectorizer(vocabulary=pos_vocab)\n",
        "  pos_tfidf_vecs = pos_tfidf.fit_transform(text)\n",
        "\n",
        "  neg_tfidf = TfidfVectorizer(vocabulary=neg_vocab)\n",
        "  neg_tfidf_vecs = neg_tfidf.fit_transform(text)\n",
        "\n",
        "  # Document scores\n",
        "  lmcd_scores = pos_tfidf_vecs.sum(axis=1) - neg_tfidf_vecs.sum(axis=1)\n",
        "  lmcd_scores = np.squeeze(np.asarray(lmcd_scores))\n",
        "\n",
        "  return lmcd_scores\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig5vNebzqiX9"
      },
      "source": [
        "\n",
        "\n",
        "d2 = pd.read_csv(EXTRA_DICT_PATH)\n",
        "\n",
        "sentiment_df = pd.read_csv(DICT_PATH) # Load Loughran-McDonald sentiment data\n",
        "sentiment_categories = ['negative', 'positive']\n",
        "sentiment_df.columns = sentiment_df.columns.str.lower() # Convert column names to lowercase\n",
        "sentiment_df = sentiment_df[['word'] + sentiment_categories] # Use only columns related to sentiment\n",
        "sentiment_df[sentiment_categories] = 1 * sentiment_df[sentiment_categories].astype(bool) # Convert to 1 or 0\n",
        "sentiment_df = sentiment_df[(sentiment_df[sentiment_categories]).any(1)] # Only use vocabulary that has sentiment ranking\n",
        "sentiment_df.word = sentiment_df.word.str.lower() # Convert words to lowercase \n",
        "sentiment_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Positive and negative vocabularies\n",
        "pos_vocab = sentiment_df[sentiment_df.positive==1].word\n",
        "pos_vocab = pos_vocab.to_list() + d2.pos_words.to_list() + ['beats']\n",
        "pos_vocab = list(set(pos_vocab))\n",
        "neg_vocab = sentiment_df[sentiment_df.negative==1].word\n",
        "neg_vocab = neg_vocab.to_list() + d2.neg_words.to_list()\n",
        "neg_vocab = list(set(neg_vocab))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bael5l9qqhn"
      },
      "source": [
        "# Load data\n",
        "tiingo_df = pickle.load(open(NEWS_SP1500_PATH,'rb'))\n",
        "tiingo_df = tiingo_df.reset_index()\n",
        "# Concatenate description and title\n",
        "tiingo_df['text'] = tiingo_df.description + ' ' + tiingo_df.title \n",
        "tiingo_df = tiingo_df.drop_duplicates(subset=['text'])\n",
        "tiingo_df = tiingo_df[['id','text','publishedDate','tickers']]\n",
        "tiingo_df = tiingo_df.rename(columns={'publishedDate':'date', 'tickers':'ticker'})\n",
        "tiingo_df['source'] = 'tiingo'\n",
        "# Remove \\n characters\n",
        "tiingo_df.text = tiingo_df.text.str.replace('\\n', ' ') \n",
        "# Convert date to EST\n",
        "tiingo_df.date = pd.to_datetime(tiingo_df.date).dt.tz_convert('US/Eastern')\n",
        "#Calculate Lougrhan McDonald scores\n",
        "tiingo_df['score'] = 0\n",
        "tiingo_df.loc[:,'score'] = loughran_mcdonald(tiingo_df.text.values, pos_vocab, neg_vocab)\n",
        "\n",
        "# Load pre-calculated pickle file\n",
        "with open(TIINGO_LEAN_PATH,'wb') as pkl_file:\n",
        "  pickle.dump(tiingo_df, pkl_file)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHr8xpEosAxt"
      },
      "source": [
        "# Load pre-calculated pickle file\n",
        "with open(FINNHUB_SP1500_PATH,'rb') as pkl_file:\n",
        "  finnhub_df = pickle.load(pkl_file)\n",
        "# Concatenate headline and summary\n",
        "finnhub_df['text'] = finnhub_df.headline + ' ' + finnhub_df.summary\n",
        "finnhub_df = finnhub_df.drop_duplicates(subset=['text'])\n",
        "finnhub_df = finnhub_df[['id', 'text', 'iso_time', 'related']]\n",
        "finnhub_df = finnhub_df.rename(columns={'iso_time':'date', 'related':'ticker'})\n",
        "finnhub_df.ticker = finnhub_df.ticker.str.lower()\n",
        "finnhub_df['source'] = 'finnhub'\n",
        "# Remove \\n characters\n",
        "finnhub_df.text = finnhub_df.text.str.replace('\\n', ' ') \n",
        "# Convert date to EST\n",
        "finnhub_df.date = pd.to_datetime(finnhub_df.date).dt.tz_convert('US/Eastern')\n",
        "#Calculate Lougrhan McDonald scores\n",
        "finnhub_df['score'] = 0\n",
        "finnhub_df.loc[:,'score'] = loughran_mcdonald(finnhub_df.text.values, pos_vocab, neg_vocab)\n",
        "\n",
        "# Load pre-calculated pickle file\n",
        "with open(FINNHUB_LEAN_PATH,'wb') as pkl_file:\n",
        "  pickle.dump(finnhub_df, pkl_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjkIKJxIyMJm"
      },
      "source": [
        "# Load pre-calculated pickle file\n",
        "with open(FINNHUB_LEAN_PATH,'rb') as pkl_file:\n",
        "  finnhub_df = pickle.load(pkl_file)\n",
        "\n",
        "# Load pre-calculated pickle file\n",
        "with open(TIINGO_LEAN_PATH,'rb') as pkl_file:\n",
        "  tiingo_df = pickle.load(pkl_file)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvrPaBiacwAB"
      },
      "source": [
        "key_dev = pd.read_csv(\"/content/drive/MyDrive/abnormal-distribution-project-data/compustat_keydev/final_keydev_data.csv\")\n",
        "df_outputs = pd.read_pickle(\"/content/drive/MyDrive/abnormal-distribution-project-data/compustat_keydev/FinBertOutput_HS_Sentiment.pkl\")\n",
        "df_outputs = df_outputs.drop('bert_features',axis=1)\n",
        "df_outputs['bert_neutral_sentiment'] = 1 - df_outputs['bert_neg_sentiment'] - df_outputs['bert_pos_sentiment']\n",
        "dd = {'bert_neutral_sentiment':1, 'bert_neg_sentiment':0,'bert_pos_sentiment':2}\n",
        "key_dev['finbert_baseline'] = df_outputs[['bert_neutral_sentiment', 'bert_neg_sentiment','bert_pos_sentiment']].idxmax(axis=1).apply(lambda x:dd[x]).values\n",
        "key_dev.datetime = pd.to_datetime(key_dev.datetime)\n",
        "#dd = (key_dev.datetime.dt.hour != 0) | (key_dev.datetime.dt.minute != 0) | (key_dev.datetime.dt.second != 0)\n",
        "#key_dev = key_dev[dd]\n",
        "#key_dev.datetime = key_dev.datetime.dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
        "key_dev = key_dev[key_dev.datetime >= pd.to_datetime('2010-12-30')]#.tz_localize('US/Eastern')]\n",
        "key_dev = key_dev[key_dev.datetime <= pd.to_datetime('2020-09-03')]#.tz_localize('US/Eastern')]\n",
        "key_dev['score'] = 0\n",
        "key_dev.loc[:,'score'] = loughran_mcdonald(key_dev.situation.values, pos_vocab, neg_vocab)\n",
        "key_dev.loc[key_dev.score<0,'score'] = -1\n",
        "key_dev.loc[key_dev.score>0,'score'] = 1\n",
        "key_dev.score += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxNR-PjxgAlB"
      },
      "source": [
        "## Master dataset with scores and news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4k-J5CPduZN"
      },
      "source": [
        "with open('/content/drive/MyDrive/abnormal-distribution-project-data/lean/keydev.p','wb') as pkl_file:\n",
        "  pickle.dump(key_dev, pkl_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZny2FrDxMLa"
      },
      "source": [
        "dd = finnhub_df.date.dt.tz_convert('UTC')\n",
        "dd = (dd.dt.hour != 0) | (dd.dt.minute != 0) | (dd.dt.second != 0)\n",
        "\n",
        "news_df = pd.concat([tiingo_df, finnhub_df[dd]])\n",
        "news_df['_id'] = news_df.id\n",
        "news_df.loc[news_df.source == 'finnhub', '_id'] = -news_df.loc[news_df.source == 'finnhub', '_id']\n",
        "news_df = news_df.rename(columns={'score':'dict_score'})\n",
        "#news_df.loc[news_df.dict_score>0, 'dict_score'] = 1\n",
        "#news_df.loc[news_df.dict_score<0, 'dict_score'] = -1\n",
        "\n",
        "news_df = news_df.sample(frac=1)\n",
        "news_df.sort_values(by='date', inplace=True)\n",
        "news_df = news_df[news_df.date >= pd.to_datetime('2010-12-30').tz_localize('US/Eastern')]\n",
        "news_df = news_df[news_df.date <= pd.to_datetime('2020-09-03').tz_localize('US/Eastern')]\n",
        "\n",
        "news_df['open_date'] = news_df.date\n",
        "news_df['close_date'] = news_df.date\n",
        "news_df.loc[news_df.date.dt.hour > 9, 'open_date'] = news_df.loc[news_df.date.dt.hour > 9, 'open_date'] +timedelta(days=1)\n",
        "news_df.loc[news_df.date.dt.hour > 16, 'close_date'] = news_df.loc[news_df.date.dt.hour > 16, 'close_date'] +timedelta(days=1)\n",
        "\n",
        "news_df['open_date'] = news_df['open_date'].dt.date\n",
        "news_df['close_date'] = news_df['close_date'].dt.date\n",
        "\n",
        "news_df.open_date = calendar.loc[news_df.open_date].values\n",
        "news_df.close_date = calendar.loc[news_df.close_date].values\n",
        "\n",
        "news_df.ticker = news_df.ticker.str.lower()\n",
        "targets['close_date'] = targets.date\n",
        "targets['open_date'] = targets.date\n",
        "\n",
        "a = news_df.merge(targets[['ret_close_2', 'ret_close_3', 'res_close_2', 'res_close_3', 'ticker', 'close_date']], on=['ticker','close_date'])\n",
        "b = news_df.merge(targets[['ret_open_2', 'ret_open_3', 'ticker', 'open_date']], on=['ticker','open_date'])[['_id','ret_open_2', 'ret_open_3']]\n",
        "\n",
        "news_df = a.merge(b, on=['_id'])\n",
        "\n",
        "news_df['dict_ret_close'] =  2 * news_df['ret_close_2']\n",
        "news_df['dict_ret_open'] =  2 * news_df['ret_open_2'] \n",
        "news_df['dict_res_close'] =  2 * news_df['res_close_2']\n",
        "news_df.loc[news_df.dict_score == 0, 'dict_ret_close'] = 1\n",
        "news_df.loc[news_df.dict_score == 0, 'dict_ret_open'] = 1\n",
        "news_df.loc[news_df.dict_score == 0, 'dict_res_close'] = 1\n",
        "\n",
        "news_df.sort_values(by='date', inplace=True)\n",
        "news_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "news_df[['ret_close_2',\t'ret_close_3', 'res_close_2',\t'res_close_3', 'ret_open_2',\t\n",
        "         'ret_open_3',\t'dict_ret_close',\t'dict_ret_open', 'dict_res_close']] = \\\n",
        "news_df[['ret_close_2',\t'ret_close_3', 'res_close_2',\t'res_close_3', 'ret_open_2',\t\n",
        "         'ret_open_3',\t'dict_ret_close',\t'dict_ret_open', 'dict_res_close']].astype(int) \n",
        "\n",
        "with open(MASTER_PATH,'wb') as pkl_file:\n",
        "  pickle.dump(news_df, pkl_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O93TBgNycEY"
      },
      "source": [
        "### Train / Valid / Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOS1k44xPGdW"
      },
      "source": [
        "with open(MASTER_PATH,'rb') as pkl_file:\n",
        "  news_df = pickle.load(pkl_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMNnK6ypadb4"
      },
      "source": [
        "train_date = pd.to_datetime('2017-12-29').tz_localize('US/Eastern')\n",
        "valid_date = pd.to_datetime('2018-12-31').tz_localize('US/Eastern')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_Wlk4j7atBS"
      },
      "source": [
        "train_df = news_df[news_df.date <= train_date].sample(frac=1).reset_index(drop=True)\n",
        "valid_df = news_df[(news_df.date > train_date ) & (news_df.date <= valid_date)].sort_values(by='date').reset_index(drop=True)\n",
        "test_df = news_df[(news_df.date > valid_date )].sort_values(by='date').reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXHyR2H-bCYm"
      },
      "source": [
        "with open(TRAIN_PATH,'wb') as pkl_file:\n",
        "  pickle.dump(train_df, pkl_file)\n",
        "with open(VALID_PATH,'wb') as pkl_file:\n",
        "  pickle.dump(valid_df, pkl_file)\n",
        "with open(TEST_PATH,'wb') as pkl_file:\n",
        "  pickle.dump(test_df, pkl_file)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOTOmznLzNQ5"
      },
      "source": [
        "with open(TRAIN_PATH,'rb') as pkl_file:\n",
        "  train_df = pickle.load(pkl_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYcWz4PrnfLS"
      },
      "source": [
        "with open('/content/drive/MyDrive/abnormal-distribution-project-data/lean/ml_filter.p','wb') as pkl_file:\n",
        "  pickle.dump(news_df[news_df.dict_res_close==1]['_id'], pkl_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf2laElNyi4v"
      },
      "source": [
        "## Informative news\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EchL-_9g-hj"
      },
      "source": [
        "# Informative news\n",
        "info_df = train_df[(train_df.res_close_3!=1) & (train_df.dict_res_close!=1)]\n",
        "\n",
        "with open(INFO_PATH,'wb') as pkl_file:\n",
        "  pickle.dump(info_df, pkl_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htIdy7ilrbIZ"
      },
      "source": [
        "\n",
        "with open(TRAIN_TXT_PATH, 'w') as f:\n",
        "  for item in info_df.text.to_list():\n",
        "      f.write(\"%s\\n\" % item)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6wlLCbdhwTB"
      },
      "source": [
        "with open(MASTER_PATH,'rb') as pkl_file:\n",
        "  news_df = pickle.load(pkl_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIBVd04JlPEu"
      },
      "source": [
        "with open(MASTER_PATH,'rb') as pkl_file:\n",
        "  news_df = pickle.load(pkl_file)\n",
        "\n",
        "scores = pd.read_csv('/content/drive/MyDrive/abnormal-distribution-project-data/lean/scores_master.csv')\n",
        "scores['_id'] = scores['id']\n",
        "scores.loc[scores.source=='finnhub','_id'] = - scores.loc[scores.source=='finnhub','_id']\n",
        "scores.finbert_baseline += 1\n",
        "\n",
        "mm = scores[['_id', 'finbert_baseline']].merge(news_df, on='_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P61tquM6KGg6"
      },
      "source": [
        "ss = mm[((mm[['dict_ret_close', 'dict_ret_open', 'dict_res_close', 'ret_close_3', 'ret_open_3']]==0).all(axis=1)) & (mm['finbert_baseline']!=0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG24pwech1tq"
      },
      "source": [
        "with open(MASTER_PATH,'rb') as pkl_file:\n",
        "  news_df = pickle.load(pkl_file)\n",
        "\n",
        "scores = pd.read_csv('/content/drive/MyDrive/abnormal-distribution-project-data/lean/scores_master.csv')\n",
        "scores['_id'] = scores['id']\n",
        "scores.loc[scores.source=='finnhub','_id'] = - scores.loc[scores.source=='finnhub','_id']\n",
        "scores.finbert_baseline += 1\n",
        "\n",
        "mm = scores[['_id', 'finbert_baseline']].merge(news_df, on='_id')\n",
        "idx_neg = (mm[['dict_ret_close', 'dict_ret_open', 'dict_res_close', 'ret_close_3', 'ret_open_3', 'finbert_baseline']]==0).all(axis=1)\n",
        "idx_pos = (mm[['dict_ret_close', 'dict_ret_open', 'dict_res_close', 'ret_close_3', 'ret_open_3', 'finbert_baseline']]==2).all(axis=1)\n",
        "idx_neu = (mm[['dict_ret_close', 'dict_ret_open', 'dict_res_close', 'ret_close_3', 'ret_open_3', 'finbert_baseline']]==1).all(axis=1)\n",
        "mm['score'] = 1\n",
        "mm.loc[idx_neg,'score'] = 0\n",
        "mm.loc[idx_pos,'score'] = 2\n",
        "mm = pd.concat([mm.loc[idx_neg,['close_date', 'text', 'score']], mm.loc[idx_pos,['close_date', 'text', 'score']], mm.loc[idx_neu,['close_date', 'text','score']]])\n",
        "\n",
        "with open('/content/drive/MyDrive/abnormal-distribution-project-data/lean/keydev.p','rb') as pkl_file:\n",
        "  key_dev = pickle.load(pkl_file)\n",
        "\n",
        "key_dev = key_dev[['situation','finbert_baseline','score','datetime']]\n",
        "key_dev = key_dev.rename(columns= {'situation':'text', 'datetime':'date'})\n",
        "key_dev.date = pd.to_datetime(key_dev.date).dt.date\n",
        "mm = mm.rename(columns= { 'close_date':'date'})\n",
        "mm.date = pd.to_datetime(mm.date).dt.date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tloSPLEwkVzj"
      },
      "source": [
        "final_set = pd.concat([mm, \n",
        "            key_dev[(key_dev.finbert_baseline == 2) & (key_dev.score == 2)][['date','text','score']],\n",
        "            key_dev[(key_dev.finbert_baseline == 1) & (key_dev.score == 1)][['date','text','score']],\n",
        "            key_dev[(key_dev.finbert_baseline == 0) & (key_dev.score == 0)][['date','text','score']]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weeJcIQJnhdo"
      },
      "source": [
        "train_date = pd.to_datetime('2017-12-29').date()\n",
        "valid_date = pd.to_datetime('2018-12-31').date()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oeLRQU9qVoO"
      },
      "source": [
        "final_set = final_set.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjv35ISRnsn7"
      },
      "source": [
        "final_train_df = final_set[final_set.date <= train_date].sample(frac=1).reset_index(drop=True)\n",
        "final_valid_df = final_set[(final_set.date > train_date ) & (final_set.date <= valid_date)].sort_values(by='date').reset_index(drop=True)\n",
        "final_test_df = final_set[(final_set.date > valid_date )].sort_values(by='date').reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlG7QZqtqoH_"
      },
      "source": [
        "final_train_df =pd.concat([final_train_df[final_train_df.score==0], \n",
        "                           final_train_df[final_train_df.score==1].sample(n=40000), \n",
        "                           final_train_df[final_train_df.score==2].sample(n=40844)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Z9-OB6F3riQH",
        "outputId": "d6f2f7cd-9bb4-4ded-f0f8-e2085c3e883c"
      },
      "source": [
        "final_train_df.reset_index(drop=True).sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30427</th>\n",
              "      <td>2011-03-16</td>\n",
              "      <td>BofA Merrill Lynch has named Bob Elfring as th...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17928</th>\n",
              "      <td>2014-02-25</td>\n",
              "      <td>JPMorgan Chase &amp; Co. will cut 136 mortgage ban...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69126</th>\n",
              "      <td>2012-04-26</td>\n",
              "      <td>Dominion Resources, Inc. announced that it is ...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64630</th>\n",
              "      <td>2015-02-06</td>\n",
              "      <td>Growth stocks can be some of the most exciting...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95343</th>\n",
              "      <td>2013-09-18</td>\n",
              "      <td>On September 18, 2013 DISH was granted a victo...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74421</th>\n",
              "      <td>2011-02-15</td>\n",
              "      <td>Broadcom Corporation announced a new family of...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15054</th>\n",
              "      <td>2012-08-07</td>\n",
              "      <td>Office Depot, Inc. reported unaudited consolid...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44300</th>\n",
              "      <td>2015-01-12</td>\n",
              "      <td>PRNewswire  Lennox International Inc NYSE LII...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43755</th>\n",
              "      <td>2016-08-22</td>\n",
              "      <td>Bonnie Baha, a U.S. bond portfolio manager who...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31145</th>\n",
              "      <td>2017-05-31</td>\n",
              "      <td>Zacks Investment Research upgraded shares of W...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             date                                               text  score\n",
              "30427  2011-03-16  BofA Merrill Lynch has named Bob Elfring as th...    1.0\n",
              "17928  2014-02-25  JPMorgan Chase & Co. will cut 136 mortgage ban...    0.0\n",
              "69126  2012-04-26  Dominion Resources, Inc. announced that it is ...    2.0\n",
              "64630  2015-02-06  Growth stocks can be some of the most exciting...    2.0\n",
              "95343  2013-09-18  On September 18, 2013 DISH was granted a victo...    2.0\n",
              "...           ...                                                ...    ...\n",
              "74421  2011-02-15  Broadcom Corporation announced a new family of...    2.0\n",
              "15054  2012-08-07  Office Depot, Inc. reported unaudited consolid...    0.0\n",
              "44300  2015-01-12   PRNewswire  Lennox International Inc NYSE LII...    1.0\n",
              "43755  2016-08-22  Bonnie Baha, a U.S. bond portfolio manager who...    1.0\n",
              "31145  2017-05-31  Zacks Investment Research upgraded shares of W...    1.0\n",
              "\n",
              "[100000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpcHHEqGsS4h"
      },
      "source": [
        "final_valid_df =pd.concat([final_valid_df[final_valid_df.score==0], \n",
        "                           final_valid_df[final_valid_df.score==1].sample(7595), \n",
        "                           final_valid_df[final_valid_df.score==2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXYQ8iLMtJgd"
      },
      "source": [
        "with open('/content/drive/MyDrive/abnormal-distribution-project-data/lean/final_train.p','wb') as pkl_file:\n",
        "  pickle.dump(final_train_df, pkl_file)\n",
        "with open('/content/drive/MyDrive/abnormal-distribution-project-data/lean/final_valid.p','wb') as pkl_file:\n",
        "  pickle.dump(final_valid_df, pkl_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}