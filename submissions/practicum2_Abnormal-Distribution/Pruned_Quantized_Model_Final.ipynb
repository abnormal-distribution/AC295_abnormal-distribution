{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pruned_Quantized_Model_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "86C4-p4lR6jE",
        "3idRbhCy12h9",
        "tAIWevAc2XYI",
        "EDHE_Nd6SCy7",
        "rOEndUuiaz7u",
        "fR7pciUW5__z",
        "SPWyyDZ90yFT",
        "ZLOi0ec9Z7my",
        "vk8zPGiHZ7nA",
        "C4TfXT23K2We",
        "2tyoK5hohJjK",
        "-4kwrLc7qTgH"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch6vpxrwQdcc"
      },
      "source": [
        "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
        "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> AC295: Advanced Practical Data Science </h1>\n",
        "\n",
        "## Practicum 2: Visual Question Answering\n",
        "\n",
        "**Harvard University, Fall 2020**  \n",
        "**Instructors**: Pavlos Protopapas  \n",
        "\n",
        "### **Team: $\\alpha\\beta normal$ $Distri\\beta ution$**\n",
        "#### **Roht Beri, Eduardo Peynetti, Jessica Wijaya, Stuart Neilson**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86zGynCVRsdD"
      },
      "source": [
        "## Pruned Model\n",
        "\n",
        "### This notebook generates and tests a pruned version of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86C4-p4lR6jE"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyGrRhuBwNJb",
        "outputId": "ce917cee-b9c0-4519-e2bd-1f327ff62b02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip3 install transformers\n",
        "!pip install -q tensorflow_model_optimization"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3idRbhCy12h9"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXHPXTLpQZ4V"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "import zipfile\n",
        "import shutil\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from glob import glob\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.layer_utils import count_params\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
        "from tensorflow_model_optimization.sparsity.keras import prune_low_magnitude\n",
        "\n",
        "from transformers import BertTokenizer, TFBertModel, TFDistilBertModel"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAIWevAc2XYI"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpYGZYyPucLA",
        "outputId": "cf68345b-8e21-422f-fa31-ed9f0eb83b5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if not os.path.exists('/content/data'):\n",
        "    os.mkdir('/content/data')\n",
        "\n",
        "!gsutil cp -r gs://practicum2-abnormal-distribution/big2 /content/data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://practicum2-abnormal-distribution/big2/answers.csv...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/functional_1.h5...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_00-of-10.records...\n",
            "/ [3 files][  1.2 GiB/  1.2 GiB]  120.0 MiB/s                                   \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_01-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_02-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_03-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_04-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_05-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_06-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_07-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_08-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_09-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/train2014_tf/vaq_raw_train2014_10-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_00-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_01-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_02-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_03-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_04-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_05-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_06-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_07-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_08-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_09-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/val2014_tf/vaq_raw_val2014_10-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/big2/vqa_model.h5...\n",
            "| [25 files][  9.7 GiB/  9.7 GiB]   80.2 MiB/s                                  \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "\n",
            "Operation completed over 25 objects/9.7 GiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hN9E7Lw4czk"
      },
      "source": [
        "#drive.mount('/content/drive', force_remount=False)\n",
        "# if not os.path.exists('/content/data'):\n",
        "#     os.mkdir('/content/data')\n",
        "#\n",
        "#!cp -r '/content/drive/My Drive/Practicum2Data/big2' /content/data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDHE_Nd6SCy7"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJhxKghA_lci"
      },
      "source": [
        "# we use the following to save the models\n",
        "class JsonEncoder(json.JSONEncoder):\n",
        "  def default(self, obj):\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, decimal.Decimal):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    else:\n",
        "        return super(JsonEncoder, self).default(obj)\n",
        "\n",
        "# save_model saves everything. weights, statuses and results. \n",
        "def save_model(model,training_results,execution_time, learning_rate, epochs, optimizer, evaluation_results,path=\"models\"):\n",
        "  model_name=model.name\n",
        "  # Get the model train history\n",
        "  model_train_history = training_results.history\n",
        "\n",
        "  # Ensure path exists\n",
        "  if not os.path.exists(path):\n",
        "      os.mkdir(path)\n",
        "\n",
        "  # Save the enitire model (structure + weights)\n",
        "  model.save(os.path.join(path,model_name+\".hdf5\"))\n",
        "\n",
        "  # Save only the weights\n",
        "  model.save_weights(os.path.join(path,model_name+\".h5\"))\n",
        "\n",
        "  # Save the structure only\n",
        "  model_json = model.to_json()\n",
        "  with open(os.path.join(path,model_name+\".json\"), \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "    \n",
        "  model_size = get_model_size(model_name=model_name)\n",
        "\n",
        "  # Save model history\n",
        "  with open(os.path.join(\"models\",model_name+\"_train_history.json\"), \"w\") as json_file:\n",
        "      json_file.write(json.dumps(model_train_history,cls=JsonEncoder))\n",
        "\n",
        "  trainable_parameters = count_params(model.trainable_weights)\n",
        "  non_trainable_parameters = count_params(model.non_trainable_weights)\n",
        "  total_params = trainable_parameters + non_trainable_parameters\n",
        "\n",
        "  # Save model metrics\n",
        "  metrics ={\n",
        "      \"total_params\":total_params,\n",
        "      \"execution_time\":execution_time,\n",
        "      \"loss\":evaluation_results[0],\n",
        "      \"accuracy\":evaluation_results[1],\n",
        "      \"model_size\":model_size,\n",
        "      \"learning_rate\":learning_rate,\n",
        "      \"epochs\":epochs,\n",
        "      \"optimizer\":type(optimizer).__name__,\n",
        "      \"name\": model_name,\n",
        "      \"id\": int(time.time())\n",
        "  }\n",
        "\n",
        "  with open(os.path.join(\"models\",model.name+\"_metrics.json\"), \"w\") as json_file:\n",
        "      json_file.write(json.dumps(metrics,cls=JsonEncoder))\n",
        "\n",
        "def get_model_size(path=\"models\",model_name=\"model01\"):\n",
        "  model_size = os.stat(os.path.join(path,model_name+\".hdf5\")).st_size\n",
        "  return model_size\n",
        "\n",
        "def evaluate_model(model,test_data, training_results,execution_time, learning_rate, epochs, \n",
        "                   optimizer,save=True, \n",
        "                   loss_metrics=[\"loss\",\"val_loss\"],\n",
        "                   acc_metrics=[\"accuracy\",\"val_accuracy\"]):\n",
        "    \n",
        "  # Get the model train history\n",
        "  model_train_history = training_results.history\n",
        "  # Get the number of epochs the training was run for\n",
        "  num_epochs = len(model_train_history[loss_metrics[0]])\n",
        "\n",
        "  # Plot training results\n",
        "  fig = plt.figure(figsize=(15,5))\n",
        "  axs = fig.add_subplot(1,2,1)\n",
        "  axs.set_title('Loss')\n",
        "  # Plot all metrics\n",
        "  for metric in loss_metrics:\n",
        "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
        "  axs.legend()\n",
        "  \n",
        "  axs = fig.add_subplot(1,2,2)\n",
        "  axs.set_title('Accuracy')\n",
        "  # Plot all metrics\n",
        "  for metric in acc_metrics:\n",
        "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
        "  axs.legend()\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  # Evaluate on test data\n",
        "  evaluation_results = model.evaluate(test_data, return_dict=True)\n",
        "  print(evaluation_results)\n",
        "\n",
        "  evaluation_results = [evaluation_results[loss_metrics[0]], evaluation_results[acc_metrics[0]]]\n",
        "  \n",
        "  if save:\n",
        "      # Save model\n",
        "      save_model(model,training_results,execution_time, learning_rate, epochs, optimizer, evaluation_results)\n",
        "  \n",
        "  return evaluation_results"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBj_9MC2l8o_"
      },
      "source": [
        "# Get Top K answers\n",
        "def get_top_K_answers(k):\n",
        "    answers = pd.read_csv(\"/content/data/big2/answers.csv\", index_col=0)\n",
        "    answers = answers.iloc[:k]\n",
        "    return answers"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKnRAy5WHxkf"
      },
      "source": [
        "# Function to parse data features\n",
        "def _parse_features_function(example):\n",
        "    # Parse the input tf.train.Example proto using the dictionary above.\n",
        "    tf_records_features = {\n",
        "        'image_raw': tf.io.FixedLenFeature([], tf.string), \n",
        "        'question' : tf.io.FixedLenFeature([], tf.string),\n",
        "        'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "        'token_type_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "        'attention_mask': tf.io.FixedLenFeature([], tf.string), \n",
        "        'answer': tf.io.FixedLenFeature([], tf.int64)\n",
        "    }\n",
        "    return tf.io.parse_single_example(example, tf_records_features)\n",
        "\n",
        "\n",
        "# Filter if answer is no\n",
        "def filter_fn(x):\n",
        "    return x['answer'] < k\n",
        "\n",
        "\n",
        "# Read image and resize it\n",
        "def read_and_decode(img):\n",
        "    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
        "    img = tf.cast(img, tf.float32)/255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "# Structure the data for training\n",
        "def structure_data(data):\n",
        "    image = data['image_raw']\n",
        "    image = read_and_decode(image)\n",
        "    \n",
        "    input_ids = tf.io.decode_raw(data['input_ids'], tf.int32)\n",
        "    attention_mask = tf.io.decode_raw(data['attention_mask'], tf.int32)\n",
        "    token_type_ids = tf.io.decode_raw(data['token_type_ids'], tf.int32)\n",
        "    \n",
        "    answer = data['answer']\n",
        "\n",
        "    return ((image, (input_ids, token_type_ids, attention_mask)), answer)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOEndUuiaz7u"
      },
      "source": [
        "### Important Variables and Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG29KGghUgqZ"
      },
      "source": [
        "# Constants\n",
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Pipeline variables\n",
        "k = 10\n",
        "batch_size = 32\n",
        "train_buffer_size = 32\n",
        "val_buffer_size = 32\n",
        "prefetch = AUTOTUNE"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR7pciUW5__z"
      },
      "source": [
        "### Get Top Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTFiNYkr6APs"
      },
      "source": [
        "top_answers = get_top_K_answers(k)\n",
        "TOP_ANSWERS = tf.constant(top_answers)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPWyyDZ90yFT"
      },
      "source": [
        "### Build Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbEY61HinbFk"
      },
      "source": [
        "# ############## #\n",
        "# # Train data # #\n",
        "# ############## #\n",
        "tfrecords_pattern_path = \"/content/data/big2/train2014_tf/vaq_raw_train2014_*-of-*.records\"\n",
        "train_files = tf.io.matching_files(tfrecords_pattern_path)\n",
        "train_files = tf.random.shuffle(train_files)\n",
        "train_shards = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "\n",
        "train = train_shards.interleave(tf.data.TFRecordDataset)\n",
        "train = train.map(_parse_features_function, num_parallel_calls=AUTOTUNE)\n",
        "train = train.filter(filter_fn)\n",
        "train = train.map(structure_data, num_parallel_calls=AUTOTUNE)\n",
        "#train = train.shuffle(buffer_size=train_buffer_size)\n",
        "train = train.batch(batch_size)\n",
        "#train = train.cache().prefetch(prefetch)\n",
        "\n",
        "# ################### #\n",
        "# # Validation data # #\n",
        "# ################### #\n",
        "tfrecords_pattern_path = \"/content/data/big2/val2014_tf/vaq_raw_val2014_*-of-*.records\"\n",
        "val_files = tf.io.matching_files(tfrecords_pattern_path)\n",
        "val_files = tf.random.shuffle(val_files)\n",
        "val_shards = tf.data.Dataset.from_tensor_slices(val_files)\n",
        "\n",
        "valid = val_shards.interleave(tf.data.TFRecordDataset)\n",
        "valid = valid.map(_parse_features_function, num_parallel_calls=AUTOTUNE)\n",
        "valid = valid.filter(filter_fn)\n",
        "valid = valid.map(structure_data, num_parallel_calls=AUTOTUNE)\n",
        "#valid = valid.shuffle(buffer_size=val_buffer_size)\n",
        "valid = valid.batch(batch_size)\n",
        "#valid = valid.cache().prefetch(prefetch)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLOi0ec9Z7my"
      },
      "source": [
        "### Build Distillation Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0PPmp624ZSp"
      },
      "source": [
        "def build_student_model(image_height, image_width, num_channels, num_classes, model_name='student'):\n",
        "  # Model input\n",
        "  input_shape = [image_height, image_width, num_channels]  # height, width, channels\n",
        "  model_name =  model_name +\"_\"+ str(int(time.time()))\n",
        "\n",
        "  image_input = layers.Input(shape=input_shape)\n",
        "  distill_img = layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", activation='relu')(image_input)\n",
        "  distill_img = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(distill_img)\n",
        "  distill_img = layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", activation='relu')(distill_img)\n",
        "  distill_img = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(distill_img)\n",
        "\n",
        "  image_hidden_states = layers.Flatten()(distill_img)\n",
        "  image_hs = layers.Lambda(lambda x: K.l2_normalize(x,axis=1))(image_hidden_states)\n",
        "\n",
        "  input_ids = layers.Input(shape=(24,), dtype=tf.int32)\n",
        "  #token_type_ids = layers.Input(shape=(24,), dtype=tf.int32)\n",
        "  #attention_mask = layers.Input(shape=(24,), dtype=tf.int32)\n",
        "\n",
        "  distill_bert = TFDistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
        "  distill_bert.trainable = False\n",
        "  question = distill_bert(\n",
        "      input_ids, \n",
        "      # token_type_ids=token_type_ids, \n",
        "      #attention_mask=attention_mask\n",
        "    )\n",
        "  question_hs = layers.Flatten()(question[0])\n",
        "  \n",
        "  cross_hs = layers.concatenate([image_hs, question_hs])\n",
        "  x = layers.Dense(32, activation='relu')(cross_hs)\n",
        "  output = layers.Dense(units=num_classes)(x)\n",
        "\n",
        "  model = Model(inputs=[image_input, (input_ids, )], outputs=output)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk8zPGiHZ7nA"
      },
      "source": [
        "### Load Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-jMYcRlMXXh",
        "outputId": "bd129928-c9a1-42e0-cbf9-c2b314bcc41e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Optimizer\n",
        "learning_rate = 0.001 \n",
        "optimizer = optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "# Loss\n",
        "student_loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Free up memory\n",
        "K.clear_session()\n",
        "\n",
        "# Build Student model\n",
        "student_model = build_student_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS, k, model_name='student_distill')\n",
        "\n",
        "# Load Weights\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "student_model.load_weights(\"/content/drive/My Drive/Practicum2Data/student_model.h5\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4TfXT23K2We"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### Set Parameters for Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us7I2lkvTlBI",
        "outputId": "f101d4b9-d090-4df7-e8f3-56b453d35899",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set Pruning Parameters\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=7138*2)\n",
        "}\n",
        "\n",
        "def no_bert(layer):\n",
        "    if layer.name in ['tf_distil_bert_model'] :\n",
        "        return layer\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "\n",
        "prune_model = tf.keras.models.clone_model(\n",
        "    student_model,\n",
        "    clone_function=no_bert,\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:200: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvIGuStcTlRT"
      },
      "source": [
        "# Compile the pruning Model\n",
        "prune_model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                      loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint( \n",
        "        filepath='/content/data/prune_model.h5', \n",
        "        monitor='val_accuracy', \n",
        "        save_best_only=True, \n",
        "        save_weights_only=True\n",
        "    ), tfmot.sparsity.keras.UpdatePruningStep()]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tyoK5hohJjK"
      },
      "source": [
        "### Prune the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiI4kDbHTlW9",
        "outputId": "320559e4-c8d3-4f8f-c4f3-698d4518347c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start_time = time.time()\n",
        "pruning_results = prune_model.fit(\n",
        "    train,\n",
        "    validation_data=valid,\n",
        "    callbacks = callbacks,\n",
        "    class_weight = (top_answers.sum()/top_answers).reset_index().frequency.to_dict(),\n",
        "    epochs=2, \n",
        "    verbose=1\n",
        ")\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Training execution time (mins)\",execution_time)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "7138/7138 [==============================] - 335s 47ms/step - loss: 14.5675 - accuracy: 0.4357 - val_loss: 1.2167 - val_accuracy: 0.3867\n",
            "Epoch 2/2\n",
            "7138/7138 [==============================] - 348s 49ms/step - loss: 17.2987 - accuracy: 0.4109 - val_loss: 1.0011 - val_accuracy: 0.4378\n",
            "Training execution time (mins) 11.536517075697581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXiwvGz7Tlcd"
      },
      "source": [
        "# code from https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/sparsity/keras/prune.py\n",
        "def _strip_pruning_wrapper(layer):\n",
        "    if layer.name in ['tf_distil_bert_model'] :\n",
        "        return layer\n",
        "    if isinstance(layer, tf.keras.Model):\n",
        "      # A keras model with prunable layers\n",
        "      return keras.models.clone_model(\n",
        "          layer, input_tensors=None, clone_function=_strip_pruning_wrapper)\n",
        "    if isinstance(layer, pruning_wrapper.PruneLowMagnitude):\n",
        "      # The _batch_input_shape attribute in the first layer makes a Sequential\n",
        "      # model to be built. This makes sure that when we remove the wrapper from\n",
        "      # the first layer the model's built state preserves.\n",
        "      if not hasattr(layer.layer, '_batch_input_shape') and hasattr(\n",
        "          layer, '_batch_input_shape'):\n",
        "        layer.layer._batch_input_shape = layer._batch_input_shape\n",
        "      return layer.layer\n",
        "    return layer\n",
        "\n",
        "prune_extracted_model = tf.keras.models.clone_model(\n",
        "    prune_model,\n",
        "    clone_function=_strip_pruning_wrapper\n",
        ")\n",
        "\n",
        "prune_extracted_model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                      loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I74VJBiPVERb",
        "outputId": "5e3daaf8-ee42-41a8-fd06-a6b7cbc27fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prune_extracted_model.save_weights('/content/drive/My Drive/Practicum2Data/prune_extracted_model.h5')\n",
        "prune_extracted_model.save('/content/drive/My Drive/Practicum2Data/pruned_extracted_model')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe14018dac8>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe1401809e8>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe1400a1be0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe1400b9208>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe1400477f0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe140055dd8>, because it is not built.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Practicum2Data/pruned_extracted_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4kwrLc7qTgH"
      },
      "source": [
        "### Quantization of the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vi3p_ZuqXFR",
        "outputId": "c39fcd39-d44d-4b15-81a4-4b832a49a6e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(prune_extracted_model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n",
        "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "with open('/content/drive/My Drive/Practicum2Data/quantized_and_pruned.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe14018dac8>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe1401809e8>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe1400a1be0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe1400b9208>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe1400477f0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fe140055dd8>, because it is not built.\n",
            "INFO:tensorflow:Assets written to: /tmp/tmp2vm8dxm1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBXSxU0MIbHd"
      },
      "source": [
        "### Check Model Sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtVhjCEDVEUF",
        "outputId": "ec2ffcfc-894b-447e-932f-29771b56e6dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def check_model_weights(model):\n",
        "  for i, w in enumerate(model.get_weights()):\n",
        "    print(model.weights[i].name,\"Total:\",w.size, \"Zeros:\", round(np.sum(w == 0) / w.size * 100,2),\"%\")\n",
        "\n",
        "check_model_weights(prune_extracted_model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2d/kernel:0 Total: 864 Zeros: 79.98 %\n",
            "conv2d/bias:0 Total: 32 Zeros: 0.0 %\n",
            "conv2d_1/kernel:0 Total: 9216 Zeros: 80.0 %\n",
            "conv2d_1/bias:0 Total: 32 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/embeddings/word_embeddings/weight:0 Total: 23440896 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/embeddings/position_embeddings/embeddings:0 Total: 393216 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/embeddings/LayerNorm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/embeddings/LayerNorm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/bias:0 Total: 3072 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/q_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/k_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/v_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/attention/out_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/sa_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin1/bias:0 Total: 3072 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/ffn/lin2/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._1/output_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/q_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/k_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/v_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/attention/out_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/sa_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin1/bias:0 Total: 3072 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/ffn/lin2/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._2/output_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/q_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/k_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/v_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/attention/out_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/sa_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin1/bias:0 Total: 3072 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/ffn/lin2/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._3/output_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/q_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/k_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/v_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/attention/out_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/sa_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin1/bias:0 Total: 3072 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/ffn/lin2/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._4/output_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/q_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/k_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/v_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/kernel:0 Total: 589824 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/attention/out_lin/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/sa_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin1/bias:0 Total: 3072 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/kernel:0 Total: 2359296 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/ffn/lin2/bias:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/gamma:0 Total: 768 Zeros: 0.0 %\n",
            "tf_distil_bert_model/distilbert/transformer/layer_._5/output_layer_norm/beta:0 Total: 768 Zeros: 0.0 %\n",
            "dense/kernel:0 Total: 790528 Zeros: 80.0 %\n",
            "dense/bias:0 Total: 32 Zeros: 0.0 %\n",
            "dense_1/kernel:0 Total: 320 Zeros: 80.0 %\n",
            "dense_1/bias:0 Total: 10 Zeros: 0.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8FUsuEKVEXN",
        "outputId": "1ec26c51-0257-42cb-e6d1-6e52ca475bbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with zipfile.ZipFile('/content/data/vqa_model.zip', \"w\", compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write('/content/data/big2/vqa_model.h5')\n",
        "print(\"Original model before zipping: %.2f Kb\"% (os.path.getsize('/content/data/big2/vqa_model.h5') / float(1000)))\n",
        "print(\"Original model after zipping: %.2f Kb\"% (os.path.getsize('/content/data/vqa_model.zip') / float(1000)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original model before zipping: 624304.14 Kb\n",
            "Original model after zipping: 578842.27 Kb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDHR2RZAXqZJ",
        "outputId": "9c58f631-6815-4daa-b8f2-e1026fa1a5de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with zipfile.ZipFile('/content/data/prune_model.zip', \"w\", compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write('/content/data/prune_model.h5')\n",
        "print(\"Pruned model before zipping: %.2f Kb\"% (os.path.getsize('/content/data/prune_model.h5') / float(1000)))\n",
        "print(\"Pruned model after zipping: %.2f Kb\"% (os.path.getsize('/content/data/prune_model.zip') / float(1000)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruned model before zipping: 272016.46 Kb\n",
            "Pruned model after zipping: 245880.97 Kb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCV4XkEjVEaC",
        "outputId": "c0607561-4320-4060-924a-e5e2bd809c5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with zipfile.ZipFile('/content/data/prune_extracted_model.zip', \"w\", compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write('/content/data/prune_extracted_model.h5')\n",
        "print(\"Extracted pruned model before zipping: %.2f Kb\"% (os.path.getsize('/content/data/prune_extracted_model.h5') / float(1000)))\n",
        "print(\"Extracted pruned model after zipping: %.2f Kb\"% (os.path.getsize('/content/data/prune_extracted_model.zip') / float(1000)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted pruned model before zipping: 268793.38 Kb\n",
            "Extracted pruned model after zipping: 245772.15 Kb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH9mqj4_Bl5q",
        "outputId": "a6abaddf-e759-4868-eb68-5ab53f4b0910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with zipfile.ZipFile('/content/data/quantized_and_pruned.zip', \"w\", compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write('quantized_and_pruned.tflite')\n",
        "print(\"Extracted quantized model before zipping: %.2f Kb\"% (os.path.getsize('quantized_and_pruned.tflite') / float(1000)))\n",
        "print(\"Extracted quantized model after zipping: %.2f Kb\"% (os.path.getsize('/content/data/quantized_and_pruned.zip') / float(1000)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracted quantized model before zipping: 133764.06 Kb\n",
            "Extracted quantized model after zipping: 121088.62 Kb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50B8OIkiC5Kq"
      },
      "source": [
        "### Evaluate Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S87pimwBXUXO",
        "outputId": "244f8915-4102-406b-e642-504ff4736cc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prune_evaluation_results = prune_extracted_model.evaluate(valid, return_dict=True)\n",
        "print(prune_evaluation_results)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3441/3441 [==============================] - 94s 27ms/step - loss: 1.0011 - accuracy: 0.4378\n",
            "{'loss': 1.0011489391326904, 'accuracy': 0.43779629468917847}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfV7NIT2txcl"
      },
      "source": [
        "def evaluate_model(interpreter):\n",
        "  input_index1 = interpreter.get_input_details()[0][\"index\"]\n",
        "  input_index2 = interpreter.get_input_details()[1][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on ever y image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  sum = 0\n",
        "  accurate = 0\n",
        "  for i, test in enumerate(valid):\n",
        "      if i==200:\n",
        "          break\n",
        "    \n",
        "      for j in range(32):\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        test_image = np.expand_dims(test[0][0][j], axis=0).astype(np.float32)\n",
        "        interpreter.set_tensor(input_index1, test_image)\n",
        "\n",
        "        test_ques = np.expand_dims(test[0][1][0][j], axis=0)\n",
        "        interpreter.set_tensor(input_index2, test_ques)\n",
        "\n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = interpreter.tensor(output_index)\n",
        "        digit = np.argmax(output()[0])\n",
        "        prediction_digits.append(digit)\n",
        "        sum += 1\n",
        "        accurate += digit==test[1][j].numpy()\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  #prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = accurate/sum\n",
        "  return accuracy"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjESsdsyFL8a",
        "outputId": "8e125177-1901-4253-a9d4-08ab38524e69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path='/content/drive/My Drive/Practicum2Data/quantized_and_pruned.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "execution_time = time.time() - start\n",
        "\n",
        "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
        "print('Average Inference time:{}'.format(execution_time/(32*200)))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "\n",
            "Pruned and quantized TFLite test_accuracy: 0.38125\n",
            "Average Inference time:0.08021565843373538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwaHqSduf1Lm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}