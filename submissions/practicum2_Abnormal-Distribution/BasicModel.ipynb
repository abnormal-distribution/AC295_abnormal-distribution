{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BasicModel.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "86C4-p4lR6jE",
        "3idRbhCy12h9",
        "tAIWevAc2XYI",
        "EDHE_Nd6SCy7",
        "SPWyyDZ90yFT",
        "fR7pciUW5__z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46a9da129665401f929777712c4acdb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_623b510108e2433495a2bd4b85b188a1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e99ded3bc28c4ff7af48ae334171b146",
              "IPY_MODEL_ea4f78b403f04d01b092b8c0c77a6900"
            ]
          }
        },
        "623b510108e2433495a2bd4b85b188a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e99ded3bc28c4ff7af48ae334171b146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_05044c95eda34583a1024bbf3f6cf69e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35b9415d96ab41b88fd074c34123c04e"
          }
        },
        "ea4f78b403f04d01b092b8c0c77a6900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_72e3477381e940a4b1407718f7b1b332",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 1.83kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1bc3605203ad48a8b79d02e2412d4bf9"
          }
        },
        "05044c95eda34583a1024bbf3f6cf69e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35b9415d96ab41b88fd074c34123c04e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72e3477381e940a4b1407718f7b1b332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1bc3605203ad48a8b79d02e2412d4bf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd8a2ed7647d406c8aca2467025a7319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_508910dda801493092ecc252de2eaa99",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_23d5aa70a7344a83b07baccfcbfcfd78",
              "IPY_MODEL_71e7355441f64829881c04a67d7754ef"
            ]
          }
        },
        "508910dda801493092ecc252de2eaa99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23d5aa70a7344a83b07baccfcbfcfd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d80d4ab855394239ab52d65d7d172eb1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 536063208,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 536063208,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efe05fd62cf04b1d86356091e8d51981"
          }
        },
        "71e7355441f64829881c04a67d7754ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_87c634da1258486189a5bec33f4541b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 536M/536M [00:06&lt;00:00, 77.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff34796d8938465b8ada8cad1b591147"
          }
        },
        "d80d4ab855394239ab52d65d7d172eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efe05fd62cf04b1d86356091e8d51981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87c634da1258486189a5bec33f4541b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff34796d8938465b8ada8cad1b591147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch6vpxrwQdcc"
      },
      "source": [
        "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
        "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> AC295: Advanced Practical Data Science </h1>\n",
        "\n",
        "## Practicum 2: Visual Question Answering\n",
        "\n",
        "**Harvard University, Fall 2020**  \n",
        "**Instructors**: Pavlos Protopapas  \n",
        "\n",
        "### **Team: $\\alpha\\beta normal$ $Distri\\beta ution$**\n",
        "#### **Roht Beri, Eduardo Peynetti, Jessica Wijaya, Stuart Neilson**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86zGynCVRsdD"
      },
      "source": [
        "## Basic Model for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86C4-p4lR6jE"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyGrRhuBwNJb",
        "outputId": "3c971bec-8497-4574-8b91-407f5c6d4ce2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 25.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 32.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 21.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 17.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 12.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 13.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 13.2MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 13.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 12.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 13.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 13.4MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 13.4MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 53.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=7b57b32fa00c2cad22d3c04e8a079d02a556ea7d4bcceb8ef626c0842a5cc3dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3idRbhCy12h9"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXHPXTLpQZ4V"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "import zipfile\n",
        "import shutil\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from glob import glob\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.layer_utils import count_params\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "from transformers import BertTokenizer, TFBertModel"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAIWevAc2XYI"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpYGZYyPucLA",
        "outputId": "83db7db1-97e1-4520-9fb4-aaf7c0e6bec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\"\n",
        "!mkdir 'data'\n",
        "#shutil.rmtree('/content/data/train2014_tf')\n",
        "#shutil.rmtree('/content/data/val2014_tf')\n",
        "!cp -r '/content/drive/My Drive/Practicum2Data/big/val2014_tf' /content/data\n",
        "!cp -r '/content/drive/My Drive/Practicum2Data/big/train2014_tf' /content/data\n",
        "!cp -r \"/content/drive/My Drive/Practicum2Data/big/answers.csv\" /content/data\n",
        "\"\"\"\n",
        "if not os.path.exists('/content/data'):\n",
        "    os.mkdir('/content/data')\n",
        "\n",
        "!gsutil cp -r gs://practicum2-abnormal-distribution/train2014_tf /content/data\n",
        "!gsutil cp -r gs://practicum2-abnormal-distribution/val2014_tf /content/data\n",
        "\n",
        "# https://storage.googleapis.com/practicum2-abnormal-distribution/train2014_tf\n",
        "# https://storage.googleapis.com/practicum2-abnormal-distribution/val2014_tf"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_00-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_01-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_02-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_03-of-10.records...\n",
            "| [4 files][  2.4 GiB/  2.4 GiB]   72.2 MiB/s                                   \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_04-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_05-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_06-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_07-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_08-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_09-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/train2014_tf/vaq_raw_train2014_10-of-10.records...\n",
            "- [11 files][  6.1 GiB/  6.1 GiB]   55.0 MiB/s                                  \n",
            "Operation completed over 11 objects/6.1 GiB.                                     \n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_00-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_01-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_02-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_03-of-10.records...\n",
            "/ [4 files][  1.2 GiB/  1.2 GiB]   60.8 MiB/s                                   \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_04-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_05-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_06-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_07-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_08-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_09-of-10.records...\n",
            "Copying gs://practicum2-abnormal-distribution/val2014_tf/vaq_raw_val2014_10-of-10.records...\n",
            "/ [11 files][  2.9 GiB/  2.9 GiB]   67.6 MiB/s                                  \n",
            "Operation completed over 11 objects/2.9 GiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hN9E7Lw4czk",
        "outputId": "2a693382-873f-4a07-b495-962c267b5442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drive.mount('/content/drive', force_remount=False)\n",
        "if not os.path.exists('/content/data'):\n",
        "    os.mkdir('/content/data')\n",
        "\n",
        "!cp -r '/content/drive/My Drive/Practicum2Data/big' /content/data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDHE_Nd6SCy7"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJhxKghA_lci"
      },
      "source": [
        "# we use the following to save the models\n",
        "class JsonEncoder(json.JSONEncoder):\n",
        "  def default(self, obj):\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, decimal.Decimal):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    else:\n",
        "        return super(JsonEncoder, self).default(obj)\n",
        "\n",
        "# save_model saves everything. weights, statuses and results. \n",
        "def save_model(model,training_results,execution_time, learning_rate, epochs, optimizer, evaluation_results,path=\"models\"):\n",
        "  model_name=model.name\n",
        "  # Get the model train history\n",
        "  model_train_history = training_results.history\n",
        "\n",
        "  # Ensure path exists\n",
        "  if not os.path.exists(path):\n",
        "      os.mkdir(path)\n",
        "\n",
        "  # Save the enitire model (structure + weights)\n",
        "  model.save(os.path.join(path,model_name+\".hdf5\"))\n",
        "\n",
        "  # Save only the weights\n",
        "  model.save_weights(os.path.join(path,model_name+\".h5\"))\n",
        "\n",
        "  # Save the structure only\n",
        "  model_json = model.to_json()\n",
        "  with open(os.path.join(path,model_name+\".json\"), \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "    \n",
        "  model_size = get_model_size(model_name=model_name)\n",
        "\n",
        "  # Save model history\n",
        "  with open(os.path.join(\"models\",model_name+\"_train_history.json\"), \"w\") as json_file:\n",
        "      json_file.write(json.dumps(model_train_history,cls=JsonEncoder))\n",
        "\n",
        "  trainable_parameters = count_params(model.trainable_weights)\n",
        "  non_trainable_parameters = count_params(model.non_trainable_weights)\n",
        "  total_params = trainable_parameters + non_trainable_parameters\n",
        "\n",
        "  # Save model metrics\n",
        "  metrics ={\n",
        "      \"total_params\":total_params,\n",
        "      \"execution_time\":execution_time,\n",
        "      \"loss\":evaluation_results[0],\n",
        "      \"accuracy\":evaluation_results[1],\n",
        "      \"model_size\":model_size,\n",
        "      \"learning_rate\":learning_rate,\n",
        "      \"epochs\":epochs,\n",
        "      \"optimizer\":type(optimizer).__name__,\n",
        "      \"name\": model_name,\n",
        "      \"id\": int(time.time())\n",
        "  }\n",
        "\n",
        "  with open(os.path.join(\"models\",model.name+\"_metrics.json\"), \"w\") as json_file:\n",
        "      json_file.write(json.dumps(metrics,cls=JsonEncoder))\n",
        "\n",
        "def get_model_size(path=\"models\",model_name=\"model01\"):\n",
        "  model_size = os.stat(os.path.join(path,model_name+\".hdf5\")).st_size\n",
        "  return model_size\n",
        "\n",
        "def evaluate_model(model,test_data, training_results,execution_time, learning_rate, epochs, \n",
        "                   optimizer,save=True, \n",
        "                   loss_metrics=[\"loss\",\"val_loss\"],\n",
        "                   acc_metrics=[\"accuracy\",\"val_accuracy\"]):\n",
        "    \n",
        "  # Get the model train history\n",
        "  model_train_history = training_results.history\n",
        "  # Get the number of epochs the training was run for\n",
        "  num_epochs = len(model_train_history[loss_metrics[0]])\n",
        "\n",
        "  # Plot training results\n",
        "  fig = plt.figure(figsize=(15,5))\n",
        "  axs = fig.add_subplot(1,2,1)\n",
        "  axs.set_title('Loss')\n",
        "  # Plot all metrics\n",
        "  for metric in loss_metrics:\n",
        "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
        "  axs.legend()\n",
        "  \n",
        "  axs = fig.add_subplot(1,2,2)\n",
        "  axs.set_title('Accuracy')\n",
        "  # Plot all metrics\n",
        "  for metric in acc_metrics:\n",
        "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
        "  axs.legend()\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  # Evaluate on test data\n",
        "  evaluation_results = model.evaluate(test_data, return_dict=True)\n",
        "  print(evaluation_results)\n",
        "\n",
        "  evaluation_results = [evaluation_results[loss_metrics[0]], evaluation_results[acc_metrics[0]]]\n",
        "  \n",
        "  if save:\n",
        "      # Save model\n",
        "      save_model(model,training_results,execution_time, learning_rate, epochs, optimizer, evaluation_results)\n",
        "  \n",
        "  return evaluation_results"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG29KGghUgqZ"
      },
      "source": [
        "# Constants\n",
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "K = 10\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Pipeline variables\n",
        "batch_size = 8\n",
        "train_buffer_size = 1000\n",
        "val_buffer_size = 200\n",
        "prefetch = AUTOTUNE"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBj_9MC2l8o_"
      },
      "source": [
        "# Get Top K answers\n",
        "def get_top_K_answers(k):\n",
        "    answers = pd.read_csv(\"/content/data/big/answers.csv\", index_col=0)\n",
        "    answers = answers.index[:k]\n",
        "    return list(answers)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKnRAy5WHxkf"
      },
      "source": [
        "# Function to parse data features\n",
        "def _parse_features_function(example):\n",
        "    # Parse the input tf.train.Example proto using the dictionary above.\n",
        "    tf_records_features = {\n",
        "        'image_raw': tf.io.FixedLenFeature([], tf.string), \n",
        "        'question' : tf.io.FixedLenFeature([], tf.string),\n",
        "        'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "        'token_type_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "        'attention_mask': tf.io.FixedLenFeature([], tf.string), \n",
        "        'answer': tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "    return tf.io.parse_single_example(example, tf_records_features)\n",
        "\n",
        "\n",
        "# Filter if answer is no\n",
        "def filter_fn(x):\n",
        "    #use broadcasting for element-wise tensor operation\n",
        "    broadcast_equal = tf.equal(TOP_ANSWERS, x['answer'])\n",
        "    broadcast_equal_int = tf.cast(broadcast_equal, tf.int8)\n",
        "    broadcast_sum = tf.reduce_sum(broadcast_equal_int)\n",
        "    return broadcast_sum > 0\n",
        "\n",
        "\n",
        "# Read image and resize it\n",
        "def read_and_decode(img):\n",
        "    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
        "    img = tf.cast(img, tf.float32)/255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "# Structure the data for training\n",
        "def structure_data(data):\n",
        "    image = data['image_raw']\n",
        "    image = read_and_decode(image)\n",
        "    \n",
        "    input_ids = tf.io.decode_raw(data['input_ids'], tf.int32)\n",
        "    attention_mask = tf.io.decode_raw(data['attention_mask'], tf.int32)\n",
        "    token_type_ids = tf.io.decode_raw(data['token_type_ids'], tf.int32)\n",
        "    \n",
        "    answer = tf.io.decode_raw(data['answer'], tf.int32)\n",
        "\n",
        "    return ((image, (input_ids, token_type_ids, attention_mask)), answer)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPWyyDZ90yFT"
      },
      "source": [
        "### Build Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbEY61HinbFk"
      },
      "source": [
        "# ############## #\n",
        "# # Train data # #\n",
        "# ############## #\n",
        "tfrecords_pattern_path = \"/content/data/big/train2014_tf/vaq_raw_train2014_*-of-*.records\"\n",
        "train_files = tf.io.matching_files(tfrecords_pattern_path)\n",
        "train_files = tf.random.shuffle(train_files)\n",
        "train_shards = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "\n",
        "train = train_shards.interleave(tf.data.TFRecordDataset)\n",
        "train = train.map(_parse_features_function, num_parallel_calls=AUTOTUNE)\n",
        "#train = train.filter(filter_fn)\n",
        "train = train.map(structure_data, num_parallel_calls=AUTOTUNE)\n",
        "train = train.shuffle(buffer_size=train_buffer_size).batch(batch_size)\n",
        "train = train.cache().prefetch(prefetch)\n",
        "\n",
        "# ################### #\n",
        "# # Validation data # #\n",
        "# ################### #\n",
        "tfrecords_pattern_path = \"/content/data/big/val2014_tf/vaq_raw_val2014_*-of-*.records\"\n",
        "val_files = tf.io.matching_files(tfrecords_pattern_path)\n",
        "val_files = tf.random.shuffle(val_files)\n",
        "val_shards = tf.data.Dataset.from_tensor_slices(val_files)\n",
        "\n",
        "valid = val_shards.interleave(tf.data.TFRecordDataset)\n",
        "\n",
        "valid = valid.map(_parse_features_function, num_parallel_calls=AUTOTUNE)\n",
        "#valid = valid.filter(filter_fn)\n",
        "valid = valid.map(structure_data, num_parallel_calls=AUTOTUNE)\n",
        "valid = valid.shuffle(buffer_size=val_buffer_size).batch(batch_size)\n",
        "valid = valid.cache().prefetch(prefetch)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR7pciUW5__z"
      },
      "source": [
        "### Get Top Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTFiNYkr6APs"
      },
      "source": [
        "top_answers = get_top_K_answers(K)\n",
        "#top_answers = tf.constant(top_answers)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2QaSsOR6G6b"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_phW9OwF_RwC"
      },
      "source": [
        "def build_vqa_model(image_height, image_width, num_channels, num_classes):\n",
        "    # Handle to pretrained model (Use a different model here)\n",
        "    input_shape=[image_height, image_width, num_channels]\n",
        "    resnet = keras.applications.InceptionResNetV2(\n",
        "        include_top=False, \n",
        "        weights='imagenet', \n",
        "        input_shape=input_shape\n",
        "    )\n",
        "    resnet.trainable = False\n",
        "    image_hidden_states = layers.Flatten()(resnet.output)\n",
        "\n",
        "    input_ids = layers.Input(shape=(24,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(24,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(24,), dtype=tf.int32)\n",
        "    bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "    bert.trainable = False\n",
        "    question = bert(\n",
        "        input_ids, \n",
        "        token_type_ids=token_type_ids, \n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "    last_hidden_states = layers.Flatten()(question[0])\n",
        "\n",
        "    x = layers.concatenate([image_hidden_states, last_hidden_states])\n",
        "    \n",
        "    # Regularize using L1\n",
        "    kernel_weight = 0.02\n",
        "    bias_weight = 0.02\n",
        "\n",
        "    \"\"\"\n",
        "    x = layers.Dense(\n",
        "        units=64, \n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l1(kernel_weight), \n",
        "        bias_regularizer=regularizers.l1(bias_weight))(x)\n",
        "    \"\"\"\n",
        "\n",
        "    output = layers.Dense(\n",
        "        units=num_classes,\n",
        "        kernel_regularizer=tf.keras.regularizers.l1(kernel_weight), \n",
        "        bias_regularizer=tf.keras.regularizers.l1(bias_weight))(x)\n",
        "\n",
        "    model = Model(inputs=[resnet.input, (input_ids, token_type_ids, attention_mask)], outputs=output)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optimizers.Adam(lr=0.001)\n",
        "\n",
        "    # Loss\n",
        "    loss = losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    # Compile\n",
        "    model.compile(\n",
        "        loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHRYgNHP1X4w",
        "outputId": "9e1ebc2b-887a-4246-d60d-c2733b8cf4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256,
          "referenced_widgets": [
            "46a9da129665401f929777712c4acdb5",
            "623b510108e2433495a2bd4b85b188a1",
            "e99ded3bc28c4ff7af48ae334171b146",
            "ea4f78b403f04d01b092b8c0c77a6900",
            "05044c95eda34583a1024bbf3f6cf69e",
            "35b9415d96ab41b88fd074c34123c04e",
            "72e3477381e940a4b1407718f7b1b332",
            "1bc3605203ad48a8b79d02e2412d4bf9",
            "fd8a2ed7647d406c8aca2467025a7319",
            "508910dda801493092ecc252de2eaa99",
            "23d5aa70a7344a83b07baccfcbfcfd78",
            "71e7355441f64829881c04a67d7754ef",
            "d80d4ab855394239ab52d65d7d172eb1",
            "efe05fd62cf04b1d86356091e8d51981",
            "87c634da1258486189a5bec33f4541b0",
            "ff34796d8938465b8ada8cad1b591147"
          ]
        }
      },
      "source": [
        "model = build_vqa_model(224, 224, 3, K)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46a9da129665401f929777712c4acdb5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd8a2ed7647d406c8aca2467025a7319",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKTg-OVr1mFP",
        "outputId": "8dba9f67-b64e-4bd2-e642-1e7bec5fb7c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train model\n",
        "start_time = time.time()\n",
        "\n",
        "training_results = model.fit(\n",
        "        train,\n",
        "        validation_data=valid,\n",
        "        epochs=10, \n",
        "        verbose=1)\n",
        "\n",
        "execution_time = (time.time() - start_time)/60.0\n",
        "print(\"Training execution time (mins)\",execution_time)\n",
        "\n",
        "evaluate_save_model(\n",
        "    model, \n",
        "    valid, \n",
        "    training_results, \n",
        "    execution_time, 0.001, \n",
        "    batch_size, \n",
        "    10, \n",
        "    optimizers.Adam(lr=0.001),\n",
        "    save=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "   1715/Unknown - 146s 85ms/step - loss: 2558149525504.0000 - accuracy: 0.3797"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-jMYcRlMXXh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}