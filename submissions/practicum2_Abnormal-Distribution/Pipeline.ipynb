{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WHec2tyQYBC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch6vpxrwQdcc"
      },
      "source": [
        "<h1 style=\"padding-top: 25px;padding-bottom: 25px;text-align: left; padding-left: 10px; background-color: #DDDDDD; \n",
        "    color: black;\"> <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> AC295: Advanced Practical Data Science </h1>\n",
        "\n",
        "## Practicum 2: Visual Question Answering\n",
        "\n",
        "**Harvard University, Fall 2020**  \n",
        "**Instructors**: Pavlos Protopapas  \n",
        "\n",
        "### **Team: $\\alpha\\beta normal$ $Distri\\beta ution$**\n",
        "#### **Roht Beri, Eduardo Peynetti, Jessica Wijaya, Stuart Neilson**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86zGynCVRsdD"
      },
      "source": [
        "## Creating Pipeline for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86C4-p4lR6jE"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXHPXTLpQZ4V"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "import zipfile\n",
        "import shutil\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from glob import glob\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.layer_utils import count_params\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDHE_Nd6SCy7"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG29KGghUgqZ"
      },
      "source": [
        "# Paths to training and validation data\n",
        "PATHS_TRAIN = '/content/data/vqa_raw_train2014.tfrecords'\n",
        "PATHS_VAL = '/content/data/vqa_raw_vl2014.tfrecords'\n",
        "\n",
        "# Constants\n",
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224\n",
        "IMG_CHANNELS = 3\n",
        "K = 10\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Pipeline variables\n",
        "batch_size = 64\n",
        "train_buffer_size = 800\n",
        "val_buffer_size = 200\n",
        "prefetch = 32"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBj_9MC2l8o_"
      },
      "source": [
        "# Get Top K answers\n",
        "def get_top_K_answers(k):\n",
        "    answers = pd.read_csv(\"/content/data/answers.csv\", index_col=0)\n",
        "    answers = answers.index[:k]\n",
        "    return list(answers)\n",
        "\n",
        "TOP_ANSWERS = get_top_K_answers(K)\n",
        "TOP_ANSWERS = tf.constant(TOP_ANSWERS)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKnRAy5WHxkf"
      },
      "source": [
        "\n",
        "\n",
        "# Function to parse data features\n",
        "def _parse_features_function(example):\n",
        "  # Parse the input tf.train.Example proto using the dictionary above.\n",
        "  tf_records_features = {  \n",
        "      'image_path': tf.io.FixedLenFeature([], tf.string),\n",
        "      'question': tf.io.FixedLenFeature([], tf.string),\n",
        "      'answer': tf.io.FixedLenFeature([], tf.string)\n",
        "  }\n",
        "  return tf.io.parse_single_example(example, tf_records_features)\n",
        "\n",
        "# Filter if answer is no\n",
        "def filter_fn(x):\n",
        "    #use broadcasting for element-wise tensor operation\n",
        "    broadcast_equal = tf.equal(TOP_ANSWERS, x['answer'])\n",
        "    broadcast_equal_int = tf.cast(broadcast_equal, tf.int8)\n",
        "    broadcast_sum = tf.reduce_sum(broadcast_equal_int)\n",
        "    return broadcast_sum > 0\n",
        "\n",
        "# Read image and resize it\n",
        "def read_and_resize(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
        "    img = tf.image.resize_with_pad(img, IMG_HEIGHT, IMG_WIDTH)\n",
        "    img = img/255.0\n",
        "    return img\n",
        "\n",
        "# Structure the data for training\n",
        "def structure_data(data):\n",
        "    path = data['image_path']\n",
        "    image = read_and_resize(path)\n",
        "    question = data['question']\n",
        "    answer = data['answer']\n",
        "    return ((image, question), answer)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPWyyDZ90yFT"
      },
      "source": [
        "### Build Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbEY61HinbFk"
      },
      "source": [
        "# ############## #\n",
        "# # Train data # #\n",
        "# ############## #\n",
        "train_file = ['/content/data/vqa_raw_train2014.tfrecords']\n",
        "train = tf.data.TFRecordDataset(\n",
        "    train_file, \n",
        "    buffer_size=100, \n",
        "    num_parallel_reads=AUTOTUNE\n",
        ")\n",
        "train = train.map(_parse_features_function, num_parallel_calls=AUTOTUNE)\n",
        "train = train.filter(filter_fn)\n",
        "train = train.map(structure_data, num_parallel_calls=AUTOTUNE)\n",
        "train = train.shuffle(buffer_size=train_buffer_size).batch(batch_size)\n",
        "train = train.cache().prefetch(prefetch)\n",
        "\n",
        "# ################### #\n",
        "# # Validation data # #\n",
        "# ################### #\n",
        "val_file = ['/content/data/vqa_raw_val2014.tfrecords']\n",
        "\n",
        "valid = tf.data.TFRecordDataset(\n",
        "    val_file, \n",
        "    buffer_size=100, \n",
        "    num_parallel_reads=AUTOTUNE\n",
        ")\n",
        "\n",
        "valid = valid.map(_parse_features_function, num_parallel_calls=AUTOTUNE)\n",
        "valid = valid.filter(filter_fn)\n",
        "valid = valid.map(structure_data, num_parallel_calls=AUTOTUNE)\n",
        "valid = valid.shuffle(buffer_size=val_buffer_size).batch(batch_size)\n",
        "valid = valid.cache().prefetch(prefetch)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU9ZEjJgxzLb",
        "outputId": "b085dc11-bd28-4351-e185-57729c441520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (((None, 224, 224, 3), (None,)), (None,)), types: ((tf.float32, tf.string), tf.string)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3JbZhJ760BN",
        "outputId": "c364ab75-3264-4ff6-911b-f3d91dbca90f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "valid"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (((None, 224, 224, 3), (None,)), (None,)), types: ((tf.float32, tf.string), tf.string)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnEX3kPOjphA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}